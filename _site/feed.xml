<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-07T01:41:58+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">thekingofcool’s website</title><entry><title type="html">Be a Good Data Engineer - Spark</title><link href="http://localhost:4000/blog/2024/09/06/spark.html" rel="alternate" type="text/html" title="Be a Good Data Engineer - Spark" /><published>2024-09-06T00:00:00+08:00</published><updated>2024-09-06T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/09/06/spark</id><content type="html" xml:base="http://localhost:4000/blog/2024/09/06/spark.html"><![CDATA[<h3 id="background">Background</h3>
<p>进入21世纪以来，随着互联网的发展，各类社交媒体，科技软件，传感器等工具一刻不停地生成着越来越多地数据。有一个说法是:</p>
<blockquote>
  <p>人类现有90%的数据来自于过去两年。</p>
</blockquote>

<p>这些数据不管是否得到了很好的利用，谁也不能否认它们可以带来的高价值，特别是现阶段以数据和能源作为养料的 artificial intelligence。</p>

<p>大数据集对传统单机数据库造成了不小的麻烦，面对如何处理大量数据并从中洞见到有价值信息的需求，Google 三篇论文——<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/gfs-sosp2003.pdf">Google File System</a> at 2003, <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/mapreduce-osdi04.pdf">MapReduce</a> at 2004, <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable</a> at 2006 启发了无数贡献者，推出了一个又一个大数据开源项目，为人类掀开了大数据时代的巨幕。</p>

<p>其中，受 Google File System 和 MapReduce 启发，yahoo 的一组工程师在 2006 年开源了 <a href="https://hadoop.apache.org/">Hadoop</a>， Hadoop 引入了两个技术，分布式存储 (HDFS) 和分布式计算 (MapReduce) ——将大文件切分成小块，分布保存在集群中的多个机器上，每个小块文件备份成多份以防某个节点出错导致文件丢失；处理计算任务时，也将任务分成多个单元，由多个 excutor 分别执行计算操作，再将结果合并在一起。</p>

<p>这种处理方法利用大量廉价机器使得大数据计算得以实现。但由于其内在机制限制，大量的中间计算结果需要落地磁盘，过程中产生的 I/O 导致整个计算花费大量的时间。随着计算机硬件水平和成本的降低，Spark 作为一个内存计算引擎，凭借其更加高效的计算的优势逐渐取代了 MapReduce 的功能。</p>

<h3 id="apache-spark">Apache Spark</h3>
<h4 id="intro">Intro</h4>
<p><a href="https://spark.apache.org/">Apache Spark</a> 是由加州大学伯克利分校的一些研究员在 2009 年推出的一个研究项目，目的就是为了解决 Hadoop 的上述限制。Spark 推出了一个 RDD(Resilient Distributed Dataset) 的概念，使数据得以数据集的形式存储在内存中，使得数据读取和处理都更加快速。</p>

<h4 id="language">Language</h4>
<p>Spark 源码是由 Scala 语言编写，但提供了 Python, Scala, Java, R 的 API 接口进行编程。Python 由于其易用性、丰富的资源库以及和 Data Science, Machine Learning 的紧密结合，其已经成为 Spark 主推的编程语言。下面的演示都以 Python 为例。</p>

<h4 id="installation">Installation</h4>
<p>Make sure java is installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>default-jre
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>pyspark
</code></pre></div></div>

<h4 id="word-count-example">Word Count Example</h4>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PySpark is included in the official releases of Spark available in the Apache Spark website.
For Python users, PySpark also provides pip installation from PyPI.
This is usually for local usage or as a client to connect to a cluster instead of setting up a cluster itself.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="n">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">explode</span><span class="p">,</span> <span class="n">split</span><span class="p">,</span> <span class="n">regexp_replace</span><span class="p">,</span> <span class="n">col</span>

<span class="c1"># init SparkSession
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
    <span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">WordCount</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">master</span><span class="p">(</span><span class="sh">"</span><span class="s">local</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="c1"># read input file
</span><span class="n">input_file</span> <span class="o">=</span> <span class="sh">"</span><span class="s">word_count.txt</span><span class="sh">"</span>
<span class="n">text_file</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">input_file</span><span class="p">)</span>

<span class="c1"># calculate word frequency
</span><span class="n">word_counts</span> <span class="o">=</span> <span class="n">text_file</span> \
               <span class="p">.</span><span class="nf">withColumn</span><span class="p">(</span><span class="sh">"</span><span class="s">cleaned_value</span><span class="sh">"</span><span class="p">,</span> <span class="nf">regexp_replace</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">value</span><span class="sh">"</span><span class="p">),</span> <span class="sh">"</span><span class="s">[^\w\s]</span><span class="sh">"</span><span class="p">,</span> <span class="sh">""</span><span class="p">))</span> \
               <span class="p">.</span><span class="nf">select</span><span class="p">(</span><span class="nf">explode</span><span class="p">(</span><span class="nf">split</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">cleaned_value</span><span class="sh">"</span><span class="p">),</span> <span class="sh">"</span><span class="s">\s+</span><span class="sh">"</span><span class="p">)).</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">word</span><span class="sh">"</span><span class="p">))</span> \
               <span class="p">.</span><span class="nf">groupBy</span><span class="p">(</span><span class="sh">"</span><span class="s">word</span><span class="sh">"</span><span class="p">)</span> \
               <span class="p">.</span><span class="nf">count</span><span class="p">()</span>

<span class="c1"># save result to output file
</span><span class="n">output_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">word_count_output</span><span class="sh">"</span>
<span class="n">word_counts</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="nf">csv</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">overwrite</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># stop SparkSession
</span><span class="n">spark</span><span class="p">.</span><span class="nf">stop</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="concepts">Concepts</h4>
<h5 id="spark-任务的运行模式">Spark 任务的运行模式</h5>
<ul>
  <li>Local Mode: 单机运行，资源有限，适合用于手动调试；</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit word_count.py

python word_count.py

spark-submit <span class="nt">--master</span> <span class="nb">local</span><span class="o">[</span><span class="k">*</span><span class="o">]</span> word_count.py
</code></pre></div></div>

<ul>
  <li>Client Mode: 任务的 Driver 在本地运行，实际计算任务分发到集群中的 Worker 节点运行，由于 Driver 在本地运行，可以方便地查看日志和调试信息；</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit <span class="nt">--master</span> yarn <span class="nt">--deploy-mode</span> client word_count.py

spark-submit <span class="nt">--master</span> spark://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> client word_count.py

spark-submit <span class="nt">--master</span> k8s://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> client word_count.py
</code></pre></div></div>

<ul>
  <li>Cluster Mode: 提交任务到分布式集群运行，可以利用更高的计算性能，适用于生产环境；</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit <span class="nt">--master</span> yarn <span class="nt">--deploy-mode</span> cluster word_count.py

spark-submit <span class="nt">--master</span> spark://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> cluster word_count.py

spark-submit <span class="nt">--master</span> k8s://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> cluster word_count.py
</code></pre></div></div>

<h5 id="spark-资源管理器">Spark 资源管理器</h5>
<p>Spark 资源管理器负责分配和调度集群资源(CPU, Memory, Disk, Network)</p>
<ol>
  <li><a href="https://spark.apache.org/docs/latest/spark-standalone.html">Standalone</a>: 默认的资源管理器；</li>
  <li><a href="https://spark.apache.org/docs/latest/running-on-yarn.html">YARN</a>: 由 Hadoop 提供；</li>
  <li><a href="https://spark.apache.org/docs/latest/running-on-mesos.html">Mesos</a>(Deprecated): 由 Apache Mesos 提供;</li>
  <li><a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html">Kubernetes</a>: 由 Kubernetes 提供</li>
</ol>

<h4 id="usage-scenarios">Usage scenarios</h4>
<h5 id="spark-sql">Spark SQL</h5>
<p><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> 是 Spark 用于处理结构化数据的模块，它提供了一个编程抽象，支持 SQL 查询、Dataframe API 和 Dataset API，而不需要了解底层分布式计算的细节。</p>

<p>它与 <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">RDD</a> 的不同在于：</p>
<ol>
  <li>RDD 提供了底层的 <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations">RDD API</a>，用户需要编写更多的代码来进行数据处理，适合处理非结构化数据；Spark SQL 将数据抽象为 <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html">Dataframe</a> 或 Dataset，提供了更高级的优化和执行策略；</li>
  <li>RDD 类型安全，DataFrame API 是非类型安全的，Dataset API 提供了类型安全的操作；</li>
  <li>Spark SQL 支持 <a href="https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html">Hive</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-avro.html">Avro</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html">Parquet</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-orc.html">ORC</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-json.html">JSON</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html">JDBC</a> 等多种数据源，而 RDD 需要用户自己实现对不同数据源的支持；</li>
</ol>

<p>总的来说，Spark SQL 更适合处理结构化数据和需要高效查询优化的场景，而 RDD 更适合处理非结构化数据和需要自定义处理逻辑的场景。</p>

<p>需要特别注意的是，Spark SQL 性能调优需要考虑的因素很多，包括数据倾斜、数据分区、数据格式、数据压缩、数据缓存等，根据实际情况参考 <a href="https://spark.apache.org/docs/latest/sql-performance-tuning.html">Spark SQL 性能调优</a> 进行调整。</p>

<h5 id="pandas-api-on-spark">Pandas API on Spark</h5>
<p><a href="https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html#">Pandas API on Spark</a> 是 Spark 提供的用于处理大规模数据集的 API，它提供了与 <a href="https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html">Pandas</a> 类似的 API，使得用户可以方便地将 Pandas 的代码迁移到 Spark 上运行。</p>

<h5 id="mllib">MLlib</h5>
<p><a href="https://spark.apache.org/docs/latest/ml-guide.html">MLlib</a> 是 Apache Spark 的机器学习库，提供了一系列用于构建和训练机器学习模型的工具和算法。MLlib 旨在简化机器学习的工作流程，并支持大规模数据集的处理。</p>

<h6 id="features">Features</h6>
<ol>
  <li><strong>丰富的算法库</strong>: 包含分类、回归、聚类、协同过滤、降维等多种机器学习算法。</li>
  <li><strong>数据处理</strong>: 提供数据预处理工具，如特征提取、转换和标准化，支持 DataFrame 和 RDD 数据结构。</li>
  <li><strong>管道 API</strong>: 允许用户构建复杂的机器学习工作流，通过管道将多个处理步骤组合在一起。</li>
  <li><strong>模型评估</strong>: 提供多种评估指标和交叉验证工具，帮助用户评估模型性能。</li>
  <li><strong>分布式计算</strong>: 利用 Spark 的分布式计算能力，支持在大规模数据集上进行高效的模型训练和预测。</li>
</ol>

<h6 id="example">Example</h6>
<p>以下是一个使用 MLlib 进行线性回归的简单示例：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="n">pyspark.ml.regression</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Create SparkSession
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
<span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">MLlibExample</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="c1"># Load training data
</span><span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="sh">"</span><span class="s">libsvm</span><span class="sh">"</span><span class="p">).</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">data/mllib/sample_linear_regression_data.txt</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Create Linear Regression model
</span><span class="n">lr</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>

<span class="c1"># Fit the model
</span><span class="n">lr_model</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1">#Print the coefficients and intercept
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Coefficients: </span><span class="si">{</span><span class="n">lr_model</span><span class="p">.</span><span class="n">coefficients</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Intercept: </span><span class="si">{</span><span class="n">lr_model</span><span class="p">.</span><span class="n">intercept</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Stop SparkSession
</span><span class="n">spark</span><span class="p">.</span><span class="nf">stop</span><span class="p">()</span>
</code></pre></div></div>

<h5 id="graphx">GraphX</h5>
<p><a href="https://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> 是 Apache Spark 的图计算库，旨在处理大规模图数据。GraphX 提供了一个统一的 API，用于图的创建、操作和分析，支持图的并行计算。</p>

<h6 id="features-1">Features</h6>
<ol>
  <li><strong>图表示</strong>: GraphX 使用边（Edge）和顶点（Vertex）来表示图，支持有向图和无向图。</li>
  <li><strong>图计算</strong>: 提供了多种图算法，如 PageRank、连接组件、最短路径等，方便用户进行图分析。</li>
  <li><strong>与 Spark 的集成</strong>: GraphX 可以与 Spark 的其他组件（如 Spark SQL 和 MLlib）无缝集成，支持复杂的数据处理和分析任务。</li>
  <li><strong>灵活的 API</strong>: 提供了基于 RDD 的 API，用户可以使用 Scala、Java 和 Python 进行图计算。</li>
</ol>

<h6 id="example-1">Example</h6>
<p>以下是一个使用 GraphX 计算 PageRank 的简单示例：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="n">pyspark.graphx</span> <span class="kn">import</span> <span class="n">GraphLoader</span>

<span class="c1"># Create SparkSession
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
<span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">GraphXExample</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="c1"># Load graph data
</span><span class="n">graph</span> <span class="o">=</span> <span class="n">GraphLoader</span><span class="p">.</span><span class="nf">edgeListFile</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="sh">"</span><span class="s">data/graphx/followers.txt</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Compute PageRank
</span><span class="n">ranks</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="nf">pageRank</span><span class="p">(</span><span class="n">maxIter</span><span class="o">=</span><span class="mi">10</span><span class="p">).</span><span class="n">vertices</span>

<span class="c1"># Print the results
</span><span class="nf">print</span><span class="p">(</span><span class="n">ranks</span><span class="p">.</span><span class="nf">collect</span><span class="p">())</span>

<span class="c1"># Stop SparkSession
</span><span class="n">spark</span><span class="p">.</span><span class="nf">stop</span><span class="p">()</span>
</code></pre></div></div>

<h5 id="structured-streaming">Structured Streaming</h5>
<p><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Structured Streaming</a> 是 Spark 的流处理库，用于构建可扩展的流式数据处理应用程序。Structured Streaming 允许用户以批处理的方式处理流数据，同时保持了与批处理相同的编程模型。</p>

<h6 id="features-2">Features</h6>
<ol>
  <li><strong>编程模型</strong>: 使用类似于批处理的编程模型来处理流数据，用户可以编写类似于批处理作业的代码来处理流数据。</li>
  <li><strong>数据源</strong>: 支持多种数据源，如 Kafka、Socket、文件系统等，方便用户将现有数据迁移到流处理任务中。</li>
  <li><strong>数据格式</strong>: 支持多种数据格式，如 CSV、JSON、Parquet 等，方便用户将现有数据迁移到流处理任务中。</li>
  <li><strong>数据处理</strong>: 支持多种数据处理操作，如过滤、转换、聚合等，方便用户进行数据处理。</li>
  <li><strong>数据输出</strong>: 支持多种数据输出方式，如 Kafka、Socket、文件系统等，方便用户将处理结果输出到外部系统。</li>
</ol>

<h6 id="example-2">Example</h6>
<p>以下是一个使用 Structured Streaming 进行实时数据处理的简单示例：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="n">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">explode</span><span class="p">,</span> <span class="n">split</span>

<span class="c1"># Create SparkSession
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
<span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">StructuredStreamingExample</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="c1"># Create DataFrame representing the stream of input data
</span><span class="n">data</span> <span class="o">=</span> <span class="n">spark</span> \
<span class="p">.</span><span class="n">readStream</span> \
<span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="sh">"</span><span class="s">socket</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">option</span><span class="p">(</span><span class="sh">"</span><span class="s">host</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">localhost</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">option</span><span class="p">(</span><span class="sh">"</span><span class="s">port</span><span class="sh">"</span><span class="p">,</span> <span class="mi">9999</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">load</span><span class="p">()</span>

<span class="c1"># Split the lines into words    
</span><span class="n">words</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">select</span><span class="p">(</span><span class="nf">explode</span><span class="p">(</span><span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">)).</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">word</span><span class="sh">"</span><span class="p">))</span>

<span class="c1"># Count the occurrences of each word
</span><span class="n">wordCounts</span> <span class="o">=</span> <span class="n">words</span><span class="p">.</span><span class="nf">groupBy</span><span class="p">(</span><span class="sh">"</span><span class="s">word</span><span class="sh">"</span><span class="p">).</span><span class="nf">count</span><span class="p">()</span>

<span class="c1"># Start the query to stream the data
</span><span class="n">query</span> <span class="o">=</span> <span class="n">wordCounts</span> \
<span class="p">.</span><span class="n">writeStream</span> \
<span class="p">.</span><span class="nf">outputMode</span><span class="p">(</span><span class="sh">"</span><span class="s">complete</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="sh">"</span><span class="s">console</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">start</span><span class="p">()</span>

<span class="c1"># Wait for the query to finish
</span><span class="n">query</span><span class="p">.</span><span class="nf">awaitTermination</span><span class="p">()</span>

<span class="c1"># Stop SparkSession
</span><span class="n">spark</span><span class="p">.</span><span class="nf">stop</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="spark-性能调优">Spark 性能调优</h3>
<p>Spark 性能调优是一个复杂的过程，除了代码层面的优化，这是一些<a href="https://spark.apache.org/docs/latest/configuration.html">配置参数</a>，以下是一些常见的性能调优方法：</p>

<h4 id="资源管理">资源管理</h4>
<ol>
  <li><strong>调整资源分配</strong>: 根据集群的资源情况，<a href="https://spark.apache.org/docs/latest/configuration.html#application-properties">合理分配</a> CPU、内存、磁盘和网络资源，确保每个任务都能获得足够的资源；</li>
  <li><strong>设置资源预取</strong>: 在任务开始前，预先分配好所需的资源，减少任务启动时间；</li>
  <li><strong>监控资源使用情况</strong>: 使用 Spark 的<a href="https://spark.apache.org/docs/latest/monitoring.html">监控工具</a>，如 Spark Web UI、Ganglia、Prometheus 等，监控集群的资源使用情况，及时发现并解决资源瓶颈。</li>
</ol>

<h4 id="数据分区">数据分区</h4>
<ol>
  <li><strong>合理设置分区数</strong>: 根据数据量和集群的资源情况，合理设置分区数，减少数据倾斜和资源浪费；</li>
  <li><strong>数据倾斜</strong>: 数据倾斜是指某些分区中的数据量远大于其他分区，导致某些任务运行时间过长，甚至导致任务失败。可以通过增加分区数、调整分区大小、使用随机分区等方式解决数据倾斜问题；</li>
  <li><strong>数据预分区</strong>: 在数据加载时，根据业务需求和集群的资源情况，合理设置数据分区，减少数据倾斜和资源浪费。</li>
</ol>

<h4 id="数据格式">数据格式</h4>
<ol>
  <li><strong>选择合适的数据格式</strong>: 根据数据的特点和业务需求，选择合适的数据格式，如 <a href="https://parquet.apache.org/">Parquet</a>、<a href="https://orc.apache.org/">ORC</a>、<a href="https://avro.apache.org/">Avro</a> 等，减少数据存储和传输的开销；</li>
  <li><strong>数据压缩</strong>: 使用<a href="https://spark.apache.org/docs/latest/configuration.html#compression-and-serialization">数据压缩算法</a>，如 <a href="https://github.com/google/snappy">Snappy</a>、<a href="https://www.gnu.org/software/gzip/">Gzip</a>、<a href="https://www.oberhumer.com/opensource/lzo/">LZO</a> 等，减少数据存储和传输的开销。</li>
</ol>

<h4 id="数据缓存">数据缓存</h4>
<ol>
  <li><strong>合理设置缓存策略</strong>: 根据数据的特点和业务需求，合理设置<a href="https://spark.apache.org/docs/latest/configuration.html#memory-management">缓存策略</a>，减少数据读取和处理的开销。</li>
</ol>

<p>To be continued…</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Background 进入21世纪以来，随着互联网的发展，各类社交媒体，科技软件，传感器等工具一刻不停地生成着越来越多地数据。有一个说法是: 人类现有90%的数据来自于过去两年。]]></summary></entry><entry><title type="html">Be a Good Data Engineer - Cron and Task Scheduler</title><link href="http://localhost:4000/blog/2024/09/04/cron_and_scheduler.html" rel="alternate" type="text/html" title="Be a Good Data Engineer - Cron and Task Scheduler" /><published>2024-09-04T00:00:00+08:00</published><updated>2024-09-04T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/09/04/cron_and_scheduler</id><content type="html" xml:base="http://localhost:4000/blog/2024/09/04/cron_and_scheduler.html"><![CDATA[<h3 id="background">Background</h3>
<p>程序设计里，依据时间或者依赖其他任务、事件触发执行命令和脚本，是开发很重要的一部分。本文以类 Unix 系统最基础的 Crontab 和生产级任务调度器 Airflow 为例，整体梳理下任务调度这件事。</p>

<h3 id="cron">Cron</h3>
<p>Cron 是类 Unix 操作系统中一款基于时间的任务管理工具，可以通过 cron 在固定的时间、日期、间隔下，执行命令或运行脚本。</p>
<ul>
  <li>check if cron is installed: <code class="language-plaintext highlighter-rouge">dpkg -l cron</code></li>
  <li>install cron: <code class="language-plaintext highlighter-rouge">apt-get install cron</code></li>
  <li>verify the status of cron: <code class="language-plaintext highlighter-rouge">systemctl status cron</code> or <code class="language-plaintext highlighter-rouge">service cron status</code></li>
  <li>start/stop cron service: <code class="language-plaintext highlighter-rouge">systemctl start/stop cron</code> or <code class="language-plaintext highlighter-rouge">service cron start/stop</code>
    <h4 id="use-cases">Use Cases</h4>
    <p>Check Crontab use case, make sure you go through <code class="language-plaintext highlighter-rouge">man crontab</code></p>
  </li>
  <li>test your command line to schedule: <code class="language-plaintext highlighter-rouge">echo "Hello World manually at $(date)" &gt;&gt; $HOME/greetings_manual.txt</code></li>
  <li>check result: <code class="language-plaintext highlighter-rouge">tail ~/greetings_manual.txt</code></li>
  <li>install new crontab job, and choose vim editor: <code class="language-plaintext highlighter-rouge">crontab -e</code></li>
  <li>add a line <code class="language-plaintext highlighter-rouge">* * * * * echo "Hello World automatically at $(date)" &gt;&gt; $HOME/greetings.txt"</code> to the end of the crontab file, save and exit the editor</li>
  <li>check result: <code class="language-plaintext highlighter-rouge">tail ~/greetings.txt</code></li>
  <li>list all crontab jobs: <code class="language-plaintext highlighter-rouge">crontab -l</code></li>
  <li>remove all crontab jobs created by current user: <code class="language-plaintext highlighter-rouge">crontab -r</code>
    <h4 id="syntax">Syntax</h4>
    <p><code class="language-plaintext highlighter-rouge">* * * * * [command to execute]</code></p>
  </li>
  <li>First * stands for representing minutes [0-59];</li>
  <li>Second * stands for representing hour[0-23];</li>
  <li>Third * stands for representing day [0-31];</li>
  <li>Fourth * stands for representing month[0-12];</li>
  <li>Fifth * stands for representing a day of the week[0-6];</li>
</ul>

<p>You can check your cron schedule expressions at: <a href="https://crontab.guru/">Cronitor</a>.</p>

<h3 id="apache-airflow">Apache Airflow</h3>
<p><a href="https://airflow.apache.org/">Airflow</a> 是一款功能强大的开源工作流调度工具，除了内置的如bash、python、email等 <a href="https://airflow.apache.org/docs/apache-airflow/stable/operators-and-hooks-ref.html">Core Operators</a> 以外，还集成了大量第三方平台应用/软件的 <a href="https://airflow.apache.org/docs/apache-airflow-providers/operators-and-hooks-ref/index.html">Community Operators</a>，包含 Apache Software Foundation, Amazon Web Services, Microsoft Azure, Google, etc 相关服务的调度，并且能够监控任务的状态。Airflow 服务器可以部署在单机，也可以根据需求扩展至多节点部署。</p>

<h4 id="installation">installation</h4>
<p>Only <code class="language-plaintext highlighter-rouge">pip</code> installation is currently officially supported.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">AIRFLOW_VERSION</span><span class="o">=</span>2.10.0

<span class="nv">PYTHON_VERSION</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>python3 <span class="nt">--version</span> | <span class="nb">cut</span> <span class="nt">-d</span> <span class="s2">" "</span> <span class="nt">-f</span> 2 | <span class="nb">cut</span> <span class="nt">-d</span> <span class="s2">"."</span> <span class="nt">-f</span> 1-2<span class="si">)</span><span class="s2">"</span>

<span class="nv">CONSTRAINT_URL</span><span class="o">=</span><span class="s2">"https://raw.githubusercontent.com/apache/airflow/constraints-</span><span class="k">${</span><span class="nv">AIRFLOW_VERSION</span><span class="k">}</span><span class="s2">/constraints-</span><span class="k">${</span><span class="nv">PYTHON_VERSION</span><span class="k">}</span><span class="s2">.txt"</span>

pip <span class="nb">install</span> <span class="s2">"apache-airflow==</span><span class="k">${</span><span class="nv">AIRFLOW_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--constraint</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CONSTRAINT_URL</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<h4 id="init-airflow-database">init airflow database</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow db init
</code></pre></div></div>

<h4 id="create-a-user">create a user</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow <span class="nb">users </span>create <span class="se">\</span>
<span class="nt">--username</span> thekingofcool <span class="se">\</span>
<span class="nt">--firstname</span> bug <span class="se">\</span>
<span class="nt">--lastname</span> hunter <span class="se">\</span>
<span class="nt">--role</span> Admin <span class="se">\</span>
<span class="nt">--email</span> sayhi@thekingof.cool
</code></pre></div></div>

<h4 id="start-airflow-scheduler-and-web-server">start airflow scheduler and web server</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow scheduler
airflow webserver <span class="nt">--port</span> 8080
</code></pre></div></div>

<p>visit <a href="http://localhost:8080">localhost:8080</a> to check out the web page.</p>

<h4 id="airflow-dag-file">airflow dag file</h4>
<p>在 Airflow 配置文件中查看 dag 文件位置:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> ~/airflow/airflow.cfg | <span class="nb">grep </span>dags_folder
</code></pre></div></div>
<p>在该文件目录下编辑 dag 文件，重启 airflow scheduler。</p>

<p>Note:</p>
<ul>
  <li>Recommend to use Postgres or MySQL as <a href="https://airflow.apache.org/docs/apache-airflow/2.10.0/howto/set-up-database.html">metadata DB</a> in production;</li>
  <li>Do not use the <a href="https://airflow.apache.org/docs/apache-airflow/2.10.0/core-concepts/executor/index.html">SequentialExecutor</a> in production.</li>
</ul>

<p>To be continued…</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Background 程序设计里，依据时间或者依赖其他任务、事件触发执行命令和脚本，是开发很重要的一部分。本文以类 Unix 系统最基础的 Crontab 和生产级任务调度器 Airflow 为例，整体梳理下任务调度这件事。]]></summary></entry><entry><title type="html">Be a Good Data Engineer - Linux and Shell</title><link href="http://localhost:4000/blog/2024/09/01/linux_and_shell.html" rel="alternate" type="text/html" title="Be a Good Data Engineer - Linux and Shell" /><published>2024-09-01T00:00:00+08:00</published><updated>2024-09-01T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/09/01/linux_and_shell</id><content type="html" xml:base="http://localhost:4000/blog/2024/09/01/linux_and_shell.html"><![CDATA[<h3 id="background">Background</h3>
<p>Linux 是基于 Unix 的开源操作系统，通常用于服务器、台式机和嵌入式系统。</p>

<p>Linux terminal 是一个用于和操作系统进行交互以及执行各种任务的命令行界面，可以用作浏览目录文件及文件中的内容，你也可以用它来安装和升级软件包，也能创建和编辑文件以及执行 shell 脚本。</p>

<h3 id="package-management">Package management</h3>
<p>常用的包管理器有：</p>
<h4 id="apt"><a href="https://wiki.debian.org/Apt">apt</a></h4>
<p>Advanced Packaging Tools, 是一款 <a href="https://www.debian.org/">Debian</a>-based Linux 系统最常用的包管理器。</p>

<p>背景信息：</p>

<p><em>Some of the most popular Debian-based Linux distributions: Linux Mint, Ubuntu, Kali Linux.</em></p>

<p><em>Meanwhile Red Hat-based distributions are: CentOS, Fedora.</em></p>

<p><em>Red Hat-based Linux distributions are often preferred for enterprise environments and servers, focusing on stability and security. Whereas Debian-based Linux distributions moreover focus on long-term support and stability.</em>
Most Used Commands:</p>
<ul>
  <li>install package: <code class="language-plaintext highlighter-rouge">sudo apt-get install [package_name]</code>;</li>
  <li>remove the package: <code class="language-plaintext highlighter-rouge">sudo apt-get remove [package_name]</code>;</li>
  <li>list details of installed package: <code class="language-plaintext highlighter-rouge">sudo apt-get list [package_name]</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://linux.die.net/man/8/apt-get">Linux man page</a></em>.</p>

<h4 id="yum"><a href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/5/html/deployment_guide/c1-yum">yum</a></h4>
<p>Yellowdog Updater Modified, 用作 <a href="https://www.redhat.com/en">Red Hat</a> Enterprise Linux versions 5 and later 的包管理；</p>

<p>Most Used Commands:</p>
<ul>
  <li>install: <code class="language-plaintext highlighter-rouge">yum install [package_name]</code>;</li>
  <li>remove: <code class="language-plaintext highlighter-rouge">yum remove [package_name]</code>;</li>
  <li>update: <code class="language-plaintext highlighter-rouge">yum update</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://access.redhat.com/articles/yum-cheat-sheet">Yum Command Cheat Sheet</a></em>.</p>

<h4 id="dnf"><a href="https://docs.fedoraproject.org/en-US/fedora/latest/system-administrators-guide/package-management/DNF/">dnf</a></h4>
<p>Dandified YUM, a package manager for .rpm-based Linux distributions, is now the default software package management tool in <a href="https://fedoraproject.org/">Fedora</a>.</p>

<p>Most Used Commands:</p>
<ul>
  <li>search: <code class="language-plaintext highlighter-rouge">dnf search packagename</code>;</li>
  <li>install: <code class="language-plaintext highlighter-rouge">dnf install packagename</code>;</li>
  <li>remove: <code class="language-plaintext highlighter-rouge">dnf remove packagename</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://dnf.readthedocs.io/en/latest/command_ref.html">DNF Command Reference</a></em>.</p>

<h4 id="pacman"><a href="https://pacman.archlinux.page/">pacman</a></h4>
<p>一款简单的 Linux 包管理器。</p>

<p>Most Used Commands:</p>
<ul>
  <li>search: <code class="language-plaintext highlighter-rouge">sudo pacman -Ss keyword</code>;</li>
  <li>install: <code class="language-plaintext highlighter-rouge">sudo pacman -S pkgname</code>;</li>
  <li>remove: <code class="language-plaintext highlighter-rouge">sudo pacman -Rs package_name</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://wiki.archlinux.org/title/Pacman#Usage">pacman usage</a></em>.</p>

<h4 id="pip"><a href="https://pip.pypa.io/en/stable/">pip</a></h4>
<p>pip is the package installer for <a href="https://www.python.org/">Python</a>. Alternatively, <a href="https://www.anaconda.com/">conda</a> is another popular package management tool.</p>

<p>Most Used Commands:</p>
<ul>
  <li>search: <code class="language-plaintext highlighter-rouge">python -m pip search "key_word"</code>;</li>
  <li>install: <code class="language-plaintext highlighter-rouge">python -m pip install SomePackage</code>;</li>
  <li>uninstall: <code class="language-plaintext highlighter-rouge">python -m pip uninstall SomePackage</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://pip.pypa.io/en/stable/user_guide/">pip User Guide</a></em>.</p>

<h4 id="brew"><a href="https://brew.sh/">brew</a></h4>
<p>Homebrew installs the stuff you need that Apple (or your Linux system) didn’t.</p>

<p>Most Used Commands:</p>
<ul>
  <li>search: <code class="language-plaintext highlighter-rouge">brew search text</code>;</li>
  <li>install: <code class="language-plaintext highlighter-rouge">brew install package_name</code>;</li>
  <li>uninstall: <code class="language-plaintext highlighter-rouge">brew uninstall package_name</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://docs.brew.sh/Manpage">brew command documatation</a></em>.</p>

<p><strong>Shell learning and troubleshooting: <a href="https://explainshell.com/">explainshell</a></strong>.</p>

<h3 id="command-line-basics">Command Line Basics</h3>
<p>What is <a href="https://www.gnu.org/software/bash/">Bash</a></p>
<ul>
  <li>man: system manual pager;</li>
  <li>ssh: Secure Shell;</li>
  <li>ls: list all the files in your current working directory;</li>
  <li>pwd: print current working directory;</li>
  <li>cd: change working directory;</li>
  <li>touch: create a file or change file timestamp;</li>
  <li>echo: write arguments to standard output;</li>
  <li>nano: simple and useful editor;</li>
  <li><a href="https://www.vim.org/">vim</a>: Vi IMproved text editor;</li>
  <li>cat: to see what’s inside a file real quickly;</li>
  <li>mkdir: make a new directory;</li>
  <li>cp: copy file to a target directory;</li>
  <li>mv: move or rename a file;</li>
  <li>rm: remove files or directories;</li>
  <li>ln: make links to file;</li>
  <li>clear: clear your terminal screen;</li>
  <li>whoami: print effective userid;</li>
  <li>useradd: create a new user or update default new user information;</li>
  <li>sudo: execute a command as another user;</li>
  <li>su: run a command with substitute user and group ID;</li>
  <li>exit: exit the shell;</li>
  <li>passwd: change user password;</li>
  <li><a href="https://curl.se/">curl</a>: transfer data with url;</li>
  <li>zip: package and compress (archive) files;</li>
  <li>unzip: extract compressed files in a zip;</li>
  <li>head: display first lines of a file;</li>
  <li>tail: display the last part of a file;</li>
  <li>diff: differential file and directory comparator;</li>
  <li>sort: sort lines of text files;</li>
  <li>find: search for files in a directory hierarchy;</li>
  <li>chmod: change file modes or Access Control Lists;</li>
  <li>chown: change file owner and group;</li>
  <li>ifconfig: configure network interface parameters;</li>
  <li>grep: file pattern searcher;</li>
  <li><a href="https://www.gnu.org/software/gawk/manual/gawk.html">awk</a>: pattern-directed scanning and processing language;</li>
  <li>ping: send ICMP ECHO_REQUEST packets to network hosts;</li>
  <li>netstat: show network status;</li>
  <li>iptables: administration tool for IPv4/IPv6 packet filtering and NAT</li>
  <li>ufw: Uncomplicated Firewall for Ubuntu;</li>
  <li>uname: display information about the system;</li>
  <li>cal: displays a calendar and the date of Easter;</li>
  <li>df: display free disk space;</li>
  <li>ps: process status;</li>
  <li>top: display sorted information about processes;</li>
  <li>kill: terminate or signal a process;</li>
  <li>pkill: find or signal processes by name;</li>
  <li>history: command history;</li>
  <li>reboot: stopping and restarting the system;</li>
  <li>shutdown: close down the system at a given time;</li>
</ul>

<p>Practical command line use cases on data science: <a href="https://jeroenjanssens.com/dsatcl/list-of-command-line-tools">Data Science at the Command Line</a>;</p>

<p>Learning the shell step by step: <a href="https://linuxcommand.org/lc3_writing_shell_scripts.php">Writing Shell Scripts</a>;</p>

<p>To be continued…</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Background Linux 是基于 Unix 的开源操作系统，通常用于服务器、台式机和嵌入式系统。]]></summary></entry><entry><title type="html">Build a PC to Play WuKong</title><link href="http://localhost:4000/blog/2024/08/24/wukong.html" rel="alternate" type="text/html" title="Build a PC to Play WuKong" /><published>2024-08-24T00:00:00+08:00</published><updated>2024-08-24T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/08/24/wukong</id><content type="html" xml:base="http://localhost:4000/blog/2024/08/24/wukong.html"><![CDATA[<h3 id="pc-build-list">PC Build List</h3>
<p>Total cost is 5310.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">item</th>
      <th style="text-align: left">model</th>
      <th>channel</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">CPU</td>
      <td style="text-align: left">AMD R5 7500F</td>
      <td>PDD</td>
      <td>929</td>
    </tr>
    <tr>
      <td style="text-align: left">RAM</td>
      <td style="text-align: left">光威 天策 16g * 2 6400 C32 M</td>
      <td>PDD</td>
      <td>679</td>
    </tr>
    <tr>
      <td style="text-align: left">GPU</td>
      <td style="text-align: left">GeForce RTX 战斧 4060 DUO 8G</td>
      <td>PDD</td>
      <td>2108</td>
    </tr>
    <tr>
      <td style="text-align: left">Heat sink</td>
      <td style="text-align: left">利民 AX120R SE ARGB</td>
      <td>PDD</td>
      <td>81</td>
    </tr>
    <tr>
      <td style="text-align: left">Power supply unit</td>
      <td style="text-align: left">鑫谷 GM650M 金牌全模</td>
      <td>PDD</td>
      <td>349</td>
    </tr>
    <tr>
      <td style="text-align: left">computer case</td>
      <td style="text-align: left">航嘉 S920 全景</td>
      <td>PDD</td>
      <td>160</td>
    </tr>
    <tr>
      <td style="text-align: left">SSD</td>
      <td style="text-align: left">金士顿 480g SSD</td>
      <td>TMall</td>
      <td>265</td>
    </tr>
    <tr>
      <td style="text-align: left">Motherboard</td>
      <td style="text-align: left">技嘉 AMD B650M GAMING WIFI</td>
      <td>JD</td>
      <td>739</td>
    </tr>
    <tr>
      <td style="text-align: left">SSD(optional)</td>
      <td style="text-align: left">Lexar ARES M.2 pcie4.0 nvmeSSD 1T</td>
      <td>TMall</td>
      <td>521</td>
    </tr>
  </tbody>
</table>

<h3 id="create-bootable-windows-11-usb-disk">Create Bootable Windows 11 USB Disk</h3>
<p>When the computer is ready, you need a Windows system boot disk. I’ll do this on a MacOS.</p>

<h4 id="format-usb-disk">Format USB Disk</h4>
<p>Go to Application, find Utilities - Disk Utility;
Choose your Disk, and erase the USB External Physical Volume to MS-DOS (FAT32);
Rename the USB Drive to <strong>WIN11</strong> or whatever you like.</p>

<h4 id="download-system-image">Download System Image</h4>
<p>Download <a href="https://www.microsoft.com/software-download/windows11">Windows 11</a> Disk Image (ISO) to your computer;</p>

<h4 id="making-a-boot-disk">Making a Boot Disk</h4>
<ol>
  <li>Install wimlib
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew install wimlib
</code></pre></div>    </div>
  </li>
  <li>Mount the ISO file into a folder
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hdiutil mount Downloads/Win11_23H2_English_x64v2.iso
</code></pre></div>    </div>
  </li>
  <li>Copy all Windows ISO contents into the USB Disk
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rsync -vha --exclude=sources/install.wim /Volumes/CCCOMA_X64FRE_EN-US_DV9/* /Volumes/WIN11
</code></pre></div>    </div>
  </li>
  <li>Split the installation file in two parts if the size is bigger than 4GB
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wimlib-imagex split /Volumes/CCCOMA_X64FRE_EN-US_DV9/sources/install.wim /Volumes/WIN11/sources/install.swm 3800
</code></pre></div>    </div>
  </li>
  <li>Eject USB device to use it for OS installation.</li>
</ol>

<h3 id="troubleshooting">Troubleshooting</h3>
<ol>
  <li>
    <p>0 free space when installing the system
<strong>Solution</strong>: There is some content in the SSD, choose ‘delete’ to erase SSD.</p>
  </li>
  <li>
    <p>The selected disk has an MBR partition table. On EFI system, Windows can only be installed to GPT disks.
<strong>Solution</strong>:</p>
    <ul>
      <li>Restart the computer, On the Windows installation screen, select ‘Repair your computer’; Choose ‘Troubleshoot’ &gt; ‘Command Prompt’.</li>
      <li>Enter <code class="language-plaintext highlighter-rouge">diskpart</code> to initiate the disk partition process;</li>
      <li>Enter <code class="language-plaintext highlighter-rouge">list disk</code>. Make a note of the MBR disk number that you want to convert to GPT format;</li>
      <li>Enter <code class="language-plaintext highlighter-rouge">select disk</code> <em><code class="language-plaintext highlighter-rouge">&lt;disk-number&gt;</code></em>, where <code class="language-plaintext highlighter-rouge">&lt;disk-number&gt;</code> is the MBR disk number to convert;</li>
      <li>Enter <code class="language-plaintext highlighter-rouge">clean</code> to delete all partitions and volumes on the disk;</li>
      <li>Enter <code class="language-plaintext highlighter-rouge">convert gpt</code> to convert the MBR disk to the GPT partition format.
The diskpart process notifies you when the conversion completes.</li>
    </ul>
  </li>
</ol>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[PC Build List Total cost is 5310.]]></summary></entry><entry><title type="html">Dana White introduces Donald Trump at the 2024 RNC</title><link href="http://localhost:4000/reading/2024/07/18/Dana_White_introduces_Donald_Trump_at_the_2024_RNC.html" rel="alternate" type="text/html" title="Dana White introduces Donald Trump at the 2024 RNC" /><published>2024-07-18T00:00:00+08:00</published><updated>2024-07-18T00:00:00+08:00</updated><id>http://localhost:4000/reading/2024/07/18/Dana_White_introduces_Donald_Trump_at_the_2024_RNC</id><content type="html" xml:base="http://localhost:4000/reading/2024/07/18/Dana_White_introduces_Donald_Trump_at_the_2024_RNC.html"><![CDATA[<p>Good evening ladies and gentlemen. I am Dana White. I am the CEO and president of the Ultimate Fighting Championship.</p>

<p>So two weeks ago I got a call from President Trump asking me if I’d be willing to speak tonight. As usual, there was no pressure, no demands. He asked me as a friend and of course I said yes. Then after I accepted his offer, he sent me a text message. And I just want to read to you a little piece of what President Trump wrote to me.</p>

<blockquote>
  <p>Dana, I’m so honored that you will be doing the introduction at the National Republican Convention. Think of it as the biggest fight you ever had, a fight for our country and even the world. I only wish you didn’t have to interrupt your family trip. But I hope they understand, they love you and they know how important this is.</p>
</blockquote>

<p>Now think about this. This man’s running for President of the United States. He’s fighting for the future of this country and he’s concerned about interrupting my family trip. That’s the President Trump that I know, a man who truly cares about people. The mainstream media likes to push the narrative that he doesn’t care about anyone but himself. I absolutely know that’s not the truth because I’ve been friends with this guy for 25 years.</p>

<p>And for the people who know me, they’ll know this is true. I just want to make something very clear. Nobody in the Trump campaign has ever told me what to say. Nobody tells me what to say and I’m nobody’s puppet. And I’m not telling you what to think. I’m telling you what I know. And I know President Trump, I know President Trump is a fighter. I’ve been saying this since 2015.</p>

<p>Now look at what’s happened over the last 10 years. We have all seen it with our own eyes. I’m in the tough guy business. And this man is the toughest, most resilient human being that I’ve ever met in my life. The higher the stakes, the harder he fights, and this guy never ever gives up. So what’s it stake here? The answer is in President Trump’s text. And I quote:</p>

<blockquote>
  <p>A fight for our country.</p>
</blockquote>

<p>I know why he’s running for President again. Why else? Would he put himself through everything he’s dealt with just to get back here? We all know he doesn’t need this. This guy’s got a great life. He has a beautiful family. And he has achieved everything that you could possibly achieve in life. I know President Trump is literally putting his life on the line for something bigger than himself. And he’s willing to risk it all because he loves this country.</p>

<p>And I know he wants what’s best for the American people. All American people. I know he’s running for President to save our American dream. I’m living in the American dream. And I know the American dream is very real. Whether you’re born in this country or came here from some place else, this is the last real land of opportunity. President Reagan once said,</p>

<blockquote>
  <p>Government’s first duty is to protect the people, not run their lives. And if you’re buried in government red tape, how will you ever start your own business? If you’re struggling to pay your bills, how can you ever afford to start a family? And if you don’t feel safe in your own town, why would you ever buy a house?</p>
</blockquote>

<p>I know the President Trump is fighting to save the American dream, and that’s what’s at stake in this election. We are choosing who we want to lead us in this fight. I know President Trump is a proven leader, a fearless leader, and this country was in a much better place when he was in the Oval Office. In my mind, the choice is clear. But this election, we all get the choose. I know I’m going to choose strength and security. I know I’m going to choose opportunity and prosperity. I know I’m going to choose real American leadership and a real American badass.</p>

<p>And I’m not telling you what choice to make, and I’m not telling you what to think. I’m telling you what I know. I know American needs a strong leader, and the world needs a strong America. I know Donald J. Trump is the best choice for President of the United States. My fellow Americans, it is my honor to introduce the 45th and soon to be 47th President of the United States. Donald J. Trump.</p>]]></content><author><name></name></author><category term="reading" /><summary type="html"><![CDATA[Good evening ladies and gentlemen. I am Dana White. I am the CEO and president of the Ultimate Fighting Championship. So two weeks ago I got a call from President Trump asking me if I’d be willing to speak tonight. As usual, there was no pressure, no demands. He asked me as a friend and of course I said yes. Then after I accepted his offer, he sent me a text message. And I just want to read to you a little piece of what President Trump wrote to me. Dana, I’m so honored that you will be doing the introduction at the National Republican Convention. Think of it as the biggest fight you ever had, a fight for our country and even the world. I only wish you didn’t have to interrupt your family trip. But I hope they understand, they love you and they know how important this is. Now think about this. This man’s running for President of the United States. He’s fighting for the future of this country and he’s concerned about interrupting my family trip. That’s the President Trump that I know, a man who truly cares about people. The mainstream media likes to push the narrative that he doesn’t care about anyone but himself. I absolutely know that’s not the truth because I’ve been friends with this guy for 25 years. And for the people who know me, they’ll know this is true. I just want to make something very clear. Nobody in the Trump campaign has ever told me what to say. Nobody tells me what to say and I’m nobody’s puppet. And I’m not telling you what to think. I’m telling you what I know. And I know President Trump, I know President Trump is a fighter. I’ve been saying this since 2015. Now look at what’s happened over the last 10 years. We have all seen it with our own eyes. I’m in the tough guy business. And this man is the toughest, most resilient human being that I’ve ever met in my life. The higher the stakes, the harder he fights, and this guy never ever gives up. So what’s it stake here? The answer is in President Trump’s text. And I quote: A fight for our country. I know why he’s running for President again. Why else? Would he put himself through everything he’s dealt with just to get back here? We all know he doesn’t need this. This guy’s got a great life. He has a beautiful family. And he has achieved everything that you could possibly achieve in life. I know President Trump is literally putting his life on the line for something bigger than himself. And he’s willing to risk it all because he loves this country. And I know he wants what’s best for the American people. All American people. I know he’s running for President to save our American dream. I’m living in the American dream. And I know the American dream is very real. Whether you’re born in this country or came here from some place else, this is the last real land of opportunity. President Reagan once said, Government’s first duty is to protect the people, not run their lives. And if you’re buried in government red tape, how will you ever start your own business? If you’re struggling to pay your bills, how can you ever afford to start a family? And if you don’t feel safe in your own town, why would you ever buy a house? I know the President Trump is fighting to save the American dream, and that’s what’s at stake in this election. We are choosing who we want to lead us in this fight. I know President Trump is a proven leader, a fearless leader, and this country was in a much better place when he was in the Oval Office. In my mind, the choice is clear. But this election, we all get the choose. I know I’m going to choose strength and security. I know I’m going to choose opportunity and prosperity. I know I’m going to choose real American leadership and a real American badass. And I’m not telling you what choice to make, and I’m not telling you what to think. I’m telling you what I know. I know American needs a strong leader, and the world needs a strong America. I know Donald J. Trump is the best choice for President of the United States. My fellow Americans, it is my honor to introduce the 45th and soon to be 47th President of the United States. Donald J. Trump.]]></summary></entry><entry><title type="html">2014~2024，仅仅10年</title><link href="http://localhost:4000/reading/2024/07/03/ten_years_ago.html" rel="alternate" type="text/html" title="2014~2024，仅仅10年" /><published>2024-07-03T00:00:00+08:00</published><updated>2024-07-03T00:00:00+08:00</updated><id>http://localhost:4000/reading/2024/07/03/ten_years_ago</id><content type="html" xml:base="http://localhost:4000/reading/2024/07/03/ten_years_ago.html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/nbLyWvV-cnj14f0rMWnV3g?WBAPIAnalysisOriUICodes=10000001&amp;aid=01A110-grIlutGzstJd54VtjM7_Idd0V8mws-Z9MgmIslKOSY.&amp;from=10E5393010&amp;v_p=90&amp;wm=3333_2001">Read Original</a></p>

<table>
  <tbody>
    <tr>
      <td>date_saved: 2024-07-03 23:15:56</td>
      <td>date_published: 2024-06-22 14:09:00</td>
    </tr>
  </tbody>
</table>

<p><strong>01</strong></p>

<p>十年前的夏天，年轻人不愿当公务员，国考人数锐减36万，热帖称“机关钱少活多”。</p>

<p>那年全国毕业生700余万，就业率超九成，复旦学生租游艇办毕业舞会，女孩们花两千元买晚礼服，夜游江海。</p>

<p>那年世界的齿轮咬合稳定，中美迎来建交35周年。美国民调中，超72%年轻人，将中国视为“朋友”。</p>

<p>夏天前，奥巴马夫人到访，体验了长城、紫禁城与成都火锅。</p>

<p>慕田峪长城上，总统夫人看燕山起伏，觉得一切宽阔且美妙，“长城的长度几乎相当于从美国缅因州到俄勒冈州的四倍”。</p>

<p>那年的国运也如山峦起伏。</p>

<p>夏天时，股市清冷，七成账户闲置，股民调侃关灯吃面，7月IPO开闸，并购潮掀起，年底股市单日放量7100亿，狂飙冲天。</p>

<p>楼市故事也相似。十年前的五一，房企奄奄一息，北京楼盘推出零首付，南京楼盘跳远减十万，上海房展出动比基尼美女吸引眼球。9月楼市松绑，炒房客陷入狂欢。</p>

<p>十年前的人们尚不知卷与颓，偶有下挫，也认为不过是插曲，对一切满怀自信。</p>

<p>贾跃亭宣布要造超级互联网汽车，罗永浩宣布要发布东半球最好用的手机。真正手机大卖的是小米，第一季度销量超过苹果。</p>

<p>夏天过后，雷军去乌镇参加首届互联网大会。他磕磕巴巴说，梦想还是要有，万一实现呢？</p>

<p>那年乌镇最风光还是BAT，三家都在硅谷设立了分支，李彦宏说机会太多，他很着急：</p>

<blockquote>
  <p>我们其实处在非常有意思的时代，这是魔幻一般的时代，正好我们这一代人赶上互联网的兴起。</p>
</blockquote>

<p>入夜，乌镇白墙黑瓦水音桨声。丁磊拼起旧木桌，摆起乌镇宴，座中人微博记录：十几瓶黄酒喝去，陈年故事吐出，煮酒笑谈云中事，天罗地网立旌旗。</p>

<p>未被邀请的马云，才是那年真正的主角。十年前的夏天，阿里启动全球最大规模IPO。</p>

<p>上市前，马云发内部邮件，建议员工不要挥霍，处理好财富，“我们这么辛苦，可不是为了变成一群土豪”。</p>

<p>当年9月，阿里上市，马云登顶中国首富，万名阿里员工成千万富翁，宝马销售和房产中介堵在阿里园区门口。</p>

<p>十年前的夏天蒸腾如梦，浩荡热风吹过中国。北京高温刷新了1951年以来纪录，居民用水多喝出4.5个昆明湖。</p>

<p>济南、上海、重庆、吐鲁番尽成火炉，更大热浪在互联网彩票服务器上。那年是巴西世界杯，足彩卖出23亿。</p>

<p>在广州，恒大正在冲击中超三连冠，教练席上，新任助教李铁说，有很多东西不是金钱所能衡量，“我给自己十年左右的时间，争取成为国家队的主教练”。</p>

<p>那个夏天，恒大冰泉形象代言人是金秀贤，来自星星的都教授横扫中国，名字一度在人大会议上提及。</p>

<p>火热韩流中，也有人抽身归来。当年10月，鹿晗宣布和韩国方解约，他的微博单条评论破千万，创下吉尼斯世界纪录。</p>

<p>而在夏天，他的队友吴亦凡更早解约，在微博霸气留言：感谢所有支持我的人，吴亦凡一直都在！</p>

<p>属于他们的时代刚拉开帷幕，而上一代的偶像韩寒，在十年前夏天推出了第一部电影。</p>

<p>他为电影写了主题曲《平凡之路》，请出隐遁的朴树演唱，年轻人喜欢那句“我曾经跨过山和大海”，有种渡尽一切后的禅机。</p>

<p>他们不知道，真正的大山大海，还在后面。</p>

<p><strong>02</strong></p>

<p>访问中国时，奥巴马夫人在成都七中演讲，说对共同的未来，抱有前所未有的信心，“希望纽带在未来几十年绵延”。</p>

<p>几个月后，奥巴马在APEC会议上说：加深美中贸易将使两国受益，更能助力世界平稳。</p>

<p>那一年，世界银行预言中国经济将持续高速增长，胡润富豪榜称坚定看好中国，“中国这年有176位百亿富豪，有8位80后白手起家”。</p>

<p>那年时代汽笛嘹亮，股市楼市火爆之下，真正动能是基建启动。</p>

<p>当年，中央批复基建项目1.56万亿，发改委21天内批了16条铁路和5个机场。</p>

<p>任泽平激情连发三篇研报，称“请不要辜负这个时代，将熊市彻底埋葬”。</p>

<p>十年前的夏天，达沃斯论坛上，第一次提出“大众创业、万众创新”，称要破除一切束缚，让创新血液自由流动：</p>

<blockquote>
  <p>“在960万平方公里大地上掀起大众创业、草根创业的新浪潮。”</p>
</blockquote>

<p>海淀图书城步行街改名创业大街，车库咖啡里装满野望。朱啸虎说，只要想看，有看不完的创业项目等着。</p>

<p>当年爆款节目《超级演说家》上，90后女孩说：我不是来适应社会的，我是来改变社会的。</p>

<p>大浪之中，十年前的企业家也带着少年意气。融创的孙宏斌说“理想主义销魂蚀骨”，万科的王石说“我坚信市场的力量”。</p>

<p>那年的俞敏洪，出版新书《在绝望中寻找希望》。他到南开鼓励大一新生，“人要靠自己选择走出无穷无尽的路来”。</p>

<p>当天地足够开阔时，天骄的心里总能装下帝国。</p>

<p>十年前夏天，恒大在草原深处举办订货会。</p>

<p>32架包机直飞乌兰浩特，80辆大巴浩荡出发，穿越草原，翻过土岭，最终到达阿尔山的荒僻山谷。</p>

<p>恒大为山谷接水接电，接通wifi，5000多人就地露营，宴席后的烟花放了半小时。</p>

<p>那夜，恒大订货成交119亿，许家印放言3年冲击世界500强。醉酒后他问部下“我怎样才能流芳百世？”</p>

<p>王健林目标则在更远处，万达将在2020年成为世界第一的旅游企业，超越迪士尼。</p>

<p>那年夏天，他正在豪买世界，说伦敦地价低得跟不要钱一样，说CNN若想被收购也可买下。</p>

<p>6月，他花2.65亿欧元，买下马德里地标西班牙大厦，坊间流传“王健林出国考察，顺便买了座大厦”。</p>

<p>那年夏天，流传更广其实是他儿子的名句，草根朋友生日他送出豪车，“我交朋友不在乎他有钱没钱，反正都没我有钱。”</p>

<p>十年前夏天金粉飘飞。网易为校招生开出年薪50万，中新网称大学生毕业三年后收入翻番。那年蚂蚁金服创立，余额宝风行，马云说阿里钱多是一种负担。</p>

<p>滴滴和快的大战发出40亿红包，高峰时一天烧掉1亿，烧得心惊肉跳，“如果把一亿元现金堆在一个屋子里烧，恐怕也得烧一整天吧”。</p>

<p>真烧钱的是李笑来，接受专访时，他对着镜头用美金点烟。</p>

<p>小苹果唱了一个盛夏，满街都是火火火火，范冰冰在马背上颠簸，扮演武则天，称再也不会有这个投资级别的大戏。</p>

<p>夏天时，华谊兄弟庆生20周年，宣称两年要赚100亿票房。那年纽约苏富比拍卖，王中军拍下梵高画的雏菊与罂粟花，耗资3.77亿元。</p>

<p>十年前盛夏，澎湃上线，发刊辞中，邱兵写道：</p>

<blockquote>
  <p>后来，嘈杂的年代就来了。我们从理想主义来到了消费主义，来到了精致的利己主义，我们迎来了无数的主义……那个夏夜，回忆起来，纠缠着，像无数个世纪。</p>

  <p>我心澎湃如昨。</p>
</blockquote>

<p><strong>03</strong></p>

<p>十年后的夏天，白宫宣布，对中国进口电动车增加关税到100%，企图以此扼杀产业。</p>

<p>中国商务部回应，这是“将经贸问题政治化、工具化，是典型的政治操弄”。</p>

<p>美国宣布提高关税后，意外受益者之一是滞留美国的贾跃亭。</p>

<p>法拉第未来股价，增幅1600%，虽然只是从0.04美元涨到0.7美元。十年后，他终于造出汽车，但只有11台，且全部因安全原因召回。</p>

<p>十年苍茫如海。贾跃亭没了乐视，罗永浩没了锤子，乌镇也已多年没了夜宴。</p>

<p>十年前宣称埋葬熊市的任泽平，去年在卖生发水。微博上，他展示生活节奏：晨跑三公里，一百个俯卧撑和卷腹，正心正念。</p>

<p>他的前老板许家印没能流芳百世，已被依法采取强制措施。另一位地产大佬王健林四处出售万达资产，距离年会唱《假行僧》也已八年。</p>

<p>五一前，王思聪在东京被网友偶遇，依旧开着豪华SUV，只是网友说“王思聪似乎比往常更加憔悴”。</p>

<p>当年和他录过节目的吴亦凡，已锒铛入狱，微博查无此人，上海杜莎夫人蜡像馆的真人蜡像也早已撤去。</p>

<p>当年蜡像揭幕时，吴亦凡受访说，他常和范冰冰等人聚会，“范冰冰说我像她孪生弟弟，很关照”。</p>

<p>范冰冰早已无暇关照她，更早跌进尘埃。去年现身新加坡国际电影节时，她说“我觉得好像人各有命”。</p>

<p>许多人的命运已经永久改变了。京东前CEO徐雷，在朋友圈说，周期和时代是两个截然不同的性质。</p>

<p>去年年底，国家公务员报考增加40万人，今年1179万毕业生中，排名最高的期待是进入国企。夸克报告中，上岸是2023年年轻人关键词。</p>

<p>一切转为务实和求稳。年轻人设闹铃抢大额存单，买货币基金“闷声发小财”，他们喜欢《繁花》里玲子说的：</p>

<blockquote>
  <p>“不是所有人都像你宝总做数学题一样地赚钱，大部分是像我一样，吭哧吭哧，一个硬币一个硬币地存起来的。”</p>
</blockquote>

<p>十年前的金粉早已飘散，3W咖啡店门紧闭，创业大街新宠是硬科技，投资人冲到高校实验室，抢投教授。</p>

<p>有投资机构说，值得欣慰的是，当下中小微企业数量还在增长。</p>

<p>狂飙已成往事，所有人都在适应新节奏。从日本归来的马云，在内部信中说，“时代变了，我们要跟上时代”。</p>

<p>今年年初，94岁的吴敬琏，发文称最重要不外乎两条：坚持改革，继续开放。他说：</p>

<blockquote>
  <p>只有一个办法，就是尽可能创造这样一种营商环境，使得个人和企业的千军万马能够往前冲，最终总有一些人和企业能够取得突破，那么就可以顺着这些突破的路径继续往前走。</p>
</blockquote>

<p>楼市风声鹤唳之际，因万科资金紧张，王石放弃了千万退休金。</p>

<p>他对行业依旧乐观，“房地产并未结束，而是刚刚开始”。</p>

<p>十年前的夏天，他带田朴珺去云南哀牢山，拜访褚时健。老人穿着圆领衫黑裤子，村口相候。</p>

<p>褚时健说，他也曾是年轻人，从新中国成立后到现在，社会变动很大，人生很多事，不是一条直线。国家要转型，始终要靠人来破解难题。</p>

<p>他说，困难多，搞好一点，信心就大一点，只有这样走，一步一步来。</p>]]></content><author><name></name></author><category term="reading" /><summary type="html"><![CDATA[Read Original]]></summary></entry><entry><title type="html">Wedding Speech</title><link href="http://localhost:4000/blog/2024/06/16/big_day.html" rel="alternate" type="text/html" title="Wedding Speech" /><published>2024-06-16T00:00:00+08:00</published><updated>2024-06-16T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/06/16/big_day</id><content type="html" xml:base="http://localhost:4000/blog/2024/06/16/big_day.html"><![CDATA[<h2 id="陈">陈:</h2>

<p>刚认识你时，记得漫威剧《Loki》正在更新，最开始对你的印象就是，我们就像平行宇宙中的不同变体——</p>

<p>我们的共同点很多：都喜欢皇马，都经常运动，都对生活和对方充满好奇。</p>

<p>随着了解加深，我发现我们也有很多的不同，你看过很多音乐剧，我听过很多摇滚乐，我陪你看了许多舞台剧，你也因为我有段时间热衷马拉松而开始喜欢长跑。</p>

<p>你让我觉得，人生是可以做着自己热爱的事来度过的。</p>

<p>未来还有很多事情值得探索，而你的陪伴也让我对这一切变得更加期待。</p>

<hr />

<p>今天场下在座的，以及台上站着的，都是我人生中最重要的人。</p>

<p>很难用语言来形容这个时刻有你们见证所感到的喜悦。</p>

<p>利用这个机会，我想先感谢我的爸妈，你们在我心里是最伟大的人，没有你们的辛勤付出，就没有我和弟弟的今天。你们以身作则，教会了我们诚实、善良、进取的品质；在那个充满未知和挑战的年代，你们把我们家从四川乡村的土墙房带到了大都市，让我和弟弟接受到了最好的教育，真正意义上实现了命运的改变。</p>

<p>我还要感谢我今天新多的爸妈，感谢你们培养了如此优秀、美丽、纯真的女儿。含辛茹苦三十载并不容易，将她护在自己的羽翼下快乐成长尤其困难。从此以后，我会倾尽所有照顾好她和我们的家。不让她受到一点委屈。</p>

<p>同时，我也要感谢从繁忙的工作和学业中赶来的弟弟和弟妹，未来在你们身上有着一切可能。以后无论身处何处，血浓于水的情感纽带都将始终将我们紧密连结。在这个变革的时代，让我们团结一心，共同努力，使这个大家庭更加繁荣和充满活力。</p>

<p>再次感谢大家来参加我们的婚礼，希望你们这几天都能玩得开心。</p>

<hr />

<h2 id="杨">杨:</h2>

<p>陈先生，你好。</p>

<p>我一直觉得，下定决心结婚是非常困难的一件事。要跟这个人朝夕相处几十年，要从今天开始以后每天都面对面在枕头上喷气儿，要共同养育一个孩子，甚至更多。甚至，你还拥有我在手术台上昏迷不醒时决定我要不要继续手术的权利，相当于我得把一半的命给你，我怎么能轻而易举的交出自己的命呢？</p>

<p>所以，陈先生，我必然不会因为你是高富帅而嫁给你，当然，这三个字也不太是用来形容你。而是因为你善良，温厚，努力上进，真实简单吗？这是我想到的夸赞你的话，但这也不是作为我爱你的理由。</p>

<p>比起这些所有人可见的美好品质，我更珍惜你的尊重，支持和包容，以及和你在一起后每一天相处的点滴，都让我深信不疑，这个人我愿意和他在一起。</p>

<p>这可能就是我今天站在这里的原因。我们在如此平凡琐碎的生活中，拥有对方有趣的灵魂，拥有彼此的绝对信任，拥有自由，平等和尊重。拥有对方无条件的鼓励和支持。</p>

<p>以前我一直觉得，一个人能过得很开心，但有了你才发现，两个人原来可以更好。</p>

<p>我爱的，是我们拥有各自的光芒，同时也依赖对方。</p>

<p>谢谢你，让我期待和你一起的未来。</p>

<p>我要我们不止新婚快乐，我要我们一直都快乐。</p>

<hr />

<p>今天算是我人生中很重要的时刻，对于我这种时常心怀感激却不善于表露的人来说也是一个表达谢意的绝佳机会。</p>

<p>首先最想感谢的是我的爸爸妈妈，谢谢你们30年来的养育，在能力范围内创造最好的条件把我培养长大，让我在爱的环境里健康成长，感谢你们包容了我每一个特立独行的勇气和决定。</p>

<p>我是这个世界上很普通的女儿，经常会惹你们生气失望，但你们在我眼里是最好的爸爸妈妈，我从未让你们骄傲，你们却待我如宝。感谢你们陪着我一起长大。</p>

<p>有人说过:“父母存在的意义不是给予孩子多舒适和富裕的生活，而是当你想到你的父母时，你的内心就会充满力量会感受到温暖，从而拥有克服困难的勇气和能力，因此获得人生真正的乐趣和自由。”</p>

<p>好巧，我跟陈都是这么幸运和幸福的人。我们都有对我们最好的爸爸妈妈，并且从今天开始，我们的幸运和幸福翻倍。未来，就让我们俩一起陪伴你们，保护你们。</p>

<p>最后，两位爸爸，父亲节快乐!</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[陈:]]></summary></entry><entry><title type="html">10 rules for financial success</title><link href="http://localhost:4000/blog/2024/06/09/financial_success.html" rel="alternate" type="text/html" title="10 rules for financial success" /><published>2024-06-09T00:00:00+08:00</published><updated>2024-06-09T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/06/09/financial_success</id><content type="html" xml:base="http://localhost:4000/blog/2024/06/09/financial_success.html"><![CDATA[<ol>
  <li>Budget</li>
  <li>Buy assets</li>
  <li>Pay off debt</li>
  <li>Invest in yourself</li>
  <li>Add income streams</li>
  <li>Refuse lifestyle creep</li>
  <li>Cut unneeded expenses</li>
  <li>Invest 20% of your income</li>
  <li>Be with business-minded people</li>
  <li>Marry someone who understands this</li>
</ol>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Budget Buy assets Pay off debt Invest in yourself Add income streams Refuse lifestyle creep Cut unneeded expenses Invest 20% of your income Be with business-minded people Marry someone who understands this]]></summary></entry><entry><title type="html">我的密码管理策略</title><link href="http://localhost:4000/blog/2024/04/01/my_password_management_strategy.html" rel="alternate" type="text/html" title="我的密码管理策略" /><published>2024-04-01T00:00:00+08:00</published><updated>2024-04-01T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/04/01/my_password_management_strategy</id><content type="html" xml:base="http://localhost:4000/blog/2024/04/01/my_password_management_strategy.html"><![CDATA[<p>随着现代人网上冲浪时间越来越长，有一件事显得愈发重要，但又常常被人忽略，那就是各类网站的密码管理。</p>

<p>如今我们注册的网站品类纷繁复杂，有的类似新闻资讯，账密丢失被盗都无足轻重，有的则涉及个人财产或社交网络，一旦被盗会给自己或朋友带来很多困扰。好在目前一些涉及到财务的网站或应用会开启设备的二次验证，但也不能涵盖我们上网情景的方方面面。</p>

<p>有人为了避免密码遗忘，把所有平台的密码设置成同一个，这样账号（通常是手机号码或邮箱）密码只要在一个平台被泄露，带有不良意图的技术人员就可以去各种网站 <strong>“<a href="https://baike.baidu.com/item/%E6%92%9E%E5%BA%93/16480882">撞库攻击</a>”</strong>。泄露的原因当然不止是自身无意暴露，当你提交注册一个网站，包含你个人信息的表单就被提交到对方服务器中，对于你来说对方是否遵守职业操守，是否合理保管你的密码，甚至是否会对你的个人信息加密存储都不一定。所以，所有平台使用一种账号密码无疑是一件十分危险的事。</p>

<p>那么如果你给每个平台都设置一个独特的密码，又应该如何去管理这些信息？一个选项是交给业内成熟的密码管理工具，不少人熟知的 <a href="https://1password.com/">1Password</a> 就是其中之一，这个工具我使用过一年，各平台同步，自动填充账号密码，确实方便。可当你对个人信息极度敏感，1Password 这种商业公司也无法让你完全信任，那应该怎么办？</p>

<p>首先针对密码管理我们要知道，我们唯一需要的功能就是一个加密的数据库，用以保管我们的所有密码，1Password 承担替我们保管这个加密数据库的职责，并收取年费。那么如果有一个开源的技术，将这个加密数据库放在我们自己的磁盘，通过记住一个密码就可以访问所有密码，而且又免费，是不是就很完美呢？</p>

<p>好在我们处在互联网，<strong>你的 99% 的需求都曾经有其他人提出，并且被具有开源精神的极客们满足了</strong>。所以问题就在于知道自己想要什么，并能获取到需要的信息。</p>

<h3 id="keepassxc">KeePassXC</h3>
<p><a href="https://keepassxc.org/">KeePassXC</a> 这款工具满足了以上大部分需求，通过它，你可以在本地创建一个后缀 .kdbx的文件作为密码数据库，通过桌面客户端打开文件，输入预制的一段密码，即可操作其中的 entry。在这个工具中，一个文件相当于一个 Database，一条账密信息作为一个 Entry，可以通过配置自动备份一个本次修改以前的文件。</p>

<p>在电脑桌面端，可以在主流浏览器安装相应<a href="https://keepassxc.org/download/#browser">插件</a>， 并通过对插件的<a href="extension://pdffhmdngciaglkoonimfcmckehcpafo/options/options.html#general-settings">配置</a>，当 Database 文件处于解锁状态时，浏览器访问到特定页面可以自动填充账号密码。</p>

<p>唯一鸡肋的一点就是在移动端暂时没有支持。</p>

<p>通过 KeePassXC + 主流云盘（OneDrive、Google Cloud、iCloud、DropBox）配合使用，可以完成跨平台密码同步。</p>

<p>说到云平台，我们可以如何最大程度利用各大厂商免费云盘为自己服务的同时又保证自己的隐私得到保护呢？</p>

<h3 id="cryptomator">Cryptomator</h3>
<p><a href="https://cryptomator.org/">Cryptomator</a> 是一个使用符合最新标准的加密技术，对本地或云上数据进行加密的工具。正如其官网描述的那样： <strong>Put a lock on your cloud</strong>.</p>

<blockquote>
  <p>With Cryptomator, the key to your data is in your hands. Cryptomator encrypts your data quickly and easily. Afterwards you upload them protected to your favorite cloud service.</p>
</blockquote>

<p>你需要指定一个路径作为 vault，通过输入密码进入 vault，你会看到一个标准的文件系统，在其中你可以放置你的个人文件，而对于服务器管理员或有权限读取当前电脑文件系统的程序而言，所有的内容都将只是无法理解的加密文件。</p>

<p>Give it a try, you won’t regret it.</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[随着现代人网上冲浪时间越来越长，有一件事显得愈发重要，但又常常被人忽略，那就是各类网站的密码管理。 如今我们注册的网站品类纷繁复杂，有的类似新闻资讯，账密丢失被盗都无足轻重，有的则涉及个人财产或社交网络，一旦被盗会给自己或朋友带来很多困扰。好在目前一些涉及到财务的网站或应用会开启设备的二次验证，但也不能涵盖我们上网情景的方方面面。 有人为了避免密码遗忘，把所有平台的密码设置成同一个，这样账号（通常是手机号码或邮箱）密码只要在一个平台被泄露，带有不良意图的技术人员就可以去各种网站 “撞库攻击”。泄露的原因当然不止是自身无意暴露，当你提交注册一个网站，包含你个人信息的表单就被提交到对方服务器中，对于你来说对方是否遵守职业操守，是否合理保管你的密码，甚至是否会对你的个人信息加密存储都不一定。所以，所有平台使用一种账号密码无疑是一件十分危险的事。 那么如果你给每个平台都设置一个独特的密码，又应该如何去管理这些信息？一个选项是交给业内成熟的密码管理工具，不少人熟知的 1Password 就是其中之一，这个工具我使用过一年，各平台同步，自动填充账号密码，确实方便。可当你对个人信息极度敏感，1Password 这种商业公司也无法让你完全信任，那应该怎么办？ 首先针对密码管理我们要知道，我们唯一需要的功能就是一个加密的数据库，用以保管我们的所有密码，1Password 承担替我们保管这个加密数据库的职责，并收取年费。那么如果有一个开源的技术，将这个加密数据库放在我们自己的磁盘，通过记住一个密码就可以访问所有密码，而且又免费，是不是就很完美呢？ 好在我们处在互联网，你的 99% 的需求都曾经有其他人提出，并且被具有开源精神的极客们满足了。所以问题就在于知道自己想要什么，并能获取到需要的信息。 KeePassXC KeePassXC 这款工具满足了以上大部分需求，通过它，你可以在本地创建一个后缀 .kdbx的文件作为密码数据库，通过桌面客户端打开文件，输入预制的一段密码，即可操作其中的 entry。在这个工具中，一个文件相当于一个 Database，一条账密信息作为一个 Entry，可以通过配置自动备份一个本次修改以前的文件。 在电脑桌面端，可以在主流浏览器安装相应插件， 并通过对插件的配置，当 Database 文件处于解锁状态时，浏览器访问到特定页面可以自动填充账号密码。 唯一鸡肋的一点就是在移动端暂时没有支持。 通过 KeePassXC + 主流云盘（OneDrive、Google Cloud、iCloud、DropBox）配合使用，可以完成跨平台密码同步。 说到云平台，我们可以如何最大程度利用各大厂商免费云盘为自己服务的同时又保证自己的隐私得到保护呢？ Cryptomator Cryptomator 是一个使用符合最新标准的加密技术，对本地或云上数据进行加密的工具。正如其官网描述的那样： Put a lock on your cloud. With Cryptomator, the key to your data is in your hands. Cryptomator encrypts your data quickly and easily. Afterwards you upload them protected to your favorite cloud service. 你需要指定一个路径作为 vault，通过输入密码进入 vault，你会看到一个标准的文件系统，在其中你可以放置你的个人文件，而对于服务器管理员或有权限读取当前电脑文件系统的程序而言，所有的内容都将只是无法理解的加密文件。 Give it a try, you won’t regret it.]]></summary></entry><entry><title type="html">How to search information efficiently</title><link href="http://localhost:4000/blog/2024/03/31/how_to_search_information_efficiently.html" rel="alternate" type="text/html" title="How to search information efficiently" /><published>2024-03-31T00:00:00+08:00</published><updated>2024-03-31T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/03/31/how_to_search_information_efficiently</id><content type="html" xml:base="http://localhost:4000/blog/2024/03/31/how_to_search_information_efficiently.html"><![CDATA[<p>日常搜索需求主要包含：信息咨询；知识技能；素材文件；工具软件。</p>

<h3 id="为什么搜">为什么搜</h3>
<p>know something；learn something；create something；do something</p>

<h3 id="搜什么">搜什么</h3>
<ol>
  <li>信息资讯：新闻、事件；</li>
  <li>知识技能：概念、教程；</li>
  <li>素材文件：图片、视频、文档；</li>
  <li>工具软件：工具、插件。</li>
</ol>

<h3 id="去哪里搜">去哪里搜</h3>
<p>搜索效率：谷歌 &gt; 公众号 &gt; 短视频 &gt; 百度</p>

<h3 id="怎么搜">怎么搜</h3>
<ol>
  <li>“word” 限定关键词</li>
  <li>intitle: word 限定标题关键词</li>
  <li>allintitle: word1 word2 联合标题关键词</li>
  <li>intext: word 限定文章内容关键词</li>
  <li>site: website 限定网站</li>
  <li>inurl: word 限定域名关键词</li>
  <li>imagesize: width x length 限定图片尺寸</li>
  <li>filetype: extension 限定结果的文件格式</li>
</ol>

<p>去信息源头获取一手信息，掌握最大化利用AI的能力。</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[日常搜索需求主要包含：信息咨询；知识技能；素材文件；工具软件。 为什么搜 know something；learn something；create something；do something 搜什么 信息资讯：新闻、事件； 知识技能：概念、教程； 素材文件：图片、视频、文档； 工具软件：工具、插件。 去哪里搜 搜索效率：谷歌 &gt; 公众号 &gt; 短视频 &gt; 百度 怎么搜 “word” 限定关键词 intitle: word 限定标题关键词 allintitle: word1 word2 联合标题关键词 intext: word 限定文章内容关键词 site: website 限定网站 inurl: word 限定域名关键词 imagesize: width x length 限定图片尺寸 filetype: extension 限定结果的文件格式 去信息源头获取一手信息，掌握最大化利用AI的能力。]]></summary></entry></feed>