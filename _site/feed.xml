<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-14T11:33:41+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">thekingofcool’s website</title><entry><title type="html">延迟退休可能是对一代人违约</title><link href="http://localhost:4000/blog/2024/09/14/delaying_retirement.html" rel="alternate" type="text/html" title="延迟退休可能是对一代人违约" /><published>2024-09-14T00:00:00+08:00</published><updated>2024-09-14T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/09/14/delaying_retirement</id><content type="html" xml:base="http://localhost:4000/blog/2024/09/14/delaying_retirement.html"><![CDATA[<p><a href="http://jingji.cntv.cn/2013/04/20/ARTI1366415217203221.shtml">Read Original</a></p>

<table>
  <tbody>
    <tr>
      <td>date_saved: 2024-09-14 06:59:46</td>
      <td>date_published:2013-04-20 08:00:00</td>
    </tr>
  </tbody>
</table>

<p>加速到来的人口老龄化又一次把延长退休年龄推向了风口浪尖。全国社保基金理事会党组书记戴相龙日前受访时表示，面对30多年后的人口老龄化高峰，国家管理的公共养老金收支会有较大缺口。他建议应逐步延长退休年龄，提出采取每5年把退休年龄延长1岁的制度设计。专家称阶梯式、渐进式的延长退休年龄是国际上通行的做法。</p>

<p>相比此前“一步到位”的激进方案，“5年延1岁”的设计温和多了。不过这是一个容易撕裂社会情感的话题，争议和阻力并未因方案的温和而减少。这种撕裂从不同阶层的态度就可以看出来。从舆情分析看，支持延迟退休的主要是两大社会群体：官员和专家。其他阶层则多数反对延迟退休，辛苦了一辈子快到领退休金安享天年的时候，突然要延迟退休，心理上接受不了。</p>

<p>分析支持者和反对者可以看到一个有意思的现象，支持延迟退休者，大多是既有养老体制的受益者，而反对者则多是相对被剥夺群体，甚至是受损者。中国在养老上实行的是双轨制，机关和事业单位发养老金，而企业单位是自己缴养老保险，从企业退休领到的养老金往往比从机关退休少得多。官员和专家支持延迟退休方案，因为这不仅不触动他们的既得利益，还能给他们带来利益；多数公众之所以反对，是因为这种改革没有触动他们最反对的养老双轨制。人们不患寡而患不均，养老账户的空账问题虽然严重，但他们可以接受一个低的养老金，可不能接受有些人比自己高那么多。人们其实不是反对延迟退休，而更多是反感政府在双轨制改革上对民意的漠视，动不了官员和专家，就拣软柿子捏。</p>

<p>公众最大的期待是养老双轨制的并轨，把每个国民一起置于平等的体制下，先解决平等问题，再解决空账问题。而“延迟退休”则回避了这个核心问题，这正是作为双轨制受益者的官员和专家所期待的。</p>

<p>延迟退休对官员和专家是有益的，官员大多希望延迟退休，因为延迟的不仅是工作，更是权力利益。按现在的退休年龄，一般官员到了60岁就得退，很多人都不适应这种退休后手中无权的落寞感，延迟退休则延长了他们的政治生命。专家也是如此，大学和科研院所多已高度行政化，是官场的翻版，退休的院长和教授自然比不上在位的。但对普通劳动者而言，工作则是一种负担，没有权力利益，辛辛苦苦熬了一辈子好不容易熬到了退休，却又赶上了延迟退休。延迟退休可能会让这些人产生一种双重的双损感：双轨制已让他们受损害，延迟退休更进一步伤害了他们的利益。</p>

<p>从另一个角度看，延迟退休可能是对一代人的违约。制度和政策应该保持一定的稳定性，尤其是这个政策涉及大的公众利益时，应给公众一个稳定的预期。什么年龄退休，什么时候能拿到养老金，是国家对国民的一种承诺和约定，不能轻易打破这样的契约。不能以“延长退休年龄是国际惯例”作为打破契约的借口，发达国家延长退休年龄是经由正当的法律程序和民主途径与国民协商而订立的契约，不能想当然和随意地改变。即使因为空账问题需要调整，也应经过民主决策，让每个利益群体都参与到博弈中。</p>

<p>而且，不能一说到养老金缺口，就把所有责任都转嫁给社会和公众，在提起延迟退休这个议题时，首先要追问政府在养老金问题上有没有承担应有的投入，履行好应有的政府保障责任。毕竟，公众纳的税不是白纳的，里面有对自己未来养老的一份投资。</p>

<p>在养老问题上，改革的次序应该是，先改掉双轨制，再谈延迟退休；先加大政府投入，再谈公众责任。</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Read Original]]></summary></entry><entry><title type="html">On Elon’s Comments on Taylor Swift’s Endorses Kamala Harris</title><link href="http://localhost:4000/blog/2024/09/11/elon_comments_on_taylor.html" rel="alternate" type="text/html" title="On Elon’s Comments on Taylor Swift’s Endorses Kamala Harris" /><published>2024-09-11T00:00:00+08:00</published><updated>2024-09-11T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/09/11/elon_comments_on_taylor</id><content type="html" xml:base="http://localhost:4000/blog/2024/09/11/elon_comments_on_taylor.html"><![CDATA[<p>看了绝大多数<a href="https://weibo.com/1893801487/OwtSU6V57">简中网友解读</a>的马斯克 X 发言都不全面或者说是错的。</p>
<blockquote>
  <p>Fine Taylor … you win … I will give you a child and guard your cats with my life</p>
</blockquote>

<p>首先这是回应泰勒在 Instagram 宣称自己将会在 2024 年总统大选给哈里斯投票，并署名 Childless Cat Lady 的发文。</p>
<blockquote>
  <p>Like many of you, I watched the debate tonight. If you haven’t already, now is a great time to do your research on the issues at hand and the stances these candidates take on the topics that matter to you the most. As a voter, I make sure to watch and read everything I can about their proposed policies and plans for this country.</p>

  <p>Recently I was made aware that Al of ‘me’ falsely endorsing Donald Trump’s presidential run was posted to his site. It really conjured up my fears around Al, and the dangers of spreading misinformation. It brought me to the conclusion that I need to be very transparent about my actual plans for this election as a voter. The simplest way to combat misinformation is with the truth.</p>

  <p>I will be casting my vote for Kamala Harris and Tim Walz in the 2024 Presidential Election. I’m voting for @kamalaharris because she fights for the rights and causes I believe need a warrior to champion them. I think she is a steady-handed, gifted leader and I believe we can accomplish so much more in this country if we are led by calm and not chaos. I was so heartened and impressed by her selection of running mate @timwalz, who has been standing up for LGBTQ+ rights, IVF, and a woman’s right to her own body for decades.</p>

  <p>I’ve done my research, and l’ve made my choice. Your research is all yours to do, and the choice is yours to make. I also want to say, especially to first time voters: Remember that in order to vote, you have to be registered! I also find it’s much easier to vote early. I’ll link where to register and find early voting dates and info in my story.</p>

  <p>With love and hope,</p>

  <p>Taylor Swift</p>

  <p>Childless Cat Lady</p>
</blockquote>

<p>这个署名是在回应先前特朗普副手 Vance 攻击哈里斯时表示美国就是被这些没孩子的养猫女人搞乱的。</p>
<blockquote>
  <p>We’re effectively run in this country via the Democrats Bea, via our corporate oligarchs, by a bunch of childless cat ladies who are miserable at their own lives and the choices that they’ve made, and so they want to make the rest of the country miserable too.</p>
</blockquote>

<p>马斯克今年旗帜鲜明地站台特朗普，他认为自己的第一个孩子也是因为受到觉醒文化影响，成年后选择变性并和马斯克断绝了关系。</p>

<p>马斯克回应泰勒说好吧，你不是没孩子吗（自认为幽默地混淆 <em>不想要孩子且没有</em> 和 <em>想要孩子但没有</em>），我给你个孩子（一起生一个），而且我还会用生命照顾好你的猫。中年男人的油腻感扑面而来。</p>

<p>抛开这些事件的戏剧性不说，从两个党派，到社会名人，再到这些人背后的一众簇拥，可以说美国内部对立情绪越来越浓。非法移民继续涌入，社会治安持续恶化，贫富差距不断加大。如果经济按衰退预期走，到一定的程度美国可能会需要一场战争来唤起美国的团结。</p>

<p>而中国目前正陷于房地产泡沫破灭，老龄少子化，地方财政捉襟见肘。届时可能也需要一个事件转移全社会的矛盾。</p>

<p>作为普通人，只有好好锻炼身体，照顾好家人，学习一切有趣的东西，然后祈祷科技变革的发生早于社会变革。</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[看了绝大多数简中网友解读的马斯克 X 发言都不全面或者说是错的。 Fine Taylor … you win … I will give you a child and guard your cats with my life]]></summary></entry><entry><title type="html">Be a Good Data Engineer - Spark</title><link href="http://localhost:4000/blog/2024/09/06/spark.html" rel="alternate" type="text/html" title="Be a Good Data Engineer - Spark" /><published>2024-09-06T00:00:00+08:00</published><updated>2024-09-06T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/09/06/spark</id><content type="html" xml:base="http://localhost:4000/blog/2024/09/06/spark.html"><![CDATA[<h3 id="background">Background</h3>
<p>进入21世纪以来，随着互联网的发展，各类社交媒体，科技软件，传感器等工具一刻不停地生成着越来越多地数据。有一个说法是:</p>
<blockquote>
  <p>人类现有90%的数据来自于过去两年。</p>
</blockquote>

<p>这些数据不管是否得到了很好的利用，谁也不能否认它们可以带来的高价值，特别是现阶段以数据和能源作为养料的 artificial intelligence。</p>

<p>大数据集对传统单机数据库造成了不小的麻烦，面对如何处理大量数据并从中洞见到有价值信息的需求，Google 三篇论文——<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/gfs-sosp2003.pdf">Google File System</a> at 2003, <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/mapreduce-osdi04.pdf">MapReduce</a> at 2004, <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable</a> at 2006 启发了无数贡献者，推出了一个又一个大数据开源项目，为人类掀开了大数据时代的巨幕。</p>

<p>其中，受 Google File System 和 MapReduce 启发，yahoo 的一组工程师在 2006 年开源了 <a href="https://hadoop.apache.org/">Hadoop</a>， Hadoop 引入了两个技术，分布式存储 (HDFS) 和分布式计算 (MapReduce) ——将大文件切分成小块，分布保存在集群中的多个机器上，每个小块文件备份成多份以防某个节点出错导致文件丢失；处理计算任务时，也将任务分成多个单元，由多个 excutor 分别执行计算操作，再将结果合并在一起。</p>

<p>这种处理方法利用大量廉价机器使得大数据计算得以实现。但由于其内在机制限制，大量的中间计算结果需要落地磁盘，过程中产生的 I/O 导致整个计算花费大量的时间。随着计算机硬件水平和成本的降低，Spark 作为一个内存计算引擎，凭借其更加高效的计算的优势逐渐取代了 MapReduce 的功能。</p>

<h3 id="apache-spark">Apache Spark</h3>
<h4 id="intro">Intro</h4>
<p><a href="https://spark.apache.org/">Apache Spark</a> 是由加州大学伯克利分校的一些研究员在 2009 年推出的一个研究项目，目的就是为了解决 Hadoop 的上述限制。Spark 推出了一个 RDD(Resilient Distributed Dataset) 的概念，使数据得以数据集的形式存储在内存中，使得数据读取和处理都更加快速。</p>

<h4 id="language">Language</h4>
<p>Spark 源码是由 Scala 语言编写，但提供了 Python, Scala, Java, R 的 API 接口进行编程。Python 由于其易用性、丰富的资源库以及和 Data Science, Machine Learning 的紧密结合，其已经成为 Spark 主推的编程语言。下面的演示都以 Python 为例。</p>

<h4 id="installation">Installation</h4>
<p>Make sure java is installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>default-jre
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>pyspark
</code></pre></div></div>

<h4 id="concepts">Concepts</h4>
<h5 id="spark-任务的运行模式">Spark 任务的运行模式</h5>
<ul>
  <li>Local Mode: 单机运行，资源有限，适合用于手动调试；</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit word_count.py

python word_count.py

spark-submit <span class="nt">--master</span> <span class="nb">local</span><span class="o">[</span><span class="k">*</span><span class="o">]</span> word_count.py
</code></pre></div></div>

<ul>
  <li>Client Mode: 任务的 Driver 在本地运行，实际计算任务分发到集群中的 Worker 节点运行，由于 Driver 在本地运行，可以方便地查看日志和调试信息；</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit <span class="nt">--master</span> yarn <span class="nt">--deploy-mode</span> client word_count.py

spark-submit <span class="nt">--master</span> spark://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> client word_count.py

spark-submit <span class="nt">--master</span> k8s://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> client word_count.py
</code></pre></div></div>

<ul>
  <li>Cluster Mode: 提交任务到分布式集群运行，可以利用更高的计算性能，适用于生产环境；</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit <span class="nt">--master</span> yarn <span class="nt">--deploy-mode</span> cluster word_count.py

spark-submit <span class="nt">--master</span> spark://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> cluster word_count.py

spark-submit <span class="nt">--master</span> k8s://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> cluster word_count.py
</code></pre></div></div>

<h5 id="spark-资源管理器">Spark 资源管理器</h5>
<p>Spark 资源管理器负责分配和调度集群资源(CPU, Memory, Disk, Network)</p>
<ol>
  <li><a href="https://spark.apache.org/docs/latest/spark-standalone.html">Standalone</a>: 默认的资源管理器；</li>
  <li><a href="https://spark.apache.org/docs/latest/running-on-yarn.html">YARN</a>: 由 Hadoop 提供；</li>
  <li><a href="https://spark.apache.org/docs/latest/running-on-mesos.html">Mesos</a>(Deprecated): 由 Apache Mesos 提供;</li>
  <li><a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html">Kubernetes</a>: 由 Kubernetes 提供</li>
</ol>

<h5 id="spark-任务物理运行原理">Spark 任务物理运行原理</h5>
<p><strong>1. 客户端提交任务</strong></p>

<p>用户通过 spark-submit 命令提交一个 Spark 应用程序。这个应用程序包含了用户的代码和依赖项。</p>

<p><strong>2. Driver 进程启动</strong></p>

<p>Spark 应用程序在 Driver 进程中启动。Driver 负责以下任务：</p>
<ul>
  <li>解析用户代码: 解析并执行用户代码中的 transformation 和 action 操作；</li>
  <li>生成 DAG: 将用户代码中的一系列 transformation 操作转换为一个有向无环图（DAG）；</li>
  <li>任务调度: 将 DAG 划分为多个阶段（stages），每个阶段包含一组可以并行执行的任务（tasks）。</li>
</ul>

<p><strong>3. 资源管理器分配资源</strong></p>

<p>Driver 向集群的资源管理器（如 YARN、Mesos 或 Kubernetes）请求资源。资源管理器分配资源并启动 Executor 进程。</p>

<p><strong>4. Executor 进程启动</strong></p>

<p>Executor 进程在集群的工作节点（Worker Nodes）上启动。每个 Executor 负责以下任务：</p>
<ul>
  <li>执行任务: 执行由 Driver 分配的任务；</li>
  <li>存储数据: 缓存和存储中间结果数据；</li>
  <li>报告状态: 向 Driver 报告任务的执行状态和结果。</li>
</ul>

<p><strong>5. 任务执行</strong></p>

<p>Driver 将任务分配给各个 Executor。任务的执行过程如下：</p>
<ul>
  <li>读取数据: 从数据源（如 HDFS、S3、Kafka 等）读取数据；</li>
  <li>执行计算: 根据用户代码中的 transformation 操作对数据进行处理；</li>
  <li>写入结果: 将计算结果写入到指定的存储位置（如 HDFS、S3、数据库等）。</li>
</ul>

<p><strong>6. 任务监控和容错</strong></p>

<p>Spark 提供了多种机制来监控和处理任务的执行：</p>
<ul>
  <li>任务重试: 如果某个任务失败，Spark 会自动重试该任务；</li>
  <li>数据备份: Spark 会将数据分片（partitions）备份到多个节点，以防止数据丢失；</li>
  <li>监控工具: Spark 提供了 Web UI 和其他监控工具，帮助用户监控任务的执行状态和性能。</li>
</ul>

<p><strong>7. 任务完成</strong></p>

<p>当所有任务都成功完成后，Driver 会将最终结果返回给用户或写入到指定的存储位置。然后，Driver 和 Executor 进程会正常退出，释放资源。</p>

<h4 id="usage-scenarios">Usage scenarios</h4>
<h5 id="spark-sql">Spark SQL</h5>
<p><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> 是 Spark 用于处理结构化数据的模块，它提供了一个编程抽象，支持 SQL 查询、Dataframe API 和 Dataset API，而不需要了解底层分布式计算的细节。</p>

<p>它与 <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">RDD</a> 的不同在于：</p>
<ol>
  <li>RDD 提供了底层的 <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations">RDD API</a>，用户需要编写更多的代码来进行数据处理，适合处理非结构化数据；Spark SQL 将数据抽象为 <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html">Dataframe</a> 或 Dataset，提供了更高级的优化和执行策略；</li>
  <li>RDD 类型安全，DataFrame API 是非类型安全的，Dataset API 提供了类型安全的操作；</li>
  <li>Spark SQL 支持 <a href="https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html">Hive</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-avro.html">Avro</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html">Parquet</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-orc.html">ORC</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-json.html">JSON</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html">JDBC</a> 等多种数据源，而 RDD 需要用户自己实现对不同数据源的支持；</li>
</ol>

<p><strong>使用 Dataframe API 案例</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import necessary libraries
</span><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="n">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="n">regexp_replace</span>

<span class="c1"># Create a Spark session
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">SparkSQLExample</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="c1"># Read CSV data into a DataFrame
</span><span class="n">ark_holdings_df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">csv</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="sh">"</span><span class="s">test_file/ark_holdings.csv</span><span class="sh">"</span><span class="p">,</span> 
                    <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                    <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Perform a query
</span><span class="n">ark_holdings_df</span> <span class="o">=</span> <span class="n">ark_holdings_df</span> \
    <span class="p">.</span><span class="nf">withColumn</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">,</span> <span class="nf">regexp_replace</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">),</span> <span class="sh">"</span><span class="s">[$,]</span><span class="sh">"</span><span class="p">,</span> <span class="sh">""</span><span class="p">).</span><span class="nf">cast</span><span class="p">(</span><span class="sh">"</span><span class="s">double</span><span class="sh">"</span><span class="p">))</span>
<span class="n">result_df</span> <span class="o">=</span> <span class="n">ark_holdings_df</span> \
    <span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">ark_holdings_df</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">ticker</span><span class="sh">"</span><span class="p">)</span> <span class="o">==</span> <span class="sh">"</span><span class="s">U</span><span class="sh">"</span><span class="p">).</span><span class="nf">select</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">)).</span><span class="nf">first</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> \
    <span class="p">.</span><span class="nf">select</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">ticker</span><span class="sh">"</span><span class="p">),</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">))</span> \
    <span class="p">.</span><span class="nf">orderBy</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">).</span><span class="nf">desc</span><span class="p">())</span>

<span class="c1"># Show the result
</span><span class="n">result_df</span><span class="p">.</span><span class="nf">show</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Stop the Spark session
</span><span class="n">spark</span><span class="p">.</span><span class="nf">stop</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>使用 RDD API 案例</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import necessary libraries
</span><span class="kn">from</span> <span class="n">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>

<span class="c1"># Create a Spark context
</span><span class="n">sc</span> <span class="o">=</span> <span class="nc">SparkContext</span><span class="p">(</span><span class="sh">"</span><span class="s">local</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">RDDExample</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Read CSV data into an RDD
</span><span class="n">ark_holdings_rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="nf">textFile</span><span class="p">(</span><span class="sh">"</span><span class="s">test_file/ark_holdings.csv</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Perform a query
</span><span class="n">result_rdd</span> <span class="o">=</span> <span class="n">ark_holdings_rdd</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="sh">"</span><span class="s">UNITY</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">line</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'"'</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">))</span>

<span class="c1"># Show the result
</span><span class="nf">print</span><span class="p">(</span><span class="n">result_rdd</span><span class="p">.</span><span class="nf">collect</span><span class="p">())</span>

<span class="c1"># Stop the Spark context
</span><span class="n">sc</span><span class="p">.</span><span class="nf">stop</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>RDD 和 Dataframe 相互转换</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Dataframe 转 RDD
</span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">csv</span><span class="p">(</span><span class="sh">"</span><span class="s">test_file/ark_holdings.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">rdd</span>

<span class="c1"># RDD 转 Dataframe
</span><span class="n">rdd</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="nf">parallelize</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">Alice</span><span class="sh">"</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="sh">"</span><span class="s">Bob</span><span class="sh">"</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="sh">"</span><span class="s">Cathy</span><span class="sh">"</span><span class="p">)])</span>
<span class="c1"># 定义 DataFrame 的 schema
</span><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nc">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])).</span><span class="nf">toDF</span><span class="p">()</span>

</code></pre></div></div>
<p><strong>Action 算子</strong></p>

<p>Spark 有一个特性 Lazy Evaluation，当一个 Spark 操作被提交时，Spark 并不会立即执行任务，而是将任务转换为一系列的 RDD 操作，只有当遇到 Action 算子时，Spark 才会真正执行任务。通过 Lazy Evaluation, Spark 可以优化执行计划，避免存储大量中间结果，提高任务的执行效率。</p>

<p>常见的 RDD action 算子有：collect, count, first, take, takeSample, reduce, fold, aggregate, foreach, saveAsTextFile, saveAsSequenceFile, saveAsObjectFile, countByKey, countByValue, takeOrdered, top, foreachPartition.</p>

<p>常见的 DataFrame action 算子有：collect, count, first, take, show, head, foreach, write, describe, summary, toPandas.</p>

<p>总的来说，Spark SQL 更适合处理结构化数据和需要高效查询优化的场景，而 RDD 更适合处理非结构化数据和需要自定义处理逻辑的场景。</p>

<p>需要特别注意的是，Spark SQL 性能调优需要考虑的因素很多，包括数据倾斜、数据分区、数据格式、数据压缩、数据缓存等，根据实际情况参考 <a href="https://spark.apache.org/docs/latest/sql-performance-tuning.html">Spark SQL 性能调优</a> 进行调整。</p>

<h5 id="pandas-api-on-spark">Pandas API on Spark</h5>
<p><a href="https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html#">Pandas API on Spark</a> 是 Spark 提供的用于处理大规模数据集的 API，它提供了与 <a href="https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html">Pandas</a> 类似的 API，使得用户可以方便地将 Pandas 的代码迁移到 Spark 上运行。</p>

<p><strong>Pandas API on Spark 案例</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import necessary libraries
</span><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Create a Spark session
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">PandasAPISpark</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="c1"># Create a DataFrame
</span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="sh">"</span><span class="s">Alice</span><span class="sh">"</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">Bob</span><span class="sh">"</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">Cathy</span><span class="sh">"</span><span class="p">,</span> <span class="mi">35</span><span class="p">)]</span>
<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="nf">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>

<span class="c1"># Convert DataFrame to Pandas DataFrame
</span><span class="n">pandas_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">toPandas</span><span class="p">()</span>

<span class="c1"># Perform operations on Pandas DataFrame
</span><span class="n">pandas_df</span><span class="p">[</span><span class="sh">'</span><span class="s">age_12_years_ago</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pandas_df</span><span class="p">[</span><span class="sh">'</span><span class="s">age</span><span class="sh">'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">12</span>

<span class="c1"># Convert Pandas DataFrame back to Spark DataFrame
</span><span class="n">spark_df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="nf">createDataFrame</span><span class="p">(</span><span class="n">pandas_df</span><span class="p">)</span>

<span class="c1"># Show the result
</span><span class="n">spark_df</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Stop the Spark session
</span><span class="n">spark</span><span class="p">.</span><span class="nf">stop</span><span class="p">()</span>
</code></pre></div></div>

<h5 id="mllib">MLlib</h5>
<p><a href="https://spark.apache.org/docs/latest/ml-guide.html">MLlib</a> 是 Apache Spark 的机器学习库，提供了一系列用于构建和训练机器学习模型的工具和算法。MLlib 旨在简化机器学习的工作流程，并支持大规模数据集的处理。</p>

<h5 id="graphx">GraphX</h5>
<p><a href="https://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> 是 Apache Spark 的图计算库，旨在处理大规模图数据。GraphX 提供了一个统一的 API，用于图的创建、操作和分析，支持图的并行计算。</p>

<h5 id="structured-streaming">Structured Streaming</h5>
<p><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Structured Streaming</a> 是 Spark 的流处理库，用于构建可扩展的流式数据处理应用程序。Structured Streaming 允许用户以批处理的方式处理流数据，同时保持了与批处理相同的编程模型。</p>

<p><strong>Structured Streaming 案例</strong></p>

<p><strong>1. 环境准备</strong></p>
<ul>
  <li>安装 zookeeper, kafka 以及必要的一些依赖包
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nb">install </span>zookeeper kafka
pip <span class="nb">install </span>kafka-python
wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.0/spark-sql-kafka-0-10_2.12-3.0.0.jar
wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.12/3.0.0/spark-streaming-kafka-0-10-assembly_2.12-3.0.0.jar
wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.9.0/commons-pool2-2.9.0.jar
</code></pre></div>    </div>
  </li>
  <li>启动 zookeeper
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>zookeeper-server-start /opt/homebrew/etc/kafka/zookeeper.properties
</code></pre></div>    </div>
  </li>
  <li>启动 kafka
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-server-start /opt/homebrew/etc/kafka/server.properties
</code></pre></div>    </div>
  </li>
  <li>创建 topic
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-topics <span class="nt">--create</span> <span class="nt">--topic</span> structured_streaming_demo <span class="nt">--bootstrap-server</span> localhost:9092 <span class="nt">--partitions</span> 1 <span class="nt">--replication-factor</span> 1
</code></pre></div>    </div>
  </li>
  <li>启动 kafka 生产者
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-console-producer <span class="nt">--topic</span> structured_streaming_demo <span class="nt">--bootstrap-server</span> localhost:9092
</code></pre></div>    </div>
  </li>
  <li>启动 spark shell
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyspark <span class="nt">--jars</span> jars/spark-sql-kafka-0-10_2.12-3.0.0.jar,jars/spark-streaming-kafka-0-10-assembly_2.12-3.0.0.jar,jars/commons-pool2-2.9.0.jar
</code></pre></div>    </div>
  </li>
</ul>

<p><strong>2. 代码实现</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="n">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Create SparkSession
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
<span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">StructuredStreamingExample</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">config</span><span class="p">(</span><span class="sh">"</span><span class="s">spark.sql.streaming.forceDeleteTempCheckpointLocation</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">true</span><span class="sh">'</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="c1"># Create DataFrame representing the stream of input data
</span><span class="n">data</span> <span class="o">=</span> <span class="n">spark</span> \
    <span class="p">.</span><span class="n">readStream</span> \
    <span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="sh">"</span><span class="s">kafka</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">option</span><span class="p">(</span><span class="sh">"</span><span class="s">kafka.bootstrap.servers</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">localhost:9092</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">option</span><span class="p">(</span><span class="sh">"</span><span class="s">subscribe</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">structured_streaming_demo</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">option</span><span class="p">(</span><span class="sh">"</span><span class="s">startingOffsets</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">earliest</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">load</span><span class="p">()</span>

<span class="c1"># Select the necessary columns
</span><span class="n">ark_holdings</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">select</span><span class="p">(</span><span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">date</span><span class="sh">"</span><span class="p">),</span> 
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">fund_name</span><span class="sh">"</span><span class="p">),</span>
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">2</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">company_name</span><span class="sh">"</span><span class="p">),</span>
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">3</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">ticker</span><span class="sh">"</span><span class="p">),</span>
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">6</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">shares</span><span class="sh">"</span><span class="p">),</span>
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">7</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">),</span>
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">5</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">))</span>

<span class="c1"># if market_value &gt; 1000000, show the data
</span><span class="n">weight_stock</span> <span class="o">=</span> <span class="n">ark_holdings</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">'</span><span class="s">market_value</span><span class="sh">'</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1000000</span><span class="p">)</span>

<span class="c1"># Start the query to stream the data
</span><span class="n">query</span> <span class="o">=</span> <span class="n">weight_stock</span> \
<span class="p">.</span><span class="n">writeStream</span> \
<span class="p">.</span><span class="nf">outputMode</span><span class="p">(</span><span class="sh">"</span><span class="s">append</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="sh">"</span><span class="s">console</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">start</span><span class="p">()</span>

<span class="c1"># Wait for the query to finish
</span><span class="n">query</span><span class="p">.</span><span class="nf">awaitTermination</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="spark-性能调优">Spark 性能调优</h3>
<p>Spark 性能调优是一个复杂的过程，除了代码层面的优化，这是一些<a href="https://spark.apache.org/docs/latest/configuration.html">配置参数</a>，以下是一些常见的性能调优方法：</p>

<h4 id="资源管理">资源管理</h4>
<ol>
  <li><strong>调整资源分配</strong>: 根据集群的资源情况，<a href="https://spark.apache.org/docs/latest/configuration.html#application-properties">合理分配</a> CPU、内存、磁盘和网络资源，确保每个任务都能获得足够的资源；</li>
  <li><strong>设置资源预取</strong>: 在任务开始前，预先分配好所需的资源，减少任务启动时间；</li>
  <li><strong>监控资源使用情况</strong>: 使用 Spark 的<a href="https://spark.apache.org/docs/latest/monitoring.html">监控工具</a>，如 Spark Web UI、Ganglia、Prometheus 等，监控集群的资源使用情况，及时发现并解决资源瓶颈。</li>
</ol>

<h4 id="数据分区">数据分区</h4>
<ol>
  <li><strong>合理设置分区数</strong>: 根据数据量和集群的资源情况，合理设置分区数，减少数据倾斜和资源浪费；</li>
  <li><strong>数据倾斜</strong>: 数据倾斜是指某些分区中的数据量远大于其他分区，导致某些任务运行时间过长，甚至导致任务失败。可以通过增加分区数、调整分区大小、使用随机分区等方式解决数据倾斜问题；</li>
  <li><strong>数据预分区</strong>: 在数据加载时，根据业务需求和集群的资源情况，合理设置数据分区，减少数据倾斜和资源浪费。</li>
</ol>

<h4 id="数据格式">数据格式</h4>
<ol>
  <li><strong>选择合适的数据格式</strong>: 根据数据的特点和业务需求，选择合适的数据格式，如 <a href="https://parquet.apache.org/">Parquet</a>、<a href="https://orc.apache.org/">ORC</a>、<a href="https://avro.apache.org/">Avro</a> 等，减少数据存储和传输的开销；</li>
  <li><strong>数据压缩</strong>: 使用<a href="https://spark.apache.org/docs/latest/configuration.html#compression-and-serialization">数据压缩算法</a>，如 <a href="https://github.com/google/snappy">Snappy</a>、<a href="https://www.gnu.org/software/gzip/">Gzip</a>、<a href="https://www.oberhumer.com/opensource/lzo/">LZO</a> 等，减少数据存储和传输的开销。</li>
</ol>

<h4 id="数据缓存">数据缓存</h4>
<ol>
  <li><strong>合理设置缓存策略</strong>: 根据数据的特点和业务需求，合理设置<a href="https://spark.apache.org/docs/latest/configuration.html#memory-management">缓存策略</a>，减少数据读取和处理的开销。</li>
</ol>

<p>To be continued…</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Background 进入21世纪以来，随着互联网的发展，各类社交媒体，科技软件，传感器等工具一刻不停地生成着越来越多地数据。有一个说法是: 人类现有90%的数据来自于过去两年。]]></summary></entry><entry><title type="html">Be a Good Data Engineer - Cron and Task Scheduler</title><link href="http://localhost:4000/blog/2024/09/04/cron_and_scheduler.html" rel="alternate" type="text/html" title="Be a Good Data Engineer - Cron and Task Scheduler" /><published>2024-09-04T00:00:00+08:00</published><updated>2024-09-04T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/09/04/cron_and_scheduler</id><content type="html" xml:base="http://localhost:4000/blog/2024/09/04/cron_and_scheduler.html"><![CDATA[<h3 id="background">Background</h3>
<p>程序设计里，依据时间或者依赖其他任务、事件触发执行命令和脚本，是开发很重要的一部分。本文以类 Unix 系统最基础的 Crontab 和生产级任务调度器 Airflow 为例，整体梳理下任务调度这件事。</p>

<h3 id="cron">Cron</h3>
<p>Cron 是类 Unix 操作系统中一款基于时间的任务管理工具，可以通过 cron 在固定的时间、日期、间隔下，执行命令或运行脚本。</p>
<ul>
  <li>check if cron is installed: <code class="language-plaintext highlighter-rouge">dpkg -l cron</code></li>
  <li>install cron: <code class="language-plaintext highlighter-rouge">apt-get install cron</code></li>
  <li>verify the status of cron: <code class="language-plaintext highlighter-rouge">systemctl status cron</code> or <code class="language-plaintext highlighter-rouge">service cron status</code></li>
  <li>start/stop cron service: <code class="language-plaintext highlighter-rouge">systemctl start/stop cron</code> or <code class="language-plaintext highlighter-rouge">service cron start/stop</code>
    <h4 id="use-cases">Use Cases</h4>
    <p>Check Crontab use case, make sure you go through <code class="language-plaintext highlighter-rouge">man crontab</code></p>
  </li>
  <li>test your command line to schedule: <code class="language-plaintext highlighter-rouge">echo "Hello World manually at $(date)" &gt;&gt; $HOME/greetings_manual.txt</code></li>
  <li>check result: <code class="language-plaintext highlighter-rouge">tail ~/greetings_manual.txt</code></li>
  <li>install new crontab job, and choose vim editor: <code class="language-plaintext highlighter-rouge">crontab -e</code></li>
  <li>add a line <code class="language-plaintext highlighter-rouge">* * * * * echo "Hello World automatically at $(date)" &gt;&gt; $HOME/greetings.txt"</code> to the end of the crontab file, save and exit the editor</li>
  <li>check result: <code class="language-plaintext highlighter-rouge">tail ~/greetings.txt</code></li>
  <li>list all crontab jobs: <code class="language-plaintext highlighter-rouge">crontab -l</code></li>
  <li>remove all crontab jobs created by current user: <code class="language-plaintext highlighter-rouge">crontab -r</code>
    <h4 id="syntax">Syntax</h4>
    <p><code class="language-plaintext highlighter-rouge">* * * * * [command to execute]</code></p>
  </li>
  <li>First * stands for representing minutes [0-59];</li>
  <li>Second * stands for representing hour[0-23];</li>
  <li>Third * stands for representing day [0-31];</li>
  <li>Fourth * stands for representing month[0-12];</li>
  <li>Fifth * stands for representing a day of the week[0-6];</li>
</ul>

<p>You can check your cron schedule expressions at: <a href="https://crontab.guru/">Cronitor</a>.</p>

<h3 id="apache-airflow">Apache Airflow</h3>
<p><a href="https://airflow.apache.org/">Airflow</a> 是一款功能强大的开源工作流调度工具，除了内置的如bash、python、email等 <a href="https://airflow.apache.org/docs/apache-airflow/stable/operators-and-hooks-ref.html">Core Operators</a> 以外，还集成了大量第三方平台应用/软件的 <a href="https://airflow.apache.org/docs/apache-airflow-providers/operators-and-hooks-ref/index.html">Community Operators</a>，包含 Apache Software Foundation, Amazon Web Services, Microsoft Azure, Google, etc 相关服务的调度，并且能够监控任务的状态。Airflow 服务器可以部署在单机，也可以根据需求扩展至多节点部署。</p>

<h4 id="installation">installation</h4>
<p>Only <code class="language-plaintext highlighter-rouge">pip</code> installation is currently officially supported.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">AIRFLOW_VERSION</span><span class="o">=</span>2.10.0

<span class="nv">PYTHON_VERSION</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>python3 <span class="nt">--version</span> | <span class="nb">cut</span> <span class="nt">-d</span> <span class="s2">" "</span> <span class="nt">-f</span> 2 | <span class="nb">cut</span> <span class="nt">-d</span> <span class="s2">"."</span> <span class="nt">-f</span> 1-2<span class="si">)</span><span class="s2">"</span>

<span class="nv">CONSTRAINT_URL</span><span class="o">=</span><span class="s2">"https://raw.githubusercontent.com/apache/airflow/constraints-</span><span class="k">${</span><span class="nv">AIRFLOW_VERSION</span><span class="k">}</span><span class="s2">/constraints-</span><span class="k">${</span><span class="nv">PYTHON_VERSION</span><span class="k">}</span><span class="s2">.txt"</span>

pip <span class="nb">install</span> <span class="s2">"apache-airflow==</span><span class="k">${</span><span class="nv">AIRFLOW_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--constraint</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CONSTRAINT_URL</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<h4 id="init-airflow-database">init airflow database</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow db init
</code></pre></div></div>

<h4 id="create-a-user">create a user</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow <span class="nb">users </span>create <span class="se">\</span>
<span class="nt">--username</span> thekingofcool <span class="se">\</span>
<span class="nt">--firstname</span> bug <span class="se">\</span>
<span class="nt">--lastname</span> hunter <span class="se">\</span>
<span class="nt">--role</span> Admin <span class="se">\</span>
<span class="nt">--email</span> sayhi@thekingof.cool
</code></pre></div></div>

<h4 id="start-airflow-scheduler-and-web-server">start airflow scheduler and web server</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow scheduler
airflow webserver <span class="nt">--port</span> 8080
</code></pre></div></div>

<p>visit <a href="http://localhost:8080">localhost:8080</a> to check out the web page.</p>

<h4 id="airflow-dag-file">airflow dag file</h4>
<p>在 Airflow 配置文件中查看 dag 文件位置:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> ~/airflow/airflow.cfg | <span class="nb">grep </span>dags_folder
</code></pre></div></div>
<p>在该文件目录下编辑 dag 文件，重启 airflow scheduler。</p>

<p>Note:</p>
<ul>
  <li>Recommend to use Postgres or MySQL as <a href="https://airflow.apache.org/docs/apache-airflow/2.10.0/howto/set-up-database.html">metadata DB</a> in production;</li>
  <li>Do not use the <a href="https://airflow.apache.org/docs/apache-airflow/2.10.0/core-concepts/executor/index.html">SequentialExecutor</a> in production.</li>
</ul>

<p>To be continued…</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Background 程序设计里，依据时间或者依赖其他任务、事件触发执行命令和脚本，是开发很重要的一部分。本文以类 Unix 系统最基础的 Crontab 和生产级任务调度器 Airflow 为例，整体梳理下任务调度这件事。]]></summary></entry><entry><title type="html">Be a Good Data Engineer - Linux and Shell</title><link href="http://localhost:4000/blog/2024/09/01/linux_and_shell.html" rel="alternate" type="text/html" title="Be a Good Data Engineer - Linux and Shell" /><published>2024-09-01T00:00:00+08:00</published><updated>2024-09-01T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/09/01/linux_and_shell</id><content type="html" xml:base="http://localhost:4000/blog/2024/09/01/linux_and_shell.html"><![CDATA[<h3 id="background">Background</h3>
<p>Linux 是基于 Unix 的开源操作系统，通常用于服务器、台式机和嵌入式系统。</p>

<p>Linux terminal 是一个用于和操作系统进行交互以及执行各种任务的命令行界面，可以用作浏览目录文件及文件中的内容，你也可以用它来安装和升级软件包，也能创建和编辑文件以及执行 shell 脚本。</p>

<h3 id="package-management">Package management</h3>
<p>常用的包管理器有：</p>
<h4 id="apt"><a href="https://wiki.debian.org/Apt">apt</a></h4>
<p>Advanced Packaging Tools, 是一款 <a href="https://www.debian.org/">Debian</a>-based Linux 系统最常用的包管理器。</p>

<p>背景信息：</p>

<p><em>Some of the most popular Debian-based Linux distributions: Linux Mint, Ubuntu, Kali Linux.</em></p>

<p><em>Meanwhile Red Hat-based distributions are: CentOS, Fedora.</em></p>

<p><em>Red Hat-based Linux distributions are often preferred for enterprise environments and servers, focusing on stability and security. Whereas Debian-based Linux distributions moreover focus on long-term support and stability.</em>
Most Used Commands:</p>
<ul>
  <li>install package: <code class="language-plaintext highlighter-rouge">sudo apt-get install [package_name]</code>;</li>
  <li>remove the package: <code class="language-plaintext highlighter-rouge">sudo apt-get remove [package_name]</code>;</li>
  <li>list details of installed package: <code class="language-plaintext highlighter-rouge">sudo apt-get list [package_name]</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://linux.die.net/man/8/apt-get">Linux man page</a></em>.</p>

<h4 id="yum"><a href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/5/html/deployment_guide/c1-yum">yum</a></h4>
<p>Yellowdog Updater Modified, 用作 <a href="https://www.redhat.com/en">Red Hat</a> Enterprise Linux versions 5 and later 的包管理；</p>

<p>Most Used Commands:</p>
<ul>
  <li>install: <code class="language-plaintext highlighter-rouge">yum install [package_name]</code>;</li>
  <li>remove: <code class="language-plaintext highlighter-rouge">yum remove [package_name]</code>;</li>
  <li>update: <code class="language-plaintext highlighter-rouge">yum update</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://access.redhat.com/articles/yum-cheat-sheet">Yum Command Cheat Sheet</a></em>.</p>

<h4 id="dnf"><a href="https://docs.fedoraproject.org/en-US/fedora/latest/system-administrators-guide/package-management/DNF/">dnf</a></h4>
<p>Dandified YUM, a package manager for .rpm-based Linux distributions, is now the default software package management tool in <a href="https://fedoraproject.org/">Fedora</a>.</p>

<p>Most Used Commands:</p>
<ul>
  <li>search: <code class="language-plaintext highlighter-rouge">dnf search packagename</code>;</li>
  <li>install: <code class="language-plaintext highlighter-rouge">dnf install packagename</code>;</li>
  <li>remove: <code class="language-plaintext highlighter-rouge">dnf remove packagename</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://dnf.readthedocs.io/en/latest/command_ref.html">DNF Command Reference</a></em>.</p>

<h4 id="pacman"><a href="https://pacman.archlinux.page/">pacman</a></h4>
<p>一款简单的 Linux 包管理器。</p>

<p>Most Used Commands:</p>
<ul>
  <li>search: <code class="language-plaintext highlighter-rouge">sudo pacman -Ss keyword</code>;</li>
  <li>install: <code class="language-plaintext highlighter-rouge">sudo pacman -S pkgname</code>;</li>
  <li>remove: <code class="language-plaintext highlighter-rouge">sudo pacman -Rs package_name</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://wiki.archlinux.org/title/Pacman#Usage">pacman usage</a></em>.</p>

<h4 id="pip"><a href="https://pip.pypa.io/en/stable/">pip</a></h4>
<p>pip is the package installer for <a href="https://www.python.org/">Python</a>. Alternatively, <a href="https://www.anaconda.com/">conda</a> is another popular package management tool.</p>

<p>Most Used Commands:</p>
<ul>
  <li>search: <code class="language-plaintext highlighter-rouge">python -m pip search "key_word"</code>;</li>
  <li>install: <code class="language-plaintext highlighter-rouge">python -m pip install SomePackage</code>;</li>
  <li>uninstall: <code class="language-plaintext highlighter-rouge">python -m pip uninstall SomePackage</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://pip.pypa.io/en/stable/user_guide/">pip User Guide</a></em>.</p>

<h4 id="brew"><a href="https://brew.sh/">brew</a></h4>
<p>Homebrew installs the stuff you need that Apple (or your Linux system) didn’t.</p>

<p>Most Used Commands:</p>
<ul>
  <li>search: <code class="language-plaintext highlighter-rouge">brew search text</code>;</li>
  <li>install: <code class="language-plaintext highlighter-rouge">brew install package_name</code>;</li>
  <li>uninstall: <code class="language-plaintext highlighter-rouge">brew uninstall package_name</code>;</li>
</ul>

<p><em>More details refer to: <a href="https://docs.brew.sh/Manpage">brew command documatation</a></em>.</p>

<p><strong>Shell learning and troubleshooting: <a href="https://explainshell.com/">explainshell</a></strong>.</p>

<h3 id="command-line-basics">Command Line Basics</h3>
<p>What is <a href="https://www.gnu.org/software/bash/">Bash</a></p>
<ul>
  <li>man: system manual pager;</li>
  <li>ssh: Secure Shell;</li>
  <li>ls: list all the files in your current working directory;</li>
  <li>pwd: print current working directory;</li>
  <li>cd: change working directory;</li>
  <li>touch: create a file or change file timestamp;</li>
  <li>echo: write arguments to standard output;</li>
  <li>nano: simple and useful editor;</li>
  <li><a href="https://www.vim.org/">vim</a>: Vi IMproved text editor;</li>
  <li>cat: to see what’s inside a file real quickly;</li>
  <li>mkdir: make a new directory;</li>
  <li>cp: copy file to a target directory;</li>
  <li>mv: move or rename a file;</li>
  <li>rm: remove files or directories;</li>
  <li>ln: make links to file;</li>
  <li>clear: clear your terminal screen;</li>
  <li>whoami: print effective userid;</li>
  <li>useradd: create a new user or update default new user information;</li>
  <li>sudo: execute a command as another user;</li>
  <li>su: run a command with substitute user and group ID;</li>
  <li>exit: exit the shell;</li>
  <li>passwd: change user password;</li>
  <li><a href="https://curl.se/">curl</a>: transfer data with url;</li>
  <li>zip: package and compress (archive) files;</li>
  <li>unzip: extract compressed files in a zip;</li>
  <li>head: display first lines of a file;</li>
  <li>tail: display the last part of a file;</li>
  <li>diff: differential file and directory comparator;</li>
  <li>sort: sort lines of text files;</li>
  <li>find: search for files in a directory hierarchy;</li>
  <li>chmod: change file modes or Access Control Lists;</li>
  <li>chown: change file owner and group;</li>
  <li>ifconfig: configure network interface parameters;</li>
  <li>grep: file pattern searcher;</li>
  <li><a href="https://www.gnu.org/software/gawk/manual/gawk.html">awk</a>: pattern-directed scanning and processing language;</li>
  <li>ping: send ICMP ECHO_REQUEST packets to network hosts;</li>
  <li>netstat: show network status;</li>
  <li>iptables: administration tool for IPv4/IPv6 packet filtering and NAT</li>
  <li>ufw: Uncomplicated Firewall for Ubuntu;</li>
  <li>uname: display information about the system;</li>
  <li>cal: displays a calendar and the date of Easter;</li>
  <li>df: display free disk space;</li>
  <li>ps: process status;</li>
  <li>top: display sorted information about processes;</li>
  <li>kill: terminate or signal a process;</li>
  <li>pkill: find or signal processes by name;</li>
  <li>history: command history;</li>
  <li>reboot: stopping and restarting the system;</li>
  <li>shutdown: close down the system at a given time;</li>
</ul>

<p>Practical command line use cases on data science: <a href="https://jeroenjanssens.com/dsatcl/list-of-command-line-tools">Data Science at the Command Line</a>;</p>

<p>Learning the shell step by step: <a href="https://linuxcommand.org/lc3_writing_shell_scripts.php">Writing Shell Scripts</a>;</p>

<p>To be continued…</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Background Linux 是基于 Unix 的开源操作系统，通常用于服务器、台式机和嵌入式系统。]]></summary></entry><entry><title type="html">Build a PC to Play WuKong</title><link href="http://localhost:4000/blog/2024/08/24/wukong.html" rel="alternate" type="text/html" title="Build a PC to Play WuKong" /><published>2024-08-24T00:00:00+08:00</published><updated>2024-08-24T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/08/24/wukong</id><content type="html" xml:base="http://localhost:4000/blog/2024/08/24/wukong.html"><![CDATA[<h3 id="pc-build-list">PC Build List</h3>
<p>Total cost is 5310.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">item</th>
      <th style="text-align: left">model</th>
      <th>channel</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">CPU</td>
      <td style="text-align: left">AMD R5 7500F</td>
      <td>PDD</td>
      <td>929</td>
    </tr>
    <tr>
      <td style="text-align: left">RAM</td>
      <td style="text-align: left">光威 天策 16g * 2 6400 C32 M</td>
      <td>PDD</td>
      <td>679</td>
    </tr>
    <tr>
      <td style="text-align: left">GPU</td>
      <td style="text-align: left">GeForce RTX 战斧 4060 DUO 8G</td>
      <td>PDD</td>
      <td>2108</td>
    </tr>
    <tr>
      <td style="text-align: left">Heat sink</td>
      <td style="text-align: left">利民 AX120R SE ARGB</td>
      <td>PDD</td>
      <td>81</td>
    </tr>
    <tr>
      <td style="text-align: left">Power supply unit</td>
      <td style="text-align: left">鑫谷 GM650M 金牌全模</td>
      <td>PDD</td>
      <td>349</td>
    </tr>
    <tr>
      <td style="text-align: left">computer case</td>
      <td style="text-align: left">航嘉 S920 全景</td>
      <td>PDD</td>
      <td>160</td>
    </tr>
    <tr>
      <td style="text-align: left">SSD</td>
      <td style="text-align: left">金士顿 480g SSD</td>
      <td>TMall</td>
      <td>265</td>
    </tr>
    <tr>
      <td style="text-align: left">Motherboard</td>
      <td style="text-align: left">技嘉 AMD B650M GAMING WIFI</td>
      <td>JD</td>
      <td>739</td>
    </tr>
    <tr>
      <td style="text-align: left">SSD(optional)</td>
      <td style="text-align: left">Lexar ARES M.2 pcie4.0 nvmeSSD 1T</td>
      <td>TMall</td>
      <td>521</td>
    </tr>
  </tbody>
</table>

<h3 id="create-bootable-windows-11-usb-disk">Create Bootable Windows 11 USB Disk</h3>
<p>When the computer is ready, you need a Windows system boot disk. I’ll do this on a MacOS.</p>

<h4 id="format-usb-disk">Format USB Disk</h4>
<p>Go to Application, find Utilities - Disk Utility;
Choose your Disk, and erase the USB External Physical Volume to MS-DOS (FAT32);
Rename the USB Drive to <strong>WIN11</strong> or whatever you like.</p>

<h4 id="download-system-image">Download System Image</h4>
<p>Download <a href="https://www.microsoft.com/software-download/windows11">Windows 11</a> Disk Image (ISO) to your computer;</p>

<h4 id="making-a-boot-disk">Making a Boot Disk</h4>
<ol>
  <li>Install wimlib
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew install wimlib
</code></pre></div>    </div>
  </li>
  <li>Mount the ISO file into a folder
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hdiutil mount Downloads/Win11_23H2_English_x64v2.iso
</code></pre></div>    </div>
  </li>
  <li>Copy all Windows ISO contents into the USB Disk
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rsync -vha --exclude=sources/install.wim /Volumes/CCCOMA_X64FRE_EN-US_DV9/* /Volumes/WIN11
</code></pre></div>    </div>
  </li>
  <li>Split the installation file in two parts if the size is bigger than 4GB
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wimlib-imagex split /Volumes/CCCOMA_X64FRE_EN-US_DV9/sources/install.wim /Volumes/WIN11/sources/install.swm 3800
</code></pre></div>    </div>
  </li>
  <li>Eject USB device to use it for OS installation.</li>
</ol>

<h3 id="troubleshooting">Troubleshooting</h3>
<ol>
  <li>
    <p>0 free space when installing the system
<strong>Solution</strong>: There is some content in the SSD, choose ‘delete’ to erase SSD.</p>
  </li>
  <li>
    <p>The selected disk has an MBR partition table. On EFI system, Windows can only be installed to GPT disks.
<strong>Solution</strong>:</p>
    <ul>
      <li>Restart the computer, On the Windows installation screen, select ‘Repair your computer’; Choose ‘Troubleshoot’ &gt; ‘Command Prompt’.</li>
      <li>Enter <code class="language-plaintext highlighter-rouge">diskpart</code> to initiate the disk partition process;</li>
      <li>Enter <code class="language-plaintext highlighter-rouge">list disk</code>. Make a note of the MBR disk number that you want to convert to GPT format;</li>
      <li>Enter <code class="language-plaintext highlighter-rouge">select disk</code> <em><code class="language-plaintext highlighter-rouge">&lt;disk-number&gt;</code></em>, where <code class="language-plaintext highlighter-rouge">&lt;disk-number&gt;</code> is the MBR disk number to convert;</li>
      <li>Enter <code class="language-plaintext highlighter-rouge">clean</code> to delete all partitions and volumes on the disk;</li>
      <li>Enter <code class="language-plaintext highlighter-rouge">convert gpt</code> to convert the MBR disk to the GPT partition format.
The diskpart process notifies you when the conversion completes.</li>
    </ul>
  </li>
</ol>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[PC Build List Total cost is 5310.]]></summary></entry><entry><title type="html">Dana White introduces Donald Trump at the 2024 RNC</title><link href="http://localhost:4000/reading/2024/07/18/Dana_White_introduces_Donald_Trump_at_the_2024_RNC.html" rel="alternate" type="text/html" title="Dana White introduces Donald Trump at the 2024 RNC" /><published>2024-07-18T00:00:00+08:00</published><updated>2024-07-18T00:00:00+08:00</updated><id>http://localhost:4000/reading/2024/07/18/Dana_White_introduces_Donald_Trump_at_the_2024_RNC</id><content type="html" xml:base="http://localhost:4000/reading/2024/07/18/Dana_White_introduces_Donald_Trump_at_the_2024_RNC.html"><![CDATA[<p>Good evening ladies and gentlemen. I am Dana White. I am the CEO and president of the Ultimate Fighting Championship.</p>

<p>So two weeks ago I got a call from President Trump asking me if I’d be willing to speak tonight. As usual, there was no pressure, no demands. He asked me as a friend and of course I said yes. Then after I accepted his offer, he sent me a text message. And I just want to read to you a little piece of what President Trump wrote to me.</p>

<blockquote>
  <p>Dana, I’m so honored that you will be doing the introduction at the National Republican Convention. Think of it as the biggest fight you ever had, a fight for our country and even the world. I only wish you didn’t have to interrupt your family trip. But I hope they understand, they love you and they know how important this is.</p>
</blockquote>

<p>Now think about this. This man’s running for President of the United States. He’s fighting for the future of this country and he’s concerned about interrupting my family trip. That’s the President Trump that I know, a man who truly cares about people. The mainstream media likes to push the narrative that he doesn’t care about anyone but himself. I absolutely know that’s not the truth because I’ve been friends with this guy for 25 years.</p>

<p>And for the people who know me, they’ll know this is true. I just want to make something very clear. Nobody in the Trump campaign has ever told me what to say. Nobody tells me what to say and I’m nobody’s puppet. And I’m not telling you what to think. I’m telling you what I know. And I know President Trump, I know President Trump is a fighter. I’ve been saying this since 2015.</p>

<p>Now look at what’s happened over the last 10 years. We have all seen it with our own eyes. I’m in the tough guy business. And this man is the toughest, most resilient human being that I’ve ever met in my life. The higher the stakes, the harder he fights, and this guy never ever gives up. So what’s it stake here? The answer is in President Trump’s text. And I quote:</p>

<blockquote>
  <p>A fight for our country.</p>
</blockquote>

<p>I know why he’s running for President again. Why else? Would he put himself through everything he’s dealt with just to get back here? We all know he doesn’t need this. This guy’s got a great life. He has a beautiful family. And he has achieved everything that you could possibly achieve in life. I know President Trump is literally putting his life on the line for something bigger than himself. And he’s willing to risk it all because he loves this country.</p>

<p>And I know he wants what’s best for the American people. All American people. I know he’s running for President to save our American dream. I’m living in the American dream. And I know the American dream is very real. Whether you’re born in this country or came here from some place else, this is the last real land of opportunity. President Reagan once said,</p>

<blockquote>
  <p>Government’s first duty is to protect the people, not run their lives. And if you’re buried in government red tape, how will you ever start your own business? If you’re struggling to pay your bills, how can you ever afford to start a family? And if you don’t feel safe in your own town, why would you ever buy a house?</p>
</blockquote>

<p>I know the President Trump is fighting to save the American dream, and that’s what’s at stake in this election. We are choosing who we want to lead us in this fight. I know President Trump is a proven leader, a fearless leader, and this country was in a much better place when he was in the Oval Office. In my mind, the choice is clear. But this election, we all get the choose. I know I’m going to choose strength and security. I know I’m going to choose opportunity and prosperity. I know I’m going to choose real American leadership and a real American badass.</p>

<p>And I’m not telling you what choice to make, and I’m not telling you what to think. I’m telling you what I know. I know American needs a strong leader, and the world needs a strong America. I know Donald J. Trump is the best choice for President of the United States. My fellow Americans, it is my honor to introduce the 45th and soon to be 47th President of the United States. Donald J. Trump.</p>]]></content><author><name></name></author><category term="reading" /><summary type="html"><![CDATA[Good evening ladies and gentlemen. I am Dana White. I am the CEO and president of the Ultimate Fighting Championship. So two weeks ago I got a call from President Trump asking me if I’d be willing to speak tonight. As usual, there was no pressure, no demands. He asked me as a friend and of course I said yes. Then after I accepted his offer, he sent me a text message. And I just want to read to you a little piece of what President Trump wrote to me. Dana, I’m so honored that you will be doing the introduction at the National Republican Convention. Think of it as the biggest fight you ever had, a fight for our country and even the world. I only wish you didn’t have to interrupt your family trip. But I hope they understand, they love you and they know how important this is. Now think about this. This man’s running for President of the United States. He’s fighting for the future of this country and he’s concerned about interrupting my family trip. That’s the President Trump that I know, a man who truly cares about people. The mainstream media likes to push the narrative that he doesn’t care about anyone but himself. I absolutely know that’s not the truth because I’ve been friends with this guy for 25 years. And for the people who know me, they’ll know this is true. I just want to make something very clear. Nobody in the Trump campaign has ever told me what to say. Nobody tells me what to say and I’m nobody’s puppet. And I’m not telling you what to think. I’m telling you what I know. And I know President Trump, I know President Trump is a fighter. I’ve been saying this since 2015. Now look at what’s happened over the last 10 years. We have all seen it with our own eyes. I’m in the tough guy business. And this man is the toughest, most resilient human being that I’ve ever met in my life. The higher the stakes, the harder he fights, and this guy never ever gives up. So what’s it stake here? The answer is in President Trump’s text. And I quote: A fight for our country. I know why he’s running for President again. Why else? Would he put himself through everything he’s dealt with just to get back here? We all know he doesn’t need this. This guy’s got a great life. He has a beautiful family. And he has achieved everything that you could possibly achieve in life. I know President Trump is literally putting his life on the line for something bigger than himself. And he’s willing to risk it all because he loves this country. And I know he wants what’s best for the American people. All American people. I know he’s running for President to save our American dream. I’m living in the American dream. And I know the American dream is very real. Whether you’re born in this country or came here from some place else, this is the last real land of opportunity. President Reagan once said, Government’s first duty is to protect the people, not run their lives. And if you’re buried in government red tape, how will you ever start your own business? If you’re struggling to pay your bills, how can you ever afford to start a family? And if you don’t feel safe in your own town, why would you ever buy a house? I know the President Trump is fighting to save the American dream, and that’s what’s at stake in this election. We are choosing who we want to lead us in this fight. I know President Trump is a proven leader, a fearless leader, and this country was in a much better place when he was in the Oval Office. In my mind, the choice is clear. But this election, we all get the choose. I know I’m going to choose strength and security. I know I’m going to choose opportunity and prosperity. I know I’m going to choose real American leadership and a real American badass. And I’m not telling you what choice to make, and I’m not telling you what to think. I’m telling you what I know. I know American needs a strong leader, and the world needs a strong America. I know Donald J. Trump is the best choice for President of the United States. My fellow Americans, it is my honor to introduce the 45th and soon to be 47th President of the United States. Donald J. Trump.]]></summary></entry><entry><title type="html">2014~2024，仅仅10年</title><link href="http://localhost:4000/reading/2024/07/03/ten_years_ago.html" rel="alternate" type="text/html" title="2014~2024，仅仅10年" /><published>2024-07-03T00:00:00+08:00</published><updated>2024-07-03T00:00:00+08:00</updated><id>http://localhost:4000/reading/2024/07/03/ten_years_ago</id><content type="html" xml:base="http://localhost:4000/reading/2024/07/03/ten_years_ago.html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/nbLyWvV-cnj14f0rMWnV3g?WBAPIAnalysisOriUICodes=10000001&amp;aid=01A110-grIlutGzstJd54VtjM7_Idd0V8mws-Z9MgmIslKOSY.&amp;from=10E5393010&amp;v_p=90&amp;wm=3333_2001">Read Original</a></p>

<table>
  <tbody>
    <tr>
      <td>date_saved: 2024-07-03 23:15:56</td>
      <td>date_published: 2024-06-22 14:09:00</td>
    </tr>
  </tbody>
</table>

<p><strong>01</strong></p>

<p>十年前的夏天，年轻人不愿当公务员，国考人数锐减36万，热帖称“机关钱少活多”。</p>

<p>那年全国毕业生700余万，就业率超九成，复旦学生租游艇办毕业舞会，女孩们花两千元买晚礼服，夜游江海。</p>

<p>那年世界的齿轮咬合稳定，中美迎来建交35周年。美国民调中，超72%年轻人，将中国视为“朋友”。</p>

<p>夏天前，奥巴马夫人到访，体验了长城、紫禁城与成都火锅。</p>

<p>慕田峪长城上，总统夫人看燕山起伏，觉得一切宽阔且美妙，“长城的长度几乎相当于从美国缅因州到俄勒冈州的四倍”。</p>

<p>那年的国运也如山峦起伏。</p>

<p>夏天时，股市清冷，七成账户闲置，股民调侃关灯吃面，7月IPO开闸，并购潮掀起，年底股市单日放量7100亿，狂飙冲天。</p>

<p>楼市故事也相似。十年前的五一，房企奄奄一息，北京楼盘推出零首付，南京楼盘跳远减十万，上海房展出动比基尼美女吸引眼球。9月楼市松绑，炒房客陷入狂欢。</p>

<p>十年前的人们尚不知卷与颓，偶有下挫，也认为不过是插曲，对一切满怀自信。</p>

<p>贾跃亭宣布要造超级互联网汽车，罗永浩宣布要发布东半球最好用的手机。真正手机大卖的是小米，第一季度销量超过苹果。</p>

<p>夏天过后，雷军去乌镇参加首届互联网大会。他磕磕巴巴说，梦想还是要有，万一实现呢？</p>

<p>那年乌镇最风光还是BAT，三家都在硅谷设立了分支，李彦宏说机会太多，他很着急：</p>

<blockquote>
  <p>我们其实处在非常有意思的时代，这是魔幻一般的时代，正好我们这一代人赶上互联网的兴起。</p>
</blockquote>

<p>入夜，乌镇白墙黑瓦水音桨声。丁磊拼起旧木桌，摆起乌镇宴，座中人微博记录：十几瓶黄酒喝去，陈年故事吐出，煮酒笑谈云中事，天罗地网立旌旗。</p>

<p>未被邀请的马云，才是那年真正的主角。十年前的夏天，阿里启动全球最大规模IPO。</p>

<p>上市前，马云发内部邮件，建议员工不要挥霍，处理好财富，“我们这么辛苦，可不是为了变成一群土豪”。</p>

<p>当年9月，阿里上市，马云登顶中国首富，万名阿里员工成千万富翁，宝马销售和房产中介堵在阿里园区门口。</p>

<p>十年前的夏天蒸腾如梦，浩荡热风吹过中国。北京高温刷新了1951年以来纪录，居民用水多喝出4.5个昆明湖。</p>

<p>济南、上海、重庆、吐鲁番尽成火炉，更大热浪在互联网彩票服务器上。那年是巴西世界杯，足彩卖出23亿。</p>

<p>在广州，恒大正在冲击中超三连冠，教练席上，新任助教李铁说，有很多东西不是金钱所能衡量，“我给自己十年左右的时间，争取成为国家队的主教练”。</p>

<p>那个夏天，恒大冰泉形象代言人是金秀贤，来自星星的都教授横扫中国，名字一度在人大会议上提及。</p>

<p>火热韩流中，也有人抽身归来。当年10月，鹿晗宣布和韩国方解约，他的微博单条评论破千万，创下吉尼斯世界纪录。</p>

<p>而在夏天，他的队友吴亦凡更早解约，在微博霸气留言：感谢所有支持我的人，吴亦凡一直都在！</p>

<p>属于他们的时代刚拉开帷幕，而上一代的偶像韩寒，在十年前夏天推出了第一部电影。</p>

<p>他为电影写了主题曲《平凡之路》，请出隐遁的朴树演唱，年轻人喜欢那句“我曾经跨过山和大海”，有种渡尽一切后的禅机。</p>

<p>他们不知道，真正的大山大海，还在后面。</p>

<p><strong>02</strong></p>

<p>访问中国时，奥巴马夫人在成都七中演讲，说对共同的未来，抱有前所未有的信心，“希望纽带在未来几十年绵延”。</p>

<p>几个月后，奥巴马在APEC会议上说：加深美中贸易将使两国受益，更能助力世界平稳。</p>

<p>那一年，世界银行预言中国经济将持续高速增长，胡润富豪榜称坚定看好中国，“中国这年有176位百亿富豪，有8位80后白手起家”。</p>

<p>那年时代汽笛嘹亮，股市楼市火爆之下，真正动能是基建启动。</p>

<p>当年，中央批复基建项目1.56万亿，发改委21天内批了16条铁路和5个机场。</p>

<p>任泽平激情连发三篇研报，称“请不要辜负这个时代，将熊市彻底埋葬”。</p>

<p>十年前的夏天，达沃斯论坛上，第一次提出“大众创业、万众创新”，称要破除一切束缚，让创新血液自由流动：</p>

<blockquote>
  <p>“在960万平方公里大地上掀起大众创业、草根创业的新浪潮。”</p>
</blockquote>

<p>海淀图书城步行街改名创业大街，车库咖啡里装满野望。朱啸虎说，只要想看，有看不完的创业项目等着。</p>

<p>当年爆款节目《超级演说家》上，90后女孩说：我不是来适应社会的，我是来改变社会的。</p>

<p>大浪之中，十年前的企业家也带着少年意气。融创的孙宏斌说“理想主义销魂蚀骨”，万科的王石说“我坚信市场的力量”。</p>

<p>那年的俞敏洪，出版新书《在绝望中寻找希望》。他到南开鼓励大一新生，“人要靠自己选择走出无穷无尽的路来”。</p>

<p>当天地足够开阔时，天骄的心里总能装下帝国。</p>

<p>十年前夏天，恒大在草原深处举办订货会。</p>

<p>32架包机直飞乌兰浩特，80辆大巴浩荡出发，穿越草原，翻过土岭，最终到达阿尔山的荒僻山谷。</p>

<p>恒大为山谷接水接电，接通wifi，5000多人就地露营，宴席后的烟花放了半小时。</p>

<p>那夜，恒大订货成交119亿，许家印放言3年冲击世界500强。醉酒后他问部下“我怎样才能流芳百世？”</p>

<p>王健林目标则在更远处，万达将在2020年成为世界第一的旅游企业，超越迪士尼。</p>

<p>那年夏天，他正在豪买世界，说伦敦地价低得跟不要钱一样，说CNN若想被收购也可买下。</p>

<p>6月，他花2.65亿欧元，买下马德里地标西班牙大厦，坊间流传“王健林出国考察，顺便买了座大厦”。</p>

<p>那年夏天，流传更广其实是他儿子的名句，草根朋友生日他送出豪车，“我交朋友不在乎他有钱没钱，反正都没我有钱。”</p>

<p>十年前夏天金粉飘飞。网易为校招生开出年薪50万，中新网称大学生毕业三年后收入翻番。那年蚂蚁金服创立，余额宝风行，马云说阿里钱多是一种负担。</p>

<p>滴滴和快的大战发出40亿红包，高峰时一天烧掉1亿，烧得心惊肉跳，“如果把一亿元现金堆在一个屋子里烧，恐怕也得烧一整天吧”。</p>

<p>真烧钱的是李笑来，接受专访时，他对着镜头用美金点烟。</p>

<p>小苹果唱了一个盛夏，满街都是火火火火，范冰冰在马背上颠簸，扮演武则天，称再也不会有这个投资级别的大戏。</p>

<p>夏天时，华谊兄弟庆生20周年，宣称两年要赚100亿票房。那年纽约苏富比拍卖，王中军拍下梵高画的雏菊与罂粟花，耗资3.77亿元。</p>

<p>十年前盛夏，澎湃上线，发刊辞中，邱兵写道：</p>

<blockquote>
  <p>后来，嘈杂的年代就来了。我们从理想主义来到了消费主义，来到了精致的利己主义，我们迎来了无数的主义……那个夏夜，回忆起来，纠缠着，像无数个世纪。</p>

  <p>我心澎湃如昨。</p>
</blockquote>

<p><strong>03</strong></p>

<p>十年后的夏天，白宫宣布，对中国进口电动车增加关税到100%，企图以此扼杀产业。</p>

<p>中国商务部回应，这是“将经贸问题政治化、工具化，是典型的政治操弄”。</p>

<p>美国宣布提高关税后，意外受益者之一是滞留美国的贾跃亭。</p>

<p>法拉第未来股价，增幅1600%，虽然只是从0.04美元涨到0.7美元。十年后，他终于造出汽车，但只有11台，且全部因安全原因召回。</p>

<p>十年苍茫如海。贾跃亭没了乐视，罗永浩没了锤子，乌镇也已多年没了夜宴。</p>

<p>十年前宣称埋葬熊市的任泽平，去年在卖生发水。微博上，他展示生活节奏：晨跑三公里，一百个俯卧撑和卷腹，正心正念。</p>

<p>他的前老板许家印没能流芳百世，已被依法采取强制措施。另一位地产大佬王健林四处出售万达资产，距离年会唱《假行僧》也已八年。</p>

<p>五一前，王思聪在东京被网友偶遇，依旧开着豪华SUV，只是网友说“王思聪似乎比往常更加憔悴”。</p>

<p>当年和他录过节目的吴亦凡，已锒铛入狱，微博查无此人，上海杜莎夫人蜡像馆的真人蜡像也早已撤去。</p>

<p>当年蜡像揭幕时，吴亦凡受访说，他常和范冰冰等人聚会，“范冰冰说我像她孪生弟弟，很关照”。</p>

<p>范冰冰早已无暇关照她，更早跌进尘埃。去年现身新加坡国际电影节时，她说“我觉得好像人各有命”。</p>

<p>许多人的命运已经永久改变了。京东前CEO徐雷，在朋友圈说，周期和时代是两个截然不同的性质。</p>

<p>去年年底，国家公务员报考增加40万人，今年1179万毕业生中，排名最高的期待是进入国企。夸克报告中，上岸是2023年年轻人关键词。</p>

<p>一切转为务实和求稳。年轻人设闹铃抢大额存单，买货币基金“闷声发小财”，他们喜欢《繁花》里玲子说的：</p>

<blockquote>
  <p>“不是所有人都像你宝总做数学题一样地赚钱，大部分是像我一样，吭哧吭哧，一个硬币一个硬币地存起来的。”</p>
</blockquote>

<p>十年前的金粉早已飘散，3W咖啡店门紧闭，创业大街新宠是硬科技，投资人冲到高校实验室，抢投教授。</p>

<p>有投资机构说，值得欣慰的是，当下中小微企业数量还在增长。</p>

<p>狂飙已成往事，所有人都在适应新节奏。从日本归来的马云，在内部信中说，“时代变了，我们要跟上时代”。</p>

<p>今年年初，94岁的吴敬琏，发文称最重要不外乎两条：坚持改革，继续开放。他说：</p>

<blockquote>
  <p>只有一个办法，就是尽可能创造这样一种营商环境，使得个人和企业的千军万马能够往前冲，最终总有一些人和企业能够取得突破，那么就可以顺着这些突破的路径继续往前走。</p>
</blockquote>

<p>楼市风声鹤唳之际，因万科资金紧张，王石放弃了千万退休金。</p>

<p>他对行业依旧乐观，“房地产并未结束，而是刚刚开始”。</p>

<p>十年前的夏天，他带田朴珺去云南哀牢山，拜访褚时健。老人穿着圆领衫黑裤子，村口相候。</p>

<p>褚时健说，他也曾是年轻人，从新中国成立后到现在，社会变动很大，人生很多事，不是一条直线。国家要转型，始终要靠人来破解难题。</p>

<p>他说，困难多，搞好一点，信心就大一点，只有这样走，一步一步来。</p>]]></content><author><name></name></author><category term="reading" /><summary type="html"><![CDATA[Read Original]]></summary></entry><entry><title type="html">Wedding Speech</title><link href="http://localhost:4000/blog/2024/06/16/big_day.html" rel="alternate" type="text/html" title="Wedding Speech" /><published>2024-06-16T00:00:00+08:00</published><updated>2024-06-16T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/06/16/big_day</id><content type="html" xml:base="http://localhost:4000/blog/2024/06/16/big_day.html"><![CDATA[<h2 id="陈">陈:</h2>

<p>刚认识你时，记得漫威剧《Loki》正在更新，最开始对你的印象就是，我们就像平行宇宙中的不同变体——</p>

<p>我们的共同点很多：都喜欢皇马，都经常运动，都对生活和对方充满好奇。</p>

<p>随着了解加深，我发现我们也有很多的不同，你看过很多音乐剧，我听过很多摇滚乐，我陪你看了许多舞台剧，你也因为我有段时间热衷马拉松而开始喜欢长跑。</p>

<p>你让我觉得，人生是可以做着自己热爱的事来度过的。</p>

<p>未来还有很多事情值得探索，而你的陪伴也让我对这一切变得更加期待。</p>

<hr />

<p>今天场下在座的，以及台上站着的，都是我人生中最重要的人。</p>

<p>很难用语言来形容这个时刻有你们见证所感到的喜悦。</p>

<p>利用这个机会，我想先感谢我的爸妈，你们在我心里是最伟大的人，没有你们的辛勤付出，就没有我和弟弟的今天。你们以身作则，教会了我们诚实、善良、进取的品质；在那个充满未知和挑战的年代，你们把我们家从四川乡村的土墙房带到了大都市，让我和弟弟接受到了最好的教育，真正意义上实现了命运的改变。</p>

<p>我还要感谢我今天新多的爸妈，感谢你们培养了如此优秀、美丽、纯真的女儿。含辛茹苦三十载并不容易，将她护在自己的羽翼下快乐成长尤其困难。从此以后，我会倾尽所有照顾好她和我们的家。不让她受到一点委屈。</p>

<p>同时，我也要感谢从繁忙的工作和学业中赶来的弟弟和弟妹，未来在你们身上有着一切可能。以后无论身处何处，血浓于水的情感纽带都将始终将我们紧密连结。在这个变革的时代，让我们团结一心，共同努力，使这个大家庭更加繁荣和充满活力。</p>

<p>再次感谢大家来参加我们的婚礼，希望你们这几天都能玩得开心。</p>

<hr />

<h2 id="杨">杨:</h2>

<p>陈先生，你好。</p>

<p>我一直觉得，下定决心结婚是非常困难的一件事。要跟这个人朝夕相处几十年，要从今天开始以后每天都面对面在枕头上喷气儿，要共同养育一个孩子，甚至更多。甚至，你还拥有我在手术台上昏迷不醒时决定我要不要继续手术的权利，相当于我得把一半的命给你，我怎么能轻而易举的交出自己的命呢？</p>

<p>所以，陈先生，我必然不会因为你是高富帅而嫁给你，当然，这三个字也不太是用来形容你。而是因为你善良，温厚，努力上进，真实简单吗？这是我想到的夸赞你的话，但这也不是作为我爱你的理由。</p>

<p>比起这些所有人可见的美好品质，我更珍惜你的尊重，支持和包容，以及和你在一起后每一天相处的点滴，都让我深信不疑，这个人我愿意和他在一起。</p>

<p>这可能就是我今天站在这里的原因。我们在如此平凡琐碎的生活中，拥有对方有趣的灵魂，拥有彼此的绝对信任，拥有自由，平等和尊重。拥有对方无条件的鼓励和支持。</p>

<p>以前我一直觉得，一个人能过得很开心，但有了你才发现，两个人原来可以更好。</p>

<p>我爱的，是我们拥有各自的光芒，同时也依赖对方。</p>

<p>谢谢你，让我期待和你一起的未来。</p>

<p>我要我们不止新婚快乐，我要我们一直都快乐。</p>

<hr />

<p>今天算是我人生中很重要的时刻，对于我这种时常心怀感激却不善于表露的人来说也是一个表达谢意的绝佳机会。</p>

<p>首先最想感谢的是我的爸爸妈妈，谢谢你们30年来的养育，在能力范围内创造最好的条件把我培养长大，让我在爱的环境里健康成长，感谢你们包容了我每一个特立独行的勇气和决定。</p>

<p>我是这个世界上很普通的女儿，经常会惹你们生气失望，但你们在我眼里是最好的爸爸妈妈，我从未让你们骄傲，你们却待我如宝。感谢你们陪着我一起长大。</p>

<p>有人说过:“父母存在的意义不是给予孩子多舒适和富裕的生活，而是当你想到你的父母时，你的内心就会充满力量会感受到温暖，从而拥有克服困难的勇气和能力，因此获得人生真正的乐趣和自由。”</p>

<p>好巧，我跟陈都是这么幸运和幸福的人。我们都有对我们最好的爸爸妈妈，并且从今天开始，我们的幸运和幸福翻倍。未来，就让我们俩一起陪伴你们，保护你们。</p>

<p>最后，两位爸爸，父亲节快乐!</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[陈:]]></summary></entry><entry><title type="html">10 rules for financial success</title><link href="http://localhost:4000/blog/2024/06/09/financial_success.html" rel="alternate" type="text/html" title="10 rules for financial success" /><published>2024-06-09T00:00:00+08:00</published><updated>2024-06-09T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/06/09/financial_success</id><content type="html" xml:base="http://localhost:4000/blog/2024/06/09/financial_success.html"><![CDATA[<ol>
  <li>Budget</li>
  <li>Buy assets</li>
  <li>Pay off debt</li>
  <li>Invest in yourself</li>
  <li>Add income streams</li>
  <li>Refuse lifestyle creep</li>
  <li>Cut unneeded expenses</li>
  <li>Invest 20% of your income</li>
  <li>Be with business-minded people</li>
  <li>Marry someone who understands this</li>
</ol>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Budget Buy assets Pay off debt Invest in yourself Add income streams Refuse lifestyle creep Cut unneeded expenses Invest 20% of your income Be with business-minded people Marry someone who understands this]]></summary></entry></feed>