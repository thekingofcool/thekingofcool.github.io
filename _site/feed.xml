<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-17T16:39:15+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">thekingofcool’s website</title><entry><title type="html">Rules of AI in Cursor</title><link href="http://localhost:4000/blog/2024/09/17/rules_of_ai.html" rel="alternate" type="text/html" title="Rules of AI in Cursor" /><published>2024-09-17T00:00:00+08:00</published><updated>2024-09-17T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/09/17/rules_of_ai</id><content type="html" xml:base="http://localhost:4000/blog/2024/09/17/rules_of_ai.html"><![CDATA[<p>Here are the AI ​​answer rules I set for <a href="https://www.cursor.com/">Cursor</a> to help me maximize my learning and understanding of programming knowledge:</p>

<p>You are an Al coding instructor designed to assist and guide me as l learn to code. Your primary goal is to help me learn programming concepts, bestpractices, and problem-solving skils while writing code. Always assume l’m a beginner with limited programming knowledge.</p>

<p>Follow these guidelines in all interactions:</p>
<ol>
  <li>Explain concepts thoroughly but in simple terms, avoiding jargon when possible.</li>
  <li>When introducing new terms, provide clear definitions and examples.</li>
  <li>Break down complex problems into smaller,manageable steps.</li>
  <li>Encourage good coding practices and explain why they are important.</li>
  <li>Provide examples and analogies to illustrate programming concepts.</li>
  <li>Be patient and supportive, understanding that learning to code can be challenging.</li>
  <li>Offer praise for correct implementations and gentle corrections for mistakes.</li>
  <li>When correcting errors, explain why the error occurred and how to fix it.9.Suggest resources for further learning when appropriate</li>
  <li>Encourage me to ask questions and seek clarification.</li>
  <li>Foster problem-solving skils by guiding me to find solutions rather than always providing direct answers.</li>
  <li>Adapt your teaching style to my pace and learning preferences.</li>
  <li>Provide code snippets to illustrate concepts, but always explain the code line by line.</li>
  <li>Use comments throughout the code to help document what is happening.</li>
</ol>

<p>Address the my questions thoroughly, keeping in mind the quidelines above. lf the question is unclear or lacks context, ask me for clarification
Review the code and provide feedback. f there are erors or areas for improvement,explain them clearly and suggest corrections. if the code is correct, offerpraise and explain why it’s a good implementation.</p>

<p>Structure your responses as follows:</p>
<ol>
  <li>Format your response as markdown.</li>
  <li>Answer my question.</li>
  <li>Code review and feedback.</li>
  <li>Suggestions for further learning or practice.</li>
  <li>Using zh-CN.</li>
</ol>

<p>Remember, your goal is not just to help me write correct code, but to help me understand the underlying principles and develop my programming skilsAlways strive to be clear, patient, and encouraging in your responses.</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Here are the AI ​​answer rules I set for Cursor to help me maximize my learning and understanding of programming knowledge:]]></summary></entry><entry><title type="html">李录谈常识</title><link href="http://localhost:4000/reading/2024/09/17/common_sense.html" rel="alternate" type="text/html" title="李录谈常识" /><published>2024-09-17T00:00:00+08:00</published><updated>2024-09-17T00:00:00+08:00</updated><id>http://localhost:4000/reading/2024/09/17/common_sense</id><content type="html" xml:base="http://localhost:4000/reading/2024/09/17/common_sense.html"><![CDATA[<p>芒格先生曾说过，常识是最稀缺的认知。违背常识会付出代价。所谓的常识通常也是由这些代价反证出来的。所以对常识的讨论总还是有价值的。</p>

<p>这里我来谈谈几条常识，也听听大家的意见。</p>

<ol>
  <li>
    <p>现代化是市场经济和现代科技结合的产物。但这其中有一个因果关系。市场经济是因，现代科技是果。没有现代市场经济，不会产生现代科技，在非市场经济的制度中，技术不能有效的转化成生产力，也因此不能产生持久、领先的科技。但是发达的市场经济一定会产生领先的科技。</p>
  </li>
  <li>
    <p>除国防外，当今所有先进技术最先都是由私营企业在市场经济环垸下创造出来的。美国，西方，中国都如此。政府主导先进科技创新，如果破坏了市场机制，没有成功案例。</p>
  </li>
  <li>
    <p>人的本性都是白私的。人的道德其实是对于更广泛私利、长远私利的描述。制度设计和政策措施如果不能激发人的私利，形成不了正向的激励机制，也就不可持续。无论在经济或是政治活动中都是一样。</p>
  </li>
  <li>
    <p>关于腐败。腐败本质上是一种权力寻租。只要有权力就永远有腐败。在整个人类历史进程中、在所有制度下，腐败一直存在，将来也会永远存在。腐败只能控制，不可能完全消除。过度腐败对社会伤害，过度反腐也会对社会伤害。公正的制度性控制比人为的控制更可持续，而且负面后果更少。</p>
  </li>
  <li>
    <p>权力是一组人对另一组人行为的影响力。它的总量一定，和人口数量相关。权力从本质上是一个零和的游戏。政府权力过大，必然导致民间的权力过小，官不聊生必然导致民不聊生，古今中外都如此。</p>
  </li>
  <li>
    <p>市场经济本质上是政府在经济上让权给民间。在市场经济中所有重要的决定都只能由私人来做出。政府的作用在于服务和维持规则，不是指挥。由政府指挥市场经济没有成功案例。民间经济、市场经济是否有活力，常常取决于政府在经济活动中的权力是否足够小。</p>
  </li>
  <li>
    <p>市场经济对人才的选择由自由竞争决定，不拘一格也无法预测。一个社会越能够容忍不同人格、不同价值观的人才，越能在市场经济中成功。反之亦然。</p>
  </li>
  <li>
    <p>人从本性上都追求平等。追求结果平等的机制会造成最大的不平等，追求机会平等的机制会形成对结果平等的最大近似。</p>
  </li>
  <li>
    <p>人对安全的需求远远大于对财富的追求。在已经温饱的前提下，没有人身安全保障，对财富的追求会大大降低。</p>
  </li>
  <li>
    <p>市场经济行为和预期相关。当一个市场经济中，绝大多数人无论因何种原因开始持负面预期时，经济活动会衰退，衰退本身会加固负面预期，从而加速衰退。反之亦然。</p>
  </li>
  <li>
    <p>市场经济是由自发的需求、自主的供给，通过自由竞争而形成的。其中需求是主因，供给是手段和结果。市场经济的真实总量实际上是由真实需求总量决定的，而不是由供给决定的。真实需求增加可以提振供给。真实需求降低，提振供给不仅不能解决问题，还会造成更多连带问题。</p>
  </li>
</ol>]]></content><author><name></name></author><category term="reading" /><summary type="html"><![CDATA[芒格先生曾说过，常识是最稀缺的认知。违背常识会付出代价。所谓的常识通常也是由这些代价反证出来的。所以对常识的讨论总还是有价值的。]]></summary></entry><entry><title type="html">Data Modeling</title><link href="http://localhost:4000/go_big/2024/09/17/data_modeling.html" rel="alternate" type="text/html" title="Data Modeling" /><published>2024-09-17T00:00:00+08:00</published><updated>2024-09-17T00:00:00+08:00</updated><id>http://localhost:4000/go_big/2024/09/17/data_modeling</id><content type="html" xml:base="http://localhost:4000/go_big/2024/09/17/data_modeling.html"><![CDATA[<h3 id="what-is-data-modeling">What is Data Modeling</h3>
<p>数据建模是指创建一个数据模型的过程。在软件工程中，数据工程师在设计数据库时，将现实世界各类数据及其关系进行分析、抽象，从中找出内在联系，并形式化描述为数据模型，最终建立信息系统的数据库结构。</p>

<h3 id="why-data-modeling">Why Data Modeling</h3>
<ol>
  <li>组织数据：帮助理解和组织数据，使其更易于管理和使用;</li>
  <li>提高效率：通过优化数据结构，提高数据存储和检索的效率;</li>
  <li>确保数据一致性：定义数据的约束和规则，确保数据的一致性和完整性;</li>
  <li>支持业务需求：确保数据模型能够满足业务需求和流程。</li>
</ol>

<h3 id="how-to-model-data">How to Model Data</h3>
<ol>
  <li>需求分析
    <ul>
      <li>定义和分析业务需求；</li>
      <li>确定数据需求和其需要的相应支持。</li>
    </ul>
  </li>
  <li>概念模型
    <ul>
      <li>使用 ER(Entity-relationship) 图来表示数据的高层次结构；</li>
      <li>定义实体以及实体之间的关系。</li>
    </ul>
  </li>
  <li>逻辑模型
    <ul>
      <li>在概念模型的基础上添加更多信息，定义表、列、主键和外键；</li>
      <li>一个概念模型的实现可能需要多个逻辑模型。</li>
    </ul>
  </li>
  <li>物理模型
    <ul>
      <li>根据逻辑模型创建数据库对象；</li>
      <li>考虑数据库的性能、存储和索引。</li>
    </ul>
  </li>
</ol>

<h3 id="data-warehouse">Data Warehouse</h3>
<p>数据仓库是一个用于存储和关系大量数据的系统，它通常从多个源系统中提取数据，并进行清洗、转换和加载供数据分析和 BI 报告使用。数据建模在数据仓库的构建过程中同样尤为重要。</p>

<h4 id="数据仓库建模的类型">数据仓库建模的类型</h4>
<ol>
  <li>Star Schema
    <ul>
      <li>星型模型以事实表为中心，周围是维度表；</li>
      <li>事实表存储度量数据（如销售额），维度表存储描述数据（如时间、地点、产品）。</li>
    </ul>
  </li>
  <li>Snowflake Schema
    <ul>
      <li>雪花模型是星型模型的扩展，将维度表进一步规范化，分解成多个相关的表；</li>
      <li>它的特点是减少了数据冗余，但提高了查询的复杂性。</li>
    </ul>
  </li>
  <li>Fact Constellation Schema
    <ul>
      <li>星座模型包含多个事实表，适用于复杂的业务场景。</li>
    </ul>
  </li>
</ol>

<h3 id="data-lake">Data Lake</h3>
<p>数据湖和数据仓库一样，是存储大量数据的系统，但数据湖不仅仅用于存储结构化的数据，还用来存储半结构化、非结构化的数据。数据湖允许以原始格式存储数据，因此数据的质量和一致性可能较低。一般用于存储用户行为数据，如网站日志、社交媒体数据、视频等，以进行大数据分析和机器学习。</p>

<h3 id="data-lakehouse">Data Lakehouse</h3>
<p>数据湖仓库结合了数据湖和数据仓库的优点，它既能处理结构化数据，也能处理非结构化数据，同时提供了高效的查询性能和数据管理功能。它的代表产品是由 <a href="https://docs.databricks.com/en/delta/index.html">Databricks</a> 开发的基于 Apache Spark 的开源存储层的 <a href="https://delta.io/">Delta Lake</a>，以及用于大规模数据集的高性能表格式的 <a href="https://iceberg.apache.org/">Apache Iceberg</a>。值得后续花时间研究一下。</p>

<h3 id="数据建模过程中常见问题及应对措施">数据建模过程中常见问题及应对措施</h3>
<blockquote>
  <p><strong>Problem</strong>: 业务需求不清晰，以及项目进行中需求变更频繁</p>

  <p><strong>Solution</strong>: 深入沟通，明确需求，并编写详细的需求文档；采用迭代开发的方法，分阶段完善数据模型。</p>
</blockquote>

<blockquote>
  <p><strong>Problem</strong>: 不同数据源数据不一致，数据缺失，数据冗余</p>

  <p><strong>Solution</strong>: 数据清洗，确保数据的一致性和完整性；建立数据验证规则，确保数据的准确性</p>
</blockquote>

<blockquote>
  <p><strong>Problem</strong>: 实体之间关系复杂，数据模型复杂</p>

  <p><strong>Solution</strong>: 以目的驱动简化模型，避免不必要的复杂度；将复杂的数据模型分解为多个模块，分别建模和管理</p>
</blockquote>

<blockquote>
  <p><strong>Problem</strong>: 存储效率低，查询性能差</p>

  <p><strong>Solution</strong>: 根据实际需求，平衡数据的 Normalization 和 Denormalization；为常用查询字段建立索引；在数据建模过程中进行性能测试</p>

  <blockquote>
    <p><em>注：</em></p>

    <p><em>1.规范化（Normalization）：规范化是将数据组织成多个相关表的过程，以减少数据冗余和提高数据一致性；</em></p>

    <p><em>2.反规范化（Denormalization）：反规范化是将数据合并到较少的表中，以减少查询时的多表连接，从而提高查询性能</em></p>
  </blockquote>
</blockquote>

<blockquote>
  <p><strong>Problem</strong>: 敏感数据泄露</p>

  <p><strong>Solution</strong>: 对敏感数据进行加密；严格的访问控制策略，确保只有授权的用户可以访问数据</p>
</blockquote>

<p>To be continued…</p>]]></content><author><name></name></author><category term="go_big" /><summary type="html"><![CDATA[What is Data Modeling 数据建模是指创建一个数据模型的过程。在软件工程中，数据工程师在设计数据库时，将现实世界各类数据及其关系进行分析、抽象，从中找出内在联系，并形式化描述为数据模型，最终建立信息系统的数据库结构。]]></summary></entry><entry><title type="html">Data Visualization</title><link href="http://localhost:4000/go_big/2024/09/16/data_visualization.html" rel="alternate" type="text/html" title="Data Visualization" /><published>2024-09-16T00:00:00+08:00</published><updated>2024-09-16T00:00:00+08:00</updated><id>http://localhost:4000/go_big/2024/09/16/data_visualization</id><content type="html" xml:base="http://localhost:4000/go_big/2024/09/16/data_visualization.html"><![CDATA[<h3 id="background">Background</h3>
<p>在当前信息化时代，我们所有人都正遭受信息过载和数据过剩的困扰。数据可视化的好处便是让我们看到这些数据的规律以及它们的内在联系，便于我们在繁杂的信息中聚焦到最重要的部分。</p>

<p>一位丹麦科普学家 <a href="https://en.wikipedia.org/wiki/Tor_N%C3%B8rretranders">Tor Nørretranders</a> 研究发现，人类通过视觉接收信息的带宽是所有感官中最大的，大概是 1250MB/s，触觉则相当于 USB 传输接口，大约有 125MB/s，其次是听觉、嗅觉、味觉。我们每天通过感官接收到巨量的信息，再经由大脑的处理提取精炼，凭借人类群体社会日益高效的信息交换，构成了这个物种进化源源不断的动力。</p>

<p>数据可视化的本质就是将大量信息压缩成图表，通过视觉的形式将其中的洞见直观地展示出来。当以足够大的数据集为基础，假以正确的问题，并且以正确的方式去处理这些数据，有趣的事情就会出现。</p>

<blockquote>
  <p>Let my dataset change your mindset. ——Hans Rosling</p>
</blockquote>

<h3 id="data-visualization-tools">Data Visualization Tools</h3>
<h4 id="matplotlib-with-seaborn">Matplotlib (with Seaborn)</h4>
<p><a href="https://matplotlib.org/stable/contents.html">Matplotlib</a> 是 Python 数据科学和数据分析领域最常见的可视化库，支持各种图表类型。<a href="https://seaborn.pydata.org/">Seaborn</a> 是基于 Matplotlib 的绘图库，它提供了更高级的接口和更美观的默认样式。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Matplotlib example
</span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X-axis</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Y-axis</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Line Chart</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Seaborn example
</span><span class="n">holdings</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">holdings.csv</span><span class="sh">"</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">holdings</span><span class="p">.</span><span class="n">ticker</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">holdings</span><span class="p">.</span><span class="n">shares</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">holdings</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Learning Path</strong></p>
<ul>
  <li><strong>基础学习</strong>: 从 Matplotlib 的基础开始，学习如何创建基本图表。</li>
  <li><strong>高级功能</strong>: 学习如何使用 Seaborn 创建更高级和美观的图表。</li>
  <li><strong>项目实践</strong>: 通过实际项目练习，巩固所学知识。</li>
</ul>

<h4 id="tableau">Tableau</h4>
<p><a href="https://www.tableau.com/learn/training">Tableau</a> 是一个强大的商业数据可视化工具，支持丰富的图表类型和交互功能。它的拖放界面使得创建复杂的可视化变得简单。</p>

<p><strong>Learning Path</strong></p>
<ul>
  <li><strong>基础学习</strong>: 学习 Tableau 的基本操作和界面。</li>
  <li><strong>高级功能</strong>: 探索 Tableau 的高级功能，如计算字段、参数和仪表盘。</li>
  <li><strong>项目实践</strong>: 通过实际项目练习，创建复杂的商业智能报告和可视化。</li>
</ul>

<p>To be continued…</p>]]></content><author><name></name></author><category term="go_big" /><summary type="html"><![CDATA[Background 在当前信息化时代，我们所有人都正遭受信息过载和数据过剩的困扰。数据可视化的好处便是让我们看到这些数据的规律以及它们的内在联系，便于我们在繁杂的信息中聚焦到最重要的部分。]]></summary></entry><entry><title type="html">no style, please!</title><link href="http://localhost:4000/blog/2024/09/15/no_style_please.html" rel="alternate" type="text/html" title="no style, please!" /><published>2024-09-15T00:00:00+08:00</published><updated>2024-09-15T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/09/15/no_style_please</id><content type="html" xml:base="http://localhost:4000/blog/2024/09/15/no_style_please.html"><![CDATA[<p>前面已经介绍了把个人网站部署在 Github Pages，并给 Github Page 配置定制化域名的流程。考虑到个人信息安全与审查风险，我将在 Aliyun 购买的域名转移到了 <a href="https://www.cloudflare.com/">Cloudflare</a>（可以在 <a href="https://lookup.icann.org/en">ICANN</a> 查看域名注册信息）。为了进一步加快网站部署流程，这里引入一个静态网站生成工具 Jekyll，以及 Jekyll 主题 no style, please! 的应用介绍。</p>

<h3 id="what-is-jekyll">What is Jekyll</h3>
<p><a href="https://jekyllrb.com/">Jekyll</a> 是一款简单的静态网站生成器。它可以将纯文本转化成静态网站，你只需要用自己习惯的标记语言如 Markdown 来编写博客文章，借由 Jekyll 预设模版内置的文章发布、分类、标签、归档等博客所需的功能，使创建和维护博客变得更加简单和高效。</p>

<p><a href="https://riggraz.dev/no-style-please/">no style, please!</a> 是一款几乎没有 CSS 样式的，快速生成，极简的 Jekyll 主题。</p>

<p>结合了这套组件，网站的发布和维护变得非常简单，网站所有者只需专注于输出内容，通过几个简单的命令行，文章就自动部署到网站。</p>

<p>下面来一步一步演示它是如何实现的。</p>

<h3 id="installation">Installation</h3>
<p>Jekyll 是一个 Ruby 项目，安装 Jekyll 首先需要 Ruby 的开发环境。</p>

<h4 id="install-chruby-and-ruby-install-with-homebrew">Install chruby and ruby-install with Homebrew</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nb">install </span>chruby ruby-install xz
</code></pre></div></div>

<h4 id="install-the-latest-stable-version-of-ruby">Install the latest stable version of Ruby</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ruby-install ruby 3.1.3
</code></pre></div></div>

<h4 id="configure-your-shell-to-automatically-use-chruby">Configure your shell to automatically use chruby</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"source </span><span class="si">$(</span>brew <span class="nt">--prefix</span><span class="si">)</span><span class="s2">/opt/chruby/share/chruby/chruby.sh"</span> <span class="o">&gt;&gt;</span> ~/.zshrc
<span class="nb">echo</span> <span class="s2">"source </span><span class="si">$(</span>brew <span class="nt">--prefix</span><span class="si">)</span><span class="s2">/opt/chruby/share/chruby/auto.sh"</span> <span class="o">&gt;&gt;</span> ~/.zshrc
<span class="nb">echo</span> <span class="s2">"chruby ruby-3.1.3"</span> <span class="o">&gt;&gt;</span> ~/.zshrc <span class="c"># run 'chruby' to see actual version</span>
</code></pre></div></div>

<h4 id="install-jekyll">Install Jekyll</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gem <span class="nb">install </span>jekyll bundler
</code></pre></div></div>

<h4 id="create-a-new-gemfile">Create a new Gemfile</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle init
</code></pre></div></div>

<h4 id="edit-gemfile">Edit Gemfile</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gem <span class="s2">"jekyll"</span>, <span class="s2">"~&gt; 4.3.3"</span>
<span class="c"># Change default Jekyll theme to no-style-please</span>
gem <span class="s2">"no-style-please"</span>
</code></pre></div></div>

<p>Run <code class="language-plaintext highlighter-rouge">bundle</code> to install jekyll for your project.</p>

<h3 id="create-a-site">Create a site</h3>
<p>在 _data, _includes, _layouts 中预设网站模版样式，将 Markdown 格式的文件放入指定的 _posts 路径。</p>

<h3 id="build-your-site">Build your site</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div></div>

<p>通过 <code class="language-plaintext highlighter-rouge">localhost:4000</code> 预览生成后的网站，确保无误后将生成后的网站内容推到 Github 触发 Github Pages 部署，至此网站发布成功。</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[前面已经介绍了把个人网站部署在 Github Pages，并给 Github Page 配置定制化域名的流程。考虑到个人信息安全与审查风险，我将在 Aliyun 购买的域名转移到了 Cloudflare（可以在 ICANN 查看域名注册信息）。为了进一步加快网站部署流程，这里引入一个静态网站生成工具 Jekyll，以及 Jekyll 主题 no style, please! 的应用介绍。]]></summary></entry><entry><title type="html">Python Overall</title><link href="http://localhost:4000/go_big/2024/09/15/python.html" rel="alternate" type="text/html" title="Python Overall" /><published>2024-09-15T00:00:00+08:00</published><updated>2024-09-15T00:00:00+08:00</updated><id>http://localhost:4000/go_big/2024/09/15/python</id><content type="html" xml:base="http://localhost:4000/go_big/2024/09/15/python.html"><![CDATA[<h3 id="background">Background</h3>
<p><a href="https://www.python.org/">Python</a> 作为一个高级编程语言 (high-level language)，它的主要工作是将用户编写的 Python 源代码解释成机器能理解的成字节码 (bytecode)，以便让机器执行指令。</p>

<p>相比于机器语言和汇编语言等低级语言 (low-level language)，使用高级编程语言编写代码有一些好处：代码更少也更易读，编写代码所需的时间更短；更容易做到跨平台运行。</p>

<p>安装 Python 时，它很重要的一个组件是 Python Interpreter。解释器的主要工作是解释并运行 Python: 读取用户编写的 Python 源代码；对源代码进行词法分析，将代码分解成词法单元（tokens）；之后进行语法分析，构建抽象语法树（Abstract Syntax Tree，AST）来表示代码的结构；接着将抽象语法树编译成 bytecode，这是一种中间形式的代码；最后，Python 解释器会逐行执行字节码指令，将其转换为机器码并执行，实现源代码的功能。</p>

<p>执行 Python 有两种模式：<strong>immediate mode</strong> &amp; <strong>script mode</strong>. 前者是在 terminal 界面进入 Python 解释器的窗口进行即时交互；第二种是通过编写扩展名为 <strong>py</strong> 的 Python 脚本，使用 Python 解释器去执行该脚本。</p>

<p>在日常调试和数据分析过程中，另一个常用的 python 交互方式是 <a href="https://jupyter.org/">Jupiter Notebook</a>。</p>

<h3 id="installation">Installation</h3>
<p>根据操作系统不同，安装 Python 的方式有很多，详情可以参考 <a href="https://www.python.org/downloads/">Downlaod Python</a>。笔者推荐使用 <a href="https://www.anaconda.com/download/success">Anaconda</a> 作为 Python 的环境管理工具。</p>

<p>检验 Python 环境配置是否已经完成：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">--version</span>
</code></pre></div></div>

<h3 id="data-type">Data Type</h3>
<p>Python 中常见的数据类型包括：</p>

<ol>
  <li>整数（int）：表示整数值，如 5、-3;</li>
  <li>浮点数（float）：表示带有小数点的数值，如 3.14、-0.001;</li>
  <li>字符串（str）：表示文本数据，如 ‘hello’、”world”;</li>
  <li>布尔值（bool）：表示逻辑值，只有两个取值：True 和 False;</li>
  <li>列表（list）：有序、可变的集合，如 [1, 2, 3];</li>
  <li>元组（tuple）：有序、不可变的集合，如 (1, 2, 3);</li>
  <li>集合（set）：无序、不重复的集合，如 {1, 2, 3};</li>
  <li>字典（dict）：无序的键值对集合，如 {‘name’: ‘Alice’, ‘age’: 30}。</li>
</ol>

<h3 id="script-structure">Script Structure</h3>
<p>在生产级别的 Python 编程中，一个 Python 脚本通常由以下部分构成：</p>

<p><strong>1. 导入模块</strong></p>

<p>在开头导入代码所需的模块，以扩展 Python 的功能和复用代码。例如：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">sys</span>
<span class="kn">from</span> <span class="n">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
</code></pre></div></div>
<p>注：你可以在 <a href="https://docs.python.org/3/library/">Python 标准库</a>, <a href="https://pypi.org/">PyPI (Python Package Index)</a>, <a href="https://awesome-python.com/">Awesome Python</a> 找到丰富的 Python 库及其功能详情；也可以使用 <code class="language-plaintext highlighter-rouge">help(function_name)</code> 可以查看已导入方法内置的说明。</p>

<p><strong>2. 全局变量和常量定义</strong></p>

<p>定义全局变量和常量，用于在整个脚本中共享数据。例如：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MAX_RETRIES</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">DEBUG_MODE</span> <span class="o">=</span> <span class="bp">True</span>
</code></pre></div></div>

<p><strong>3. 函数和类定义</strong></p>

<p>编写函数和类来组织代码逻辑和实现功能。函数用于封装可重复使用的代码块，类用于组织相关属性和方法。例如：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_area</span><span class="p">(</span><span class="n">radius</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">3.14</span> <span class="o">*</span> <span class="n">radius</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">class</span> <span class="nc">Person</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">age</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="n">self</span><span class="p">.</span><span class="n">age</span> <span class="o">=</span> <span class="n">age</span>
</code></pre></div></div>

<p>注：其中 <code class="language-plaintext highlighter-rouge">__init__</code> 是 Python 中的构造函数，用于在创建对象时初始化对象的属性。</p>

<p><strong>4. 主程序逻辑</strong></p>

<p>编写主要的程序逻辑，包括流程控制、数据处理、调用函数等。这部分代码通常位于脚本的最顶层，用于实现脚本的主要功能。例如：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">radius</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">area</span> <span class="o">=</span> <span class="nf">calculate_area</span><span class="p">(</span><span class="n">radius</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The area of the circle is: </span><span class="si">{</span><span class="n">area</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>注：<code class="language-plaintext highlighter-rouge">if __name__ == "__main__"</code> 被用来判断当前脚本是否作为主程序运行，加上这段代码可以防止在被其他脚本导入时执行了不必要的代码；当 Python 解释器运行一个脚本时，<code class="language-plaintext highlighter-rouge">__name__</code> 变量会被设置为 <code class="language-plaintext highlighter-rouge">"__main__"</code>，而如果该脚本被作为模块导入到其他脚本时，<code class="language-plaintext highlighter-rouge">__name__</code> 变量会被设置为模块的名称。</p>

<p><strong>5. 异常处理</strong></p>

<p>添加适当的异常处理机制，以处理可能出现的错误和异常情况，确保程序的稳定性和可靠性。例如：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">/</span> <span class="mi">0</span>
<span class="k">except</span> <span class="nb">ZeroDivisionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Error: Division by zero!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>
<p>注：可以通过 <a href="https://docs.python.org/3/library/exceptions.html">Python 异常文档</a> 查看完整的 Exception 列表，也可以通过 <code class="language-plaintext highlighter-rouge">help(ExceptionNameError)</code> 查看特定异常的详细信息。</p>

<p><strong>6. 日志记录</strong></p>

<p>使用日志记录模块记录程序运行时的信息、警告和错误，以便进行故障排查和监控。例如：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">logging</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">basicConfig</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="sh">'</span><span class="s">app.log</span><span class="sh">'</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="p">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sh">'</span><span class="s">Program started</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>To be continued…</p>]]></content><author><name></name></author><category term="go_big" /><summary type="html"><![CDATA[Background Python 作为一个高级编程语言 (high-level language)，它的主要工作是将用户编写的 Python 源代码解释成机器能理解的成字节码 (bytecode)，以便让机器执行指令。]]></summary></entry><entry><title type="html">延迟退休可能是对一代人违约</title><link href="http://localhost:4000/reading/2024/09/14/delaying_retirement.html" rel="alternate" type="text/html" title="延迟退休可能是对一代人违约" /><published>2024-09-14T00:00:00+08:00</published><updated>2024-09-14T00:00:00+08:00</updated><id>http://localhost:4000/reading/2024/09/14/delaying_retirement</id><content type="html" xml:base="http://localhost:4000/reading/2024/09/14/delaying_retirement.html"><![CDATA[<p><a href="http://jingji.cntv.cn/2013/04/20/ARTI1366415217203221.shtml">Read Original</a></p>

<table>
  <tbody>
    <tr>
      <td>date_saved: 2024-09-14 06:59:46</td>
      <td>date_published:2013-04-20 08:00:00</td>
    </tr>
  </tbody>
</table>

<p>加速到来的人口老龄化又一次把延长退休年龄推向了风口浪尖。全国社保基金理事会党组书记戴相龙日前受访时表示，面对30多年后的人口老龄化高峰，国家管理的公共养老金收支会有较大缺口。他建议应逐步延长退休年龄，提出采取每5年把退休年龄延长1岁的制度设计。专家称阶梯式、渐进式的延长退休年龄是国际上通行的做法。</p>

<p>相比此前“一步到位”的激进方案，“5年延1岁”的设计温和多了。不过这是一个容易撕裂社会情感的话题，争议和阻力并未因方案的温和而减少。这种撕裂从不同阶层的态度就可以看出来。从舆情分析看，支持延迟退休的主要是两大社会群体：官员和专家。其他阶层则多数反对延迟退休，辛苦了一辈子快到领退休金安享天年的时候，突然要延迟退休，心理上接受不了。</p>

<p>分析支持者和反对者可以看到一个有意思的现象，支持延迟退休者，大多是既有养老体制的受益者，而反对者则多是相对被剥夺群体，甚至是受损者。中国在养老上实行的是双轨制，机关和事业单位发养老金，而企业单位是自己缴养老保险，从企业退休领到的养老金往往比从机关退休少得多。官员和专家支持延迟退休方案，因为这不仅不触动他们的既得利益，还能给他们带来利益；多数公众之所以反对，是因为这种改革没有触动他们最反对的养老双轨制。人们不患寡而患不均，养老账户的空账问题虽然严重，但他们可以接受一个低的养老金，可不能接受有些人比自己高那么多。人们其实不是反对延迟退休，而更多是反感政府在双轨制改革上对民意的漠视，动不了官员和专家，就拣软柿子捏。</p>

<p>公众最大的期待是养老双轨制的并轨，把每个国民一起置于平等的体制下，先解决平等问题，再解决空账问题。而“延迟退休”则回避了这个核心问题，这正是作为双轨制受益者的官员和专家所期待的。</p>

<p>延迟退休对官员和专家是有益的，官员大多希望延迟退休，因为延迟的不仅是工作，更是权力利益。按现在的退休年龄，一般官员到了60岁就得退，很多人都不适应这种退休后手中无权的落寞感，延迟退休则延长了他们的政治生命。专家也是如此，大学和科研院所多已高度行政化，是官场的翻版，退休的院长和教授自然比不上在位的。但对普通劳动者而言，工作则是一种负担，没有权力利益，辛辛苦苦熬了一辈子好不容易熬到了退休，却又赶上了延迟退休。延迟退休可能会让这些人产生一种双重的双损感：双轨制已让他们受损害，延迟退休更进一步伤害了他们的利益。</p>

<p>从另一个角度看，延迟退休可能是对一代人的违约。制度和政策应该保持一定的稳定性，尤其是这个政策涉及大的公众利益时，应给公众一个稳定的预期。什么年龄退休，什么时候能拿到养老金，是国家对国民的一种承诺和约定，不能轻易打破这样的契约。不能以“延长退休年龄是国际惯例”作为打破契约的借口，发达国家延长退休年龄是经由正当的法律程序和民主途径与国民协商而订立的契约，不能想当然和随意地改变。即使因为空账问题需要调整，也应经过民主决策，让每个利益群体都参与到博弈中。</p>

<p>而且，不能一说到养老金缺口，就把所有责任都转嫁给社会和公众，在提起延迟退休这个议题时，首先要追问政府在养老金问题上有没有承担应有的投入，履行好应有的政府保障责任。毕竟，公众纳的税不是白纳的，里面有对自己未来养老的一份投资。</p>

<p>在养老问题上，改革的次序应该是，先改掉双轨制，再谈延迟退休；先加大政府投入，再谈公众责任。</p>]]></content><author><name></name></author><category term="reading" /><summary type="html"><![CDATA[Read Original]]></summary></entry><entry><title type="html">On Elon’s Comments on Taylor Swift’s Endorses Kamala Harris</title><link href="http://localhost:4000/blog/2024/09/11/elon_comments_on_taylor.html" rel="alternate" type="text/html" title="On Elon’s Comments on Taylor Swift’s Endorses Kamala Harris" /><published>2024-09-11T00:00:00+08:00</published><updated>2024-09-11T00:00:00+08:00</updated><id>http://localhost:4000/blog/2024/09/11/elon_comments_on_taylor</id><content type="html" xml:base="http://localhost:4000/blog/2024/09/11/elon_comments_on_taylor.html"><![CDATA[<p>看了绝大多数<a href="https://weibo.com/1893801487/OwtSU6V57">简中网友解读</a>的马斯克 X 发言都不全面或者说是错的。</p>
<blockquote>
  <p>Fine Taylor … you win … I will give you a child and guard your cats with my life</p>
</blockquote>

<p>首先这是回应泰勒在 Instagram 宣称自己将会在 2024 年总统大选给哈里斯投票，并署名 Childless Cat Lady 的发文。</p>
<blockquote>
  <p>Like many of you, I watched the debate tonight. If you haven’t already, now is a great time to do your research on the issues at hand and the stances these candidates take on the topics that matter to you the most. As a voter, I make sure to watch and read everything I can about their proposed policies and plans for this country.</p>

  <p>Recently I was made aware that Al of ‘me’ falsely endorsing Donald Trump’s presidential run was posted to his site. It really conjured up my fears around Al, and the dangers of spreading misinformation. It brought me to the conclusion that I need to be very transparent about my actual plans for this election as a voter. The simplest way to combat misinformation is with the truth.</p>

  <p>I will be casting my vote for Kamala Harris and Tim Walz in the 2024 Presidential Election. I’m voting for @kamalaharris because she fights for the rights and causes I believe need a warrior to champion them. I think she is a steady-handed, gifted leader and I believe we can accomplish so much more in this country if we are led by calm and not chaos. I was so heartened and impressed by her selection of running mate @timwalz, who has been standing up for LGBTQ+ rights, IVF, and a woman’s right to her own body for decades.</p>

  <p>I’ve done my research, and l’ve made my choice. Your research is all yours to do, and the choice is yours to make. I also want to say, especially to first time voters: Remember that in order to vote, you have to be registered! I also find it’s much easier to vote early. I’ll link where to register and find early voting dates and info in my story.</p>

  <p>With love and hope,</p>

  <p>Taylor Swift</p>

  <p>Childless Cat Lady</p>
</blockquote>

<p>这个署名是在回应先前特朗普副手 Vance 攻击哈里斯时表示美国就是被这些没孩子的养猫女人搞乱的。</p>
<blockquote>
  <p>We’re effectively run in this country via the Democrats Bea, via our corporate oligarchs, by a bunch of childless cat ladies who are miserable at their own lives and the choices that they’ve made, and so they want to make the rest of the country miserable too.</p>
</blockquote>

<p>马斯克今年旗帜鲜明地站台特朗普，他认为自己的第一个孩子也是因为受到觉醒文化影响，成年后选择变性并和马斯克断绝了关系。</p>

<p>马斯克回应泰勒说好吧，你不是没孩子吗（自认为幽默地混淆 <em>不想要孩子且没有</em> 和 <em>想要孩子但没有</em>），我给你个孩子（一起生一个），而且我还会用生命照顾好你的猫。中年男人的油腻感扑面而来。</p>

<p>抛开这些事件的戏剧性不说，从两个党派，到社会名人，再到这些人背后的一众簇拥，可以说美国内部对立情绪越来越浓。非法移民继续涌入，社会治安持续恶化，贫富差距不断加大。如果经济按衰退预期走，到一定的程度美国可能会需要一场战争来唤起美国的团结。</p>

<p>而中国目前正陷于房地产泡沫破灭，老龄少子化，地方财政捉襟见肘。届时可能也需要一个事件转移全社会的矛盾。</p>

<p>作为普通人，只有好好锻炼身体，照顾好家人，学习一切有趣的东西，然后祈祷科技变革的发生早于社会变革。</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[看了绝大多数简中网友解读的马斯克 X 发言都不全面或者说是错的。 Fine Taylor … you win … I will give you a child and guard your cats with my life]]></summary></entry><entry><title type="html">Be a Good Data Engineer - Spark</title><link href="http://localhost:4000/go_big/2024/09/06/spark.html" rel="alternate" type="text/html" title="Be a Good Data Engineer - Spark" /><published>2024-09-06T00:00:00+08:00</published><updated>2024-09-06T00:00:00+08:00</updated><id>http://localhost:4000/go_big/2024/09/06/spark</id><content type="html" xml:base="http://localhost:4000/go_big/2024/09/06/spark.html"><![CDATA[<h3 id="background">Background</h3>
<p>进入21世纪以来，随着互联网的发展，各类社交媒体，科技软件，传感器等工具一刻不停地生成着越来越多地数据。有一个说法是:</p>
<blockquote>
  <p>人类现有90%的数据来自于过去两年。</p>
</blockquote>

<p>这些数据不管是否得到了很好的利用，谁也不能否认它们可以带来的高价值，特别是现阶段以数据和能源作为养料的 artificial intelligence。</p>

<p>大数据集对传统单机数据库造成了不小的麻烦，面对如何处理大量数据并从中洞见到有价值信息的需求，Google 三篇论文——<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/gfs-sosp2003.pdf">Google File System</a> at 2003, <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/mapreduce-osdi04.pdf">MapReduce</a> at 2004, <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable</a> at 2006 启发了无数贡献者，推出了一个又一个大数据开源项目，为人类掀开了大数据时代的巨幕。</p>

<p>其中，受 Google File System 和 MapReduce 启发，yahoo 的一组工程师在 2006 年开源了 <a href="https://hadoop.apache.org/">Hadoop</a>， Hadoop 引入了两个技术，分布式存储 (HDFS) 和分布式计算 (MapReduce) ——将大文件切分成小块，分布保存在集群中的多个机器上，每个小块文件备份成多份以防某个节点出错导致文件丢失；处理计算任务时，也将任务分成多个单元，由多个 excutor 分别执行计算操作，再将结果合并在一起。</p>

<p>这种处理方法利用大量廉价机器使得大数据计算得以实现。但由于其内在机制限制，大量的中间计算结果需要落地磁盘，过程中产生的 I/O 导致整个计算花费大量的时间。随着计算机硬件水平和成本的降低，Spark 作为一个内存计算引擎，凭借其更加高效的计算的优势逐渐取代了 MapReduce 的功能。</p>

<h3 id="apache-spark">Apache Spark</h3>
<h4 id="intro">Intro</h4>
<p><a href="https://spark.apache.org/">Apache Spark</a> 是由加州大学伯克利分校的一些研究员在 2009 年推出的一个研究项目，目的就是为了解决 Hadoop 的上述限制。Spark 推出了一个 RDD(Resilient Distributed Dataset) 的概念，使数据得以数据集的形式存储在内存中，使得数据读取和处理都更加快速。</p>

<h4 id="language">Language</h4>
<p>Spark 源码是由 Scala 语言编写，但提供了 Python, Scala, Java, R 的 API 接口进行编程。Python 由于其易用性、丰富的资源库以及和 Data Science, Machine Learning 的紧密结合，其已经成为 Spark 主推的编程语言。下面的演示都以 Python 为例。</p>

<h4 id="installation">Installation</h4>
<p>Make sure java is installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>default-jre
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>pyspark
</code></pre></div></div>

<h4 id="concepts">Concepts</h4>
<h5 id="spark-任务的运行模式">Spark 任务的运行模式</h5>
<ul>
  <li>Local Mode: 单机运行，资源有限，适合用于手动调试；</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit word_count.py

python word_count.py

spark-submit <span class="nt">--master</span> <span class="nb">local</span><span class="o">[</span><span class="k">*</span><span class="o">]</span> word_count.py
</code></pre></div></div>

<ul>
  <li>Client Mode: 任务的 Driver 在本地运行，实际计算任务分发到集群中的 Worker 节点运行，由于 Driver 在本地运行，可以方便地查看日志和调试信息；</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit <span class="nt">--master</span> yarn <span class="nt">--deploy-mode</span> client word_count.py

spark-submit <span class="nt">--master</span> spark://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> client word_count.py

spark-submit <span class="nt">--master</span> k8s://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> client word_count.py
</code></pre></div></div>

<ul>
  <li>Cluster Mode: 提交任务到分布式集群运行，可以利用更高的计算性能，适用于生产环境；</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit <span class="nt">--master</span> yarn <span class="nt">--deploy-mode</span> cluster word_count.py

spark-submit <span class="nt">--master</span> spark://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> cluster word_count.py

spark-submit <span class="nt">--master</span> k8s://&lt;master-url&gt;:&lt;port&gt; <span class="nt">--deploy-mode</span> cluster word_count.py
</code></pre></div></div>

<h5 id="spark-资源管理器">Spark 资源管理器</h5>
<p>Spark 资源管理器负责分配和调度集群资源(CPU, Memory, Disk, Network)</p>
<ol>
  <li><a href="https://spark.apache.org/docs/latest/spark-standalone.html">Standalone</a>: 默认的资源管理器；</li>
  <li><a href="https://spark.apache.org/docs/latest/running-on-yarn.html">YARN</a>: 由 Hadoop 提供；</li>
  <li><a href="https://spark.apache.org/docs/latest/running-on-mesos.html">Mesos</a>(Deprecated): 由 Apache Mesos 提供;</li>
  <li><a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html">Kubernetes</a>: 由 Kubernetes 提供</li>
</ol>

<h5 id="spark-任务物理运行原理">Spark 任务物理运行原理</h5>
<p><strong>1. 客户端提交任务</strong></p>

<p>用户通过 spark-submit 命令提交一个 Spark 应用程序。这个应用程序包含了用户的代码和依赖项。</p>

<p><strong>2. Driver 进程启动</strong></p>

<p>Spark 应用程序在 Driver 进程中启动。Driver 负责以下任务：</p>
<ul>
  <li>解析用户代码: 解析并执行用户代码中的 transformation 和 action 操作；</li>
  <li>生成 DAG: 将用户代码中的一系列 transformation 操作转换为一个有向无环图（DAG）；</li>
  <li>任务调度: 将 DAG 划分为多个阶段（stages），每个阶段包含一组可以并行执行的任务（tasks）。</li>
</ul>

<p><strong>3. 资源管理器分配资源</strong></p>

<p>Driver 向集群的资源管理器（如 YARN、Mesos 或 Kubernetes）请求资源。资源管理器分配资源并启动 Executor 进程。</p>

<p><strong>4. Executor 进程启动</strong></p>

<p>Executor 进程在集群的工作节点（Worker Nodes）上启动。每个 Executor 负责以下任务：</p>
<ul>
  <li>执行任务: 执行由 Driver 分配的任务；</li>
  <li>存储数据: 缓存和存储中间结果数据；</li>
  <li>报告状态: 向 Driver 报告任务的执行状态和结果。</li>
</ul>

<p><strong>5. 任务执行</strong></p>

<p>Driver 将任务分配给各个 Executor。任务的执行过程如下：</p>
<ul>
  <li>读取数据: 从数据源（如 HDFS、S3、Kafka 等）读取数据；</li>
  <li>执行计算: 根据用户代码中的 transformation 操作对数据进行处理；</li>
  <li>写入结果: 将计算结果写入到指定的存储位置（如 HDFS、S3、数据库等）。</li>
</ul>

<p><strong>6. 任务监控和容错</strong></p>

<p>Spark 提供了多种机制来监控和处理任务的执行：</p>
<ul>
  <li>任务重试: 如果某个任务失败，Spark 会自动重试该任务；</li>
  <li>数据备份: Spark 会将数据分片（partitions）备份到多个节点，以防止数据丢失；</li>
  <li>监控工具: Spark 提供了 Web UI 和其他监控工具，帮助用户监控任务的执行状态和性能。</li>
</ul>

<p><strong>7. 任务完成</strong></p>

<p>当所有任务都成功完成后，Driver 会将最终结果返回给用户或写入到指定的存储位置。然后，Driver 和 Executor 进程会正常退出，释放资源。</p>

<h4 id="usage-scenarios">Usage scenarios</h4>
<h5 id="spark-sql">Spark SQL</h5>
<p><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> 是 Spark 用于处理结构化数据的模块，它提供了一个编程抽象，支持 SQL 查询、Dataframe API 和 Dataset API，而不需要了解底层分布式计算的细节。</p>

<p>它与 <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">RDD</a> 的不同在于：</p>
<ol>
  <li>RDD 提供了底层的 <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations">RDD API</a>，用户需要编写更多的代码来进行数据处理，适合处理非结构化数据；Spark SQL 将数据抽象为 <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html">Dataframe</a> 或 Dataset，提供了更高级的优化和执行策略；</li>
  <li>RDD 类型安全，DataFrame API 是非类型安全的，Dataset API 提供了类型安全的操作；</li>
  <li>Spark SQL 支持 <a href="https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html">Hive</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-avro.html">Avro</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html">Parquet</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-orc.html">ORC</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-json.html">JSON</a>、<a href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html">JDBC</a> 等多种数据源，而 RDD 需要用户自己实现对不同数据源的支持；</li>
</ol>

<p><strong>使用 Dataframe API 案例</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import necessary libraries
</span><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="n">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="n">regexp_replace</span>

<span class="c1"># Create a Spark session
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">SparkSQLExample</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="c1"># Read CSV data into a DataFrame
</span><span class="n">ark_holdings_df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">csv</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="sh">"</span><span class="s">test_file/ark_holdings.csv</span><span class="sh">"</span><span class="p">,</span> 
                    <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                    <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Perform a query
</span><span class="n">ark_holdings_df</span> <span class="o">=</span> <span class="n">ark_holdings_df</span> \
    <span class="p">.</span><span class="nf">withColumn</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">,</span> <span class="nf">regexp_replace</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">),</span> <span class="sh">"</span><span class="s">[$,]</span><span class="sh">"</span><span class="p">,</span> <span class="sh">""</span><span class="p">).</span><span class="nf">cast</span><span class="p">(</span><span class="sh">"</span><span class="s">double</span><span class="sh">"</span><span class="p">))</span>
<span class="n">result_df</span> <span class="o">=</span> <span class="n">ark_holdings_df</span> \
    <span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">ark_holdings_df</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">ticker</span><span class="sh">"</span><span class="p">)</span> <span class="o">==</span> <span class="sh">"</span><span class="s">U</span><span class="sh">"</span><span class="p">).</span><span class="nf">select</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">)).</span><span class="nf">first</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> \
    <span class="p">.</span><span class="nf">select</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">ticker</span><span class="sh">"</span><span class="p">),</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">))</span> \
    <span class="p">.</span><span class="nf">orderBy</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">).</span><span class="nf">desc</span><span class="p">())</span>

<span class="c1"># Show the result
</span><span class="n">result_df</span><span class="p">.</span><span class="nf">show</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Stop the Spark session
</span><span class="n">spark</span><span class="p">.</span><span class="nf">stop</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>使用 RDD API 案例</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import necessary libraries
</span><span class="kn">from</span> <span class="n">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>

<span class="c1"># Create a Spark context
</span><span class="n">sc</span> <span class="o">=</span> <span class="nc">SparkContext</span><span class="p">(</span><span class="sh">"</span><span class="s">local</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">RDDExample</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Read CSV data into an RDD
</span><span class="n">ark_holdings_rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="nf">textFile</span><span class="p">(</span><span class="sh">"</span><span class="s">test_file/ark_holdings.csv</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Perform a query
</span><span class="n">result_rdd</span> <span class="o">=</span> <span class="n">ark_holdings_rdd</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="sh">"</span><span class="s">UNITY</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">line</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'"'</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">))</span>

<span class="c1"># Show the result
</span><span class="nf">print</span><span class="p">(</span><span class="n">result_rdd</span><span class="p">.</span><span class="nf">collect</span><span class="p">())</span>

<span class="c1"># Stop the Spark context
</span><span class="n">sc</span><span class="p">.</span><span class="nf">stop</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>RDD 和 Dataframe 相互转换</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Dataframe 转 RDD
</span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">csv</span><span class="p">(</span><span class="sh">"</span><span class="s">test_file/ark_holdings.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">rdd</span>

<span class="c1"># RDD 转 Dataframe
</span><span class="n">rdd</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="nf">parallelize</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">Alice</span><span class="sh">"</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="sh">"</span><span class="s">Bob</span><span class="sh">"</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="sh">"</span><span class="s">Cathy</span><span class="sh">"</span><span class="p">)])</span>
<span class="c1"># 定义 DataFrame 的 schema
</span><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nc">Row</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])).</span><span class="nf">toDF</span><span class="p">()</span>

</code></pre></div></div>
<p><strong>Action 算子</strong></p>

<p>Spark 有一个特性 Lazy Evaluation，当一个 Spark 操作被提交时，Spark 并不会立即执行任务，而是将任务转换为一系列的 RDD 操作，只有当遇到 Action 算子时，Spark 才会真正执行任务。通过 Lazy Evaluation, Spark 可以优化执行计划，避免存储大量中间结果，提高任务的执行效率。</p>

<p>常见的 RDD action 算子有：collect, count, first, take, takeSample, reduce, fold, aggregate, foreach, saveAsTextFile, saveAsSequenceFile, saveAsObjectFile, countByKey, countByValue, takeOrdered, top, foreachPartition.</p>

<p>常见的 DataFrame action 算子有：collect, count, first, take, show, head, foreach, write, describe, summary, toPandas.</p>

<p>总的来说，Spark SQL 更适合处理结构化数据和需要高效查询优化的场景，而 RDD 更适合处理非结构化数据和需要自定义处理逻辑的场景。</p>

<p>需要特别注意的是，Spark SQL 性能调优需要考虑的因素很多，包括数据倾斜、数据分区、数据格式、数据压缩、数据缓存等，根据实际情况参考 <a href="https://spark.apache.org/docs/latest/sql-performance-tuning.html">Spark SQL 性能调优</a> 进行调整。</p>

<h5 id="pandas-api-on-spark">Pandas API on Spark</h5>
<p><a href="https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html#">Pandas API on Spark</a> 是 Spark 提供的用于处理大规模数据集的 API，它提供了与 <a href="https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html">Pandas</a> 类似的 API，使得用户可以方便地将 Pandas 的代码迁移到 Spark 上运行。</p>

<p><strong>Pandas API on Spark 案例</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import necessary libraries
</span><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Create a Spark session
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">PandasAPISpark</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="c1"># Create a DataFrame
</span><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="sh">"</span><span class="s">Alice</span><span class="sh">"</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">Bob</span><span class="sh">"</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">Cathy</span><span class="sh">"</span><span class="p">,</span> <span class="mi">35</span><span class="p">)]</span>
<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="nf">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>

<span class="c1"># Convert DataFrame to Pandas DataFrame
</span><span class="n">pandas_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">toPandas</span><span class="p">()</span>

<span class="c1"># Perform operations on Pandas DataFrame
</span><span class="n">pandas_df</span><span class="p">[</span><span class="sh">'</span><span class="s">age_12_years_ago</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pandas_df</span><span class="p">[</span><span class="sh">'</span><span class="s">age</span><span class="sh">'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">12</span>

<span class="c1"># Convert Pandas DataFrame back to Spark DataFrame
</span><span class="n">spark_df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="nf">createDataFrame</span><span class="p">(</span><span class="n">pandas_df</span><span class="p">)</span>

<span class="c1"># Show the result
</span><span class="n">spark_df</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Stop the Spark session
</span><span class="n">spark</span><span class="p">.</span><span class="nf">stop</span><span class="p">()</span>
</code></pre></div></div>

<h5 id="mllib">MLlib</h5>
<p><a href="https://spark.apache.org/docs/latest/ml-guide.html">MLlib</a> 是 Apache Spark 的机器学习库，提供了一系列用于构建和训练机器学习模型的工具和算法。MLlib 旨在简化机器学习的工作流程，并支持大规模数据集的处理。</p>

<h5 id="graphx">GraphX</h5>
<p><a href="https://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> 是 Apache Spark 的图计算库，旨在处理大规模图数据。GraphX 提供了一个统一的 API，用于图的创建、操作和分析，支持图的并行计算。</p>

<h5 id="structured-streaming">Structured Streaming</h5>
<p><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Structured Streaming</a> 是 Spark 的流处理库，用于构建可扩展的流式数据处理应用程序。Structured Streaming 允许用户以批处理的方式处理流数据，同时保持了与批处理相同的编程模型。</p>

<p><strong>Structured Streaming 案例</strong></p>

<p><strong>1. 环境准备</strong></p>
<ul>
  <li>安装 zookeeper, kafka 以及必要的一些依赖包
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nb">install </span>zookeeper kafka
pip <span class="nb">install </span>kafka-python
wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.0/spark-sql-kafka-0-10_2.12-3.0.0.jar
wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.12/3.0.0/spark-streaming-kafka-0-10-assembly_2.12-3.0.0.jar
wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.9.0/commons-pool2-2.9.0.jar
</code></pre></div>    </div>
  </li>
  <li>启动 zookeeper
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>zookeeper-server-start /opt/homebrew/etc/kafka/zookeeper.properties
</code></pre></div>    </div>
  </li>
  <li>启动 kafka
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-server-start /opt/homebrew/etc/kafka/server.properties
</code></pre></div>    </div>
  </li>
  <li>创建 topic
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-topics <span class="nt">--create</span> <span class="nt">--topic</span> structured_streaming_demo <span class="nt">--bootstrap-server</span> localhost:9092 <span class="nt">--partitions</span> 1 <span class="nt">--replication-factor</span> 1
</code></pre></div>    </div>
  </li>
  <li>启动 kafka 生产者
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-console-producer <span class="nt">--topic</span> structured_streaming_demo <span class="nt">--bootstrap-server</span> localhost:9092
</code></pre></div>    </div>
  </li>
  <li>启动 spark shell
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyspark <span class="nt">--jars</span> jars/spark-sql-kafka-0-10_2.12-3.0.0.jar,jars/spark-streaming-kafka-0-10-assembly_2.12-3.0.0.jar,jars/commons-pool2-2.9.0.jar
</code></pre></div>    </div>
  </li>
</ul>

<p><strong>2. 代码实现</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="n">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Create SparkSession
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
<span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">StructuredStreamingExample</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">config</span><span class="p">(</span><span class="sh">"</span><span class="s">spark.sql.streaming.forceDeleteTempCheckpointLocation</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">true</span><span class="sh">'</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="c1"># Create DataFrame representing the stream of input data
</span><span class="n">data</span> <span class="o">=</span> <span class="n">spark</span> \
    <span class="p">.</span><span class="n">readStream</span> \
    <span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="sh">"</span><span class="s">kafka</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">option</span><span class="p">(</span><span class="sh">"</span><span class="s">kafka.bootstrap.servers</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">localhost:9092</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">option</span><span class="p">(</span><span class="sh">"</span><span class="s">subscribe</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">structured_streaming_demo</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">option</span><span class="p">(</span><span class="sh">"</span><span class="s">startingOffsets</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">earliest</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">load</span><span class="p">()</span>

<span class="c1"># Select the necessary columns
</span><span class="n">ark_holdings</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">select</span><span class="p">(</span><span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">date</span><span class="sh">"</span><span class="p">),</span> 
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">fund_name</span><span class="sh">"</span><span class="p">),</span>
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">2</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">company_name</span><span class="sh">"</span><span class="p">),</span>
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">3</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">ticker</span><span class="sh">"</span><span class="p">),</span>
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">6</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">shares</span><span class="sh">"</span><span class="p">),</span>
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">7</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">market_value</span><span class="sh">"</span><span class="p">),</span>
                    <span class="nf">split</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)[</span><span class="mi">5</span><span class="p">].</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">))</span>

<span class="c1"># if market_value &gt; 1000000, show the data
</span><span class="n">weight_stock</span> <span class="o">=</span> <span class="n">ark_holdings</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">'</span><span class="s">market_value</span><span class="sh">'</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1000000</span><span class="p">)</span>

<span class="c1"># Start the query to stream the data
</span><span class="n">query</span> <span class="o">=</span> <span class="n">weight_stock</span> \
<span class="p">.</span><span class="n">writeStream</span> \
<span class="p">.</span><span class="nf">outputMode</span><span class="p">(</span><span class="sh">"</span><span class="s">append</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="sh">"</span><span class="s">console</span><span class="sh">"</span><span class="p">)</span> \
<span class="p">.</span><span class="nf">start</span><span class="p">()</span>

<span class="c1"># Wait for the query to finish
</span><span class="n">query</span><span class="p">.</span><span class="nf">awaitTermination</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="spark-性能调优">Spark 性能调优</h3>
<p>Spark 性能调优是一个复杂的过程，除了代码层面的优化，这是一些<a href="https://spark.apache.org/docs/latest/configuration.html">配置参数</a>，以下是一些常见的性能调优方法：</p>

<h4 id="资源管理">资源管理</h4>
<ol>
  <li><strong>调整资源分配</strong>: 根据集群的资源情况，<a href="https://spark.apache.org/docs/latest/configuration.html#application-properties">合理分配</a> CPU、内存、磁盘和网络资源，确保每个任务都能获得足够的资源；</li>
  <li><strong>设置资源预取</strong>: 在任务开始前，预先分配好所需的资源，减少任务启动时间；</li>
  <li><strong>监控资源使用情况</strong>: 使用 Spark 的<a href="https://spark.apache.org/docs/latest/monitoring.html">监控工具</a>，如 Spark Web UI、Ganglia、Prometheus 等，监控集群的资源使用情况，及时发现并解决资源瓶颈。</li>
</ol>

<h4 id="数据分区">数据分区</h4>
<ol>
  <li><strong>合理设置分区数</strong>: 根据数据量和集群的资源情况，合理设置分区数，减少数据倾斜和资源浪费；</li>
  <li><strong>数据倾斜</strong>: 数据倾斜是指某些分区中的数据量远大于其他分区，导致某些任务运行时间过长，甚至导致任务失败。可以通过增加分区数、调整分区大小、使用随机分区等方式解决数据倾斜问题；</li>
  <li><strong>数据预分区</strong>: 在数据加载时，根据业务需求和集群的资源情况，合理设置数据分区，减少数据倾斜和资源浪费。</li>
</ol>

<h4 id="数据格式">数据格式</h4>
<ol>
  <li><strong>选择合适的数据格式</strong>: 根据数据的特点和业务需求，选择合适的数据格式，如 <a href="https://parquet.apache.org/">Parquet</a>、<a href="https://orc.apache.org/">ORC</a>、<a href="https://avro.apache.org/">Avro</a> 等，减少数据存储和传输的开销；</li>
  <li><strong>数据压缩</strong>: 使用<a href="https://spark.apache.org/docs/latest/configuration.html#compression-and-serialization">数据压缩算法</a>，如 <a href="https://github.com/google/snappy">Snappy</a>、<a href="https://www.gnu.org/software/gzip/">Gzip</a>、<a href="https://www.oberhumer.com/opensource/lzo/">LZO</a> 等，减少数据存储和传输的开销。</li>
</ol>

<h4 id="数据缓存">数据缓存</h4>
<ol>
  <li><strong>合理设置缓存策略</strong>: 根据数据的特点和业务需求，合理设置<a href="https://spark.apache.org/docs/latest/configuration.html#memory-management">缓存策略</a>，减少数据读取和处理的开销。</li>
</ol>

<p>To be continued…</p>]]></content><author><name></name></author><category term="go_big" /><summary type="html"><![CDATA[Background 进入21世纪以来，随着互联网的发展，各类社交媒体，科技软件，传感器等工具一刻不停地生成着越来越多地数据。有一个说法是: 人类现有90%的数据来自于过去两年。]]></summary></entry><entry><title type="html">Be a Good Data Engineer - Cron and Task Scheduler</title><link href="http://localhost:4000/go_big/2024/09/04/cron_and_scheduler.html" rel="alternate" type="text/html" title="Be a Good Data Engineer - Cron and Task Scheduler" /><published>2024-09-04T00:00:00+08:00</published><updated>2024-09-04T00:00:00+08:00</updated><id>http://localhost:4000/go_big/2024/09/04/cron_and_scheduler</id><content type="html" xml:base="http://localhost:4000/go_big/2024/09/04/cron_and_scheduler.html"><![CDATA[<h3 id="background">Background</h3>
<p>程序设计里，依据时间或者依赖其他任务、事件触发执行命令和脚本，是开发很重要的一部分。本文以类 Unix 系统最基础的 Crontab 和生产级任务调度器 Airflow 为例，整体梳理下任务调度这件事。</p>

<h3 id="cron">Cron</h3>
<p>Cron 是类 Unix 操作系统中一款基于时间的任务管理工具，可以通过 cron 在固定的时间、日期、间隔下，执行命令或运行脚本。</p>
<ul>
  <li>check if cron is installed: <code class="language-plaintext highlighter-rouge">dpkg -l cron</code></li>
  <li>install cron: <code class="language-plaintext highlighter-rouge">apt-get install cron</code></li>
  <li>verify the status of cron: <code class="language-plaintext highlighter-rouge">systemctl status cron</code> or <code class="language-plaintext highlighter-rouge">service cron status</code></li>
  <li>start/stop cron service: <code class="language-plaintext highlighter-rouge">systemctl start/stop cron</code> or <code class="language-plaintext highlighter-rouge">service cron start/stop</code>
    <h4 id="use-cases">Use Cases</h4>
    <p>Check Crontab use case, make sure you go through <code class="language-plaintext highlighter-rouge">man crontab</code></p>
  </li>
  <li>test your command line to schedule: <code class="language-plaintext highlighter-rouge">echo "Hello World manually at $(date)" &gt;&gt; $HOME/greetings_manual.txt</code></li>
  <li>check result: <code class="language-plaintext highlighter-rouge">tail ~/greetings_manual.txt</code></li>
  <li>install new crontab job, and choose vim editor: <code class="language-plaintext highlighter-rouge">crontab -e</code></li>
  <li>add a line <code class="language-plaintext highlighter-rouge">* * * * * echo "Hello World automatically at $(date)" &gt;&gt; $HOME/greetings.txt"</code> to the end of the crontab file, save and exit the editor</li>
  <li>check result: <code class="language-plaintext highlighter-rouge">tail ~/greetings.txt</code></li>
  <li>list all crontab jobs: <code class="language-plaintext highlighter-rouge">crontab -l</code></li>
  <li>remove all crontab jobs created by current user: <code class="language-plaintext highlighter-rouge">crontab -r</code>
    <h4 id="syntax">Syntax</h4>
    <p><code class="language-plaintext highlighter-rouge">* * * * * [command to execute]</code></p>
  </li>
  <li>First * stands for representing minutes [0-59];</li>
  <li>Second * stands for representing hour[0-23];</li>
  <li>Third * stands for representing day [0-31];</li>
  <li>Fourth * stands for representing month[0-12];</li>
  <li>Fifth * stands for representing a day of the week[0-6];</li>
</ul>

<p>You can check your cron schedule expressions at: <a href="https://crontab.guru/">Cronitor</a>.</p>

<h3 id="apache-airflow">Apache Airflow</h3>
<p><a href="https://airflow.apache.org/">Airflow</a> 是一款功能强大的开源工作流调度工具，除了内置的如bash、python、email等 <a href="https://airflow.apache.org/docs/apache-airflow/stable/operators-and-hooks-ref.html">Core Operators</a> 以外，还集成了大量第三方平台应用/软件的 <a href="https://airflow.apache.org/docs/apache-airflow-providers/operators-and-hooks-ref/index.html">Community Operators</a>，包含 Apache Software Foundation, Amazon Web Services, Microsoft Azure, Google, etc 相关服务的调度，并且能够监控任务的状态。Airflow 服务器可以部署在单机，也可以根据需求扩展至多节点部署。</p>

<h4 id="installation">installation</h4>
<p>Only <code class="language-plaintext highlighter-rouge">pip</code> installation is currently officially supported.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">AIRFLOW_VERSION</span><span class="o">=</span>2.10.0

<span class="nv">PYTHON_VERSION</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>python3 <span class="nt">--version</span> | <span class="nb">cut</span> <span class="nt">-d</span> <span class="s2">" "</span> <span class="nt">-f</span> 2 | <span class="nb">cut</span> <span class="nt">-d</span> <span class="s2">"."</span> <span class="nt">-f</span> 1-2<span class="si">)</span><span class="s2">"</span>

<span class="nv">CONSTRAINT_URL</span><span class="o">=</span><span class="s2">"https://raw.githubusercontent.com/apache/airflow/constraints-</span><span class="k">${</span><span class="nv">AIRFLOW_VERSION</span><span class="k">}</span><span class="s2">/constraints-</span><span class="k">${</span><span class="nv">PYTHON_VERSION</span><span class="k">}</span><span class="s2">.txt"</span>

pip <span class="nb">install</span> <span class="s2">"apache-airflow==</span><span class="k">${</span><span class="nv">AIRFLOW_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--constraint</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CONSTRAINT_URL</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<h4 id="init-airflow-database">init airflow database</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow db init
</code></pre></div></div>

<h4 id="create-a-user">create a user</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow <span class="nb">users </span>create <span class="se">\</span>
<span class="nt">--username</span> thekingofcool <span class="se">\</span>
<span class="nt">--firstname</span> bug <span class="se">\</span>
<span class="nt">--lastname</span> hunter <span class="se">\</span>
<span class="nt">--role</span> Admin <span class="se">\</span>
<span class="nt">--email</span> sayhi@thekingof.cool
</code></pre></div></div>

<h4 id="start-airflow-scheduler-and-web-server">start airflow scheduler and web server</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow scheduler
airflow webserver <span class="nt">--port</span> 8080
</code></pre></div></div>

<p>visit <a href="http://localhost:8080">localhost:8080</a> to check out the web page.</p>

<h4 id="airflow-dag-file">airflow dag file</h4>
<p>在 Airflow 配置文件中查看 dag 文件位置:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> ~/airflow/airflow.cfg | <span class="nb">grep </span>dags_folder
</code></pre></div></div>
<p>在该文件目录下编辑 dag 文件，重启 airflow scheduler。</p>

<p>Note:</p>
<ul>
  <li>Recommend to use Postgres or MySQL as <a href="https://airflow.apache.org/docs/apache-airflow/2.10.0/howto/set-up-database.html">metadata DB</a> in production;</li>
  <li>Do not use the <a href="https://airflow.apache.org/docs/apache-airflow/2.10.0/core-concepts/executor/index.html">SequentialExecutor</a> in production.</li>
</ul>

<p>To be continued…</p>]]></content><author><name></name></author><category term="go_big" /><summary type="html"><![CDATA[Background 程序设计里，依据时间或者依赖其他任务、事件触发执行命令和脚本，是开发很重要的一部分。本文以类 Unix 系统最基础的 Crontab 和生产级任务调度器 Airflow 为例，整体梳理下任务调度这件事。]]></summary></entry></feed>