<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>about me</title><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="thekingofcool&apos;s qqzone" /><link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico" />
  <link rel="stylesheet" href="/assets/css/main.css" />

  <link href="https://cdn.jsdelivr.net/gh/gangdong/gangdong.github.io@dev/assets/css/syntax_monokai.css" rel="stylesheet"/>
</head><body a="auto">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/"><-</a><h1></h1>

<h1 id="edward-chen">Edward Chen</h1>

<table>
  <tbody>
    <tr>
      <td>üìû 188-8387-8352</td>
      <td>üìß sayhi@thekingof.cool</td>
    </tr>
    <tr>
      <td><a href="https://linkedin.com/in/thekingofcool">LinkedIn</a></td>
      <td><a href="https://github.com/thekingofcool">GitHub</a></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="professional-summary">Professional Summary</h2>
<p>Former Structural Engineer (2016-2017), now Big Data Engineer (2018-present).</p>

<p>Engaged in data development for more than 6 years. Experienced in cloud technology like AWS and using Python, R, and Scala as programming language. Skilled in Airflow and Apache Hadoop/Spark ecosystem applications such as Hadoop, Hive, Spark, HDFS/S3.</p>

<p>Before as a big data engineer, I was a structural engineer after 4 years‚Äô of Civil Engineering study at ChongQing University, which are both the position of creating things.</p>

<p>I like sports. Boxing, frisbee, running. Make Sport a Daily Habit.</p>

<hr />

<h2 id="professional-experience">Professional Experience</h2>

<h3 id="big-data-engineer">Big Data Engineer</h3>
<p><strong>Nike (Contract)</strong><br />
<em>Mar 2019 - Present</em></p>

<ul>
  <li>Responsible for data delivery, data validation, task monitoring automation, and platform migration.</li>
  <li>Successfully participated in and led the design, development, and deployment of multiple big data projects.</li>
</ul>

<h3 id="big-data-engineer-1">Big Data Engineer</h3>
<p><strong>Shanghai Shouyi Information Technology Co., Ltd.</strong><br />
<em>Aug 2018 - Feb 2019</em></p>

<ul>
  <li>Participated in building the company‚Äôs big data architecture and applied stream processing frameworks to meet business needs.</li>
  <li>Conducted offline analysis of daily data, completed risk control and abnormal user data validation.</li>
  <li>Researched and quickly implemented new technologies, and wrote technical documentation.</li>
</ul>

<h3 id="technician">Technician</h3>
<p><strong>China State Construction Third Engineering Bureau</strong><br />
<em>Jul 2016 - Nov 2017</em></p>

<ul>
  <li>Responsible for pre-reviewing construction drawings, drafting construction plans, and inspecting construction nodes.</li>
  <li>Awarded first prize in the 2017 Xiamen QC Group Activity.</li>
</ul>

<hr />

<h2 id="education">Education</h2>
<p><strong>Chongqing University</strong><br />
Bachelor‚Äôs in Civil Engineering (2012 - 2016)</p>

<hr />

<h2 id="certifications--training">Certifications &amp; Training</h2>
<ul>
  <li><a href="https://www.credly.com/users/thekingofcool/badges">Snowflake Certified Data Engineer</a></li>
</ul>

<hr />

<h2 id="projects">Projects</h2>
<h3 id="nike-consumer-promise-review">Nike Consumer Promise Review</h3>

<ul>
  <li><strong>Background:</strong></li>
</ul>

<p>Consumer Promise Review (CPR) allows users to scan key Consumer Promise metrics across the Digital Supply Chain. On the Consumer Promise Dashboard, users can delve into KPIs on: ¬†<strong>Service</strong>¬†(<strong>Digital Units Shipped</strong>,¬†<strong>EDD% Precision</strong>), <strong>Shipment</strong>(<strong>Inner Region%</strong>, <strong>2D Ground%</strong>‚Äù), ¬†<strong>Convenience</strong>¬†(<strong>Split Shipment%</strong>), ¬†<strong>Sustainability</strong>¬†(<strong>Outbound AF%</strong>, <strong>RSC INV Accuracy</strong>) and¬†<strong>Cost</strong>¬†(<strong>CPU</strong>). These metrics are reported to the leadership team on a monthly and quarterly basis to provide a broader view of the business.</p>
<ul>
  <li><strong>Tech Stack:</strong></li>
</ul>

<p>Spark, Hive/Presto, Python, Airflow</p>
<ul>
  <li><strong>Role:</strong></li>
</ul>

<p>ETL and feed Presto tables to Tableau for reporting.</p>

<h3 id="gc-inbound-transportation">GC Inbound Transportation</h3>

<ul>
  <li><strong>Background:</strong></li>
</ul>

<p>GC inbound transportation owns the shipment capability of inventory movement between factory and GC Central DC. The entire inbound shipment process including¬†manufacturing in factory, consolidator processing, port to port transit, Custom clearance¬†and DC inbound processing. Through¬†appropriate transport mode and shipment tracking in different stages between cross team, inventory would be deployed into Central DC on time and accurately.¬†Cross teams are using inbound foundation data to track PO completion status, carriers‚Äô transportation performance, cost situation and CLC capicity forecast to make better decisions and take timely actions to optimize the vendor management, inbound route/mode selection and load priority optimization.</p>
<ul>
  <li><strong>Tech Stack:</strong></li>
</ul>

<p>Spark, Python, EMR, Snowflake, Box, Airflow</p>
<ul>
  <li><strong>Role:</strong></li>
</ul>

<p>ETL, generate data reports through Box and Snowflake.</p>

<h3 id="nike-united-intelligent-inventory">Nike United Intelligent Inventory</h3>

<ul>
  <li><strong>Background:</strong></li>
</ul>

<p>UII means united intelligent inventory, also called CTM means connect the marketplace. UII (CTM) system gathers real-time data from Top Doors, ADC inventory, Nike available orders and inventory. Based on Machine Learning algorithm, the CTM system do demand forecasting for Nike to predict the future sales of stores, and then finds the replenishment opportunities, and intelligently proposes the replenishment quantity for stores.</p>
<ul>
  <li><strong>Tech Stack:</strong></li>
</ul>

<p>Spark, Python, S3, EMR, Snowflake, Airflow</p>
<ul>
  <li><strong>Role:</strong></li>
</ul>

<p>Collect and cleanse data to form model features table for consumption by machine learning models; maintain various dimensional information; and monitor data quality.</p>

<h3 id="cryptocurrency-transaction-data-analysis-platform">Cryptocurrency Transaction Data Analysis Platform</h3>

<ul>
  <li><strong>Background:</strong></li>
</ul>

<p>In order to efficiently meet business needs, a big data platform is built to complete data cleansing based on log data to meet real-time/offline analysis such as recording user token holdings, labeling of abnormal transaction users, and average transaction interval time.</p>
<ul>
  <li><strong>Tech Stack:</strong></li>
</ul>

<p>Flume, Kafka, Hadoop, Structured Streaming, Aurora</p>
<ul>
  <li><strong>Role:</strong></li>
</ul>

<p>Consume the Kafka topics via Structured Streaming to obtain user asset change, transaction, and historical order information from system logs, and to meet the business other data requirements.</p>

<hr />

<h2 id="additional-information">Additional Information</h2>
<ul>
  <li><strong>Language:</strong> Fluent in English</li>
  <li><strong>Interests:</strong> Build body in pysical world and also skills in cyber world</li>
</ul>

      </div>
    </main>
  </body>
</html>